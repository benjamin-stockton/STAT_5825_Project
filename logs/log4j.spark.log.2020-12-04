20/12/04 12:51:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 12:51:27 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 12:51:27 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 12:51:27 INFO SecurityManager: Changing view acls groups to: 
20/12/04 12:51:27 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 12:51:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 12:51:28 INFO SparkContext: Running Spark version 3.0.1
20/12/04 12:51:28 INFO ResourceUtils: ==============================================================
20/12/04 12:51:28 INFO ResourceUtils: Resources for spark.driver:

20/12/04 12:51:28 INFO ResourceUtils: ==============================================================
20/12/04 12:51:28 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 12:51:28 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 12:51:28 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 12:51:28 INFO SecurityManager: Changing view acls groups to: 
20/12/04 12:51:28 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 12:51:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 12:51:28 INFO Utils: Successfully started service 'sparkDriver' on port 35135.
20/12/04 12:51:28 INFO SparkEnv: Registering MapOutputTracker
20/12/04 12:51:28 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 12:51:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 12:51:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 12:51:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 12:51:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a68a7cdf-aa5e-4ab8-a898-291839deefea
20/12/04 12:51:28 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 12:51:29 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 12:51:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 12:51:29 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 12:51:29 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 12:51:30 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 12:51:31 INFO Configuration: resource-types.xml not found
20/12/04 12:51:31 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 12:51:31 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 12:51:31 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 12:51:31 INFO Client: Setting up container launch context for our AM
20/12/04 12:51:31 INFO Client: Setting up the launch environment for our AM container
20/12/04 12:51:31 INFO Client: Preparing resources for our AM container
20/12/04 12:51:31 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 12:51:33 INFO Client: Uploading resource file:/tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/__spark_libs__7952973920460326308.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0040/__spark_libs__7952973920460326308.zip
20/12/04 12:51:34 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0040/pyspark.zip
20/12/04 12:51:34 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0040/py4j-0.10.9-src.zip
20/12/04 12:51:34 INFO Client: Uploading resource file:/tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/__spark_conf__6021449405568082422.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0040/__spark_conf__.zip
20/12/04 12:51:34 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 12:51:34 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 12:51:34 INFO SecurityManager: Changing view acls groups to: 
20/12/04 12:51:34 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 12:51:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 12:51:34 INFO Client: Submitting application application_1607015337794_0040 to ResourceManager
20/12/04 12:51:34 INFO YarnClientImpl: Submitted application application_1607015337794_0040
20/12/04 12:51:35 INFO Client: Application report for application_1607015337794_0040 (state: ACCEPTED)
20/12/04 12:51:35 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607104294832
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0040/
	 user: bsuconn
20/12/04 12:51:36 INFO Client: Application report for application_1607015337794_0040 (state: ACCEPTED)
20/12/04 12:51:37 INFO Client: Application report for application_1607015337794_0040 (state: ACCEPTED)
20/12/04 12:51:38 INFO Client: Application report for application_1607015337794_0040 (state: ACCEPTED)
20/12/04 12:51:39 INFO Client: Application report for application_1607015337794_0040 (state: ACCEPTED)
20/12/04 12:51:40 INFO Client: Application report for application_1607015337794_0040 (state: ACCEPTED)
20/12/04 12:51:41 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0040), /proxy/application_1607015337794_0040
20/12/04 12:51:41 INFO Client: Application report for application_1607015337794_0040 (state: RUNNING)
20/12/04 12:51:41 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607104294832
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0040/
	 user: bsuconn
20/12/04 12:51:41 INFO YarnClientSchedulerBackend: Application application_1607015337794_0040 has started running.
20/12/04 12:51:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40619.
20/12/04 12:51:41 INFO NettyBlockTransferService: Server created on 192.168.1.9:40619
20/12/04 12:51:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 12:51:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 40619, None)
20/12/04 12:51:41 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40619 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 40619, None)
20/12/04 12:51:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 40619, None)
20/12/04 12:51:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 40619, None)
20/12/04 12:51:42 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:51:42 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 12:51:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 12:51:46 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:40014) with ID 1
20/12/04 12:51:46 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 12:51:47 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:35263 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 35263, None)
20/12/04 12:51:47 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 12:51:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 12:51:47 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 12:51:47 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:51:47 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:51:47 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:51:47 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:51:47 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:51:47 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:35135/files/darima.zip with timestamp 1607104307887
20/12/04 12:51:47 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/userFiles-6460f17e-4c35-4ecf-8e1b-c0c1808c8422/darima.zip
20/12/04 12:51:49 INFO InMemoryFileIndex: It took 83 ms to list leaf files for 1 paths.
20/12/04 12:51:51 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 1 paths.
20/12/04 12:51:52 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 12:51:52 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 12:51:52 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 12:51:52 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 12:51:53 INFO CodeGenerator: Code generated in 245.05083 ms
20/12/04 12:51:53 INFO CodeGenerator: Code generated in 31.755998 ms
20/12/04 12:51:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 12:51:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 12:51:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:40619 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 12:51:53 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 12:51:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 12:51:53 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 12:51:53 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 12:51:53 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 12:51:53 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 12:51:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 12:51:53 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 12:51:53 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 12:51:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 12:51:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 12:51:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:40619 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 12:51:53 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 12:51:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 12:51:53 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 12:51:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 12:51:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:35263 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 12:51:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:35263 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 12:51:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3455 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 12:51:57 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 12:51:57 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.612 s
20/12/04 12:51:57 INFO DAGScheduler: looking for newly runnable stages
20/12/04 12:51:57 INFO DAGScheduler: running: Set()
20/12/04 12:51:57 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 12:51:57 INFO DAGScheduler: failed: Set()
20/12/04 12:51:57 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 12:51:57 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 12:51:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 12:51:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:40619 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 12:51:57 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 12:51:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 12:51:57 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 12:51:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 12:51:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:35263 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 12:51:57 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:40014
20/12/04 12:51:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 383 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 12:51:57 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 12:51:57 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.402 s
20/12/04 12:51:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 12:51:57 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 12:51:57 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 4.111236 s
20/12/04 12:51:59 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 12:51:59 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:40619 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 12:51:59 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:35263 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 12:51:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:40619 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 12:51:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:35263 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 12:52:00 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 12:52:00 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 12:52:00 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 12:52:00 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 12:52:00 INFO CodeGenerator: Code generated in 27.749457 ms
20/12/04 12:52:01 INFO CodeGenerator: Code generated in 17.478753 ms
20/12/04 12:52:01 INFO CodeGenerator: Code generated in 22.295394 ms
20/12/04 12:52:01 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 12:52:01 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 12:52:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:40619 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 12:52:01 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/userFiles-6460f17e-4c35-4ecf-8e1b-c0c1808c8422/darima.zip/darima/dlsa.py:22
20/12/04 12:52:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 12:52:01 INFO SparkContext: Starting job: toPandas at /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/userFiles-6460f17e-4c35-4ecf-8e1b-c0c1808c8422/darima.zip/darima/dlsa.py:22
20/12/04 12:52:01 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/userFiles-6460f17e-4c35-4ecf-8e1b-c0c1808c8422/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 12:52:01 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/userFiles-6460f17e-4c35-4ecf-8e1b-c0c1808c8422/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 12:52:01 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/userFiles-6460f17e-4c35-4ecf-8e1b-c0c1808c8422/darima.zip/darima/dlsa.py:22)
20/12/04 12:52:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 12:52:01 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 12:52:01 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/userFiles-6460f17e-4c35-4ecf-8e1b-c0c1808c8422/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 12:52:01 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 12:52:01 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 12:52:01 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:40619 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 12:52:01 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 12:52:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/userFiles-6460f17e-4c35-4ecf-8e1b-c0c1808c8422/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 12:52:01 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 12:52:01 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 12:52:01 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:35263 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 12:52:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:35263 (size: 28.7 KiB, free: 912.2 MiB)
20/12/04 12:52:02 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1385 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 12:52:02 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 12:52:02 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 43503
20/12/04 12:52:02 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/userFiles-6460f17e-4c35-4ecf-8e1b-c0c1808c8422/darima.zip/darima/dlsa.py:22) finished in 1.420 s
20/12/04 12:52:02 INFO DAGScheduler: looking for newly runnable stages
20/12/04 12:52:02 INFO DAGScheduler: running: Set()
20/12/04 12:52:02 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 12:52:02 INFO DAGScheduler: failed: Set()
20/12/04 12:52:02 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/userFiles-6460f17e-4c35-4ecf-8e1b-c0c1808c8422/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 12:52:02 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 12:52:02 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.4 KiB, free 5.8 GiB)
20/12/04 12:52:02 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:40619 (size: 131.4 KiB, free: 5.8 GiB)
20/12/04 12:52:02 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 12:52:02 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/userFiles-6460f17e-4c35-4ecf-8e1b-c0c1808c8422/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 12:52:02 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 12:52:02 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:02 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:35263 (size: 131.4 KiB, free: 912.1 MiB)
20/12/04 12:52:03 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:40014
20/12/04 12:52:06 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:06 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 12:52:07 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:07 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 12:52:07 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:07 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 12:52:08 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:08 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 12:52:09 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:09 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 12:52:10 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:10 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 12:52:11 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:11 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 12:52:11 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 12:52:11 INFO YarnScheduler: Cancelling stage 3
20/12/04 12:52:11 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 12:52:11 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 12:52:11 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/userFiles-6460f17e-4c35-4ecf-8e1b-c0c1808c8422/darima.zip/darima/dlsa.py:22) failed in 8.659 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 12:52:11 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/userFiles-6460f17e-4c35-4ecf-8e1b-c0c1808c8422/darima.zip/darima/dlsa.py:22, took 10.132418 s
20/12/04 12:52:11 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 12:52:11 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 12:52:11 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 12:52:11 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 12:52:11 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 12:52:11 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 12:52:11 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 12:52:11 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 12:52:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 12:52:12 INFO MemoryStore: MemoryStore cleared
20/12/04 12:52:12 INFO BlockManager: BlockManager stopped
20/12/04 12:52:12 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 12:52:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 12:52:12 INFO SparkContext: Successfully stopped SparkContext
20/12/04 12:52:12 INFO ShutdownHookManager: Shutdown hook called
20/12/04 12:52:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5/pyspark-4a2945ad-b6c2-45a9-a345-a4474f02da13
20/12/04 12:52:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-d3574d61-f194-427d-b025-c12d8de97cf5
20/12/04 12:52:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-6cd50fc4-a54e-4357-8479-87f438267e17
20/12/04 12:52:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 12:52:14 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 12:52:14 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 12:52:14 INFO SecurityManager: Changing view acls groups to: 
20/12/04 12:52:14 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 12:52:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 12:52:15 INFO SparkContext: Running Spark version 3.0.1
20/12/04 12:52:15 INFO ResourceUtils: ==============================================================
20/12/04 12:52:15 INFO ResourceUtils: Resources for spark.driver:

20/12/04 12:52:15 INFO ResourceUtils: ==============================================================
20/12/04 12:52:15 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 12:52:15 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 12:52:15 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 12:52:15 INFO SecurityManager: Changing view acls groups to: 
20/12/04 12:52:15 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 12:52:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 12:52:16 INFO Utils: Successfully started service 'sparkDriver' on port 45235.
20/12/04 12:52:16 INFO SparkEnv: Registering MapOutputTracker
20/12/04 12:52:16 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 12:52:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 12:52:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 12:52:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 12:52:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b53e3ab0-b065-4002-bcea-2b1662fd5f2f
20/12/04 12:52:16 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 12:52:16 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 12:52:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 12:52:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 12:52:17 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 12:52:17 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 12:52:17 INFO Configuration: resource-types.xml not found
20/12/04 12:52:17 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 12:52:17 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 12:52:17 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 12:52:17 INFO Client: Setting up container launch context for our AM
20/12/04 12:52:17 INFO Client: Setting up the launch environment for our AM container
20/12/04 12:52:17 INFO Client: Preparing resources for our AM container
20/12/04 12:52:17 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 12:52:20 INFO Client: Uploading resource file:/tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/__spark_libs__17068776452922741564.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0041/__spark_libs__17068776452922741564.zip
20/12/04 12:52:20 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0041/pyspark.zip
20/12/04 12:52:20 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0041/py4j-0.10.9-src.zip
20/12/04 12:52:21 INFO Client: Uploading resource file:/tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/__spark_conf__4996199432954137742.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0041/__spark_conf__.zip
20/12/04 12:52:21 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 12:52:21 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 12:52:21 INFO SecurityManager: Changing view acls groups to: 
20/12/04 12:52:21 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 12:52:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 12:52:21 INFO Client: Submitting application application_1607015337794_0041 to ResourceManager
20/12/04 12:52:21 INFO YarnClientImpl: Submitted application application_1607015337794_0041
20/12/04 12:52:22 INFO Client: Application report for application_1607015337794_0041 (state: ACCEPTED)
20/12/04 12:52:22 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607104341296
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0041/
	 user: bsuconn
20/12/04 12:52:23 INFO Client: Application report for application_1607015337794_0041 (state: ACCEPTED)
20/12/04 12:52:24 INFO Client: Application report for application_1607015337794_0041 (state: ACCEPTED)
20/12/04 12:52:25 INFO Client: Application report for application_1607015337794_0041 (state: ACCEPTED)
20/12/04 12:52:26 INFO Client: Application report for application_1607015337794_0041 (state: RUNNING)
20/12/04 12:52:26 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607104341296
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0041/
	 user: bsuconn
20/12/04 12:52:26 INFO YarnClientSchedulerBackend: Application application_1607015337794_0041 has started running.
20/12/04 12:52:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44857.
20/12/04 12:52:26 INFO NettyBlockTransferService: Server created on 192.168.1.9:44857
20/12/04 12:52:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 12:52:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 44857, None)
20/12/04 12:52:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:44857 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 44857, None)
20/12/04 12:52:26 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0041), /proxy/application_1607015337794_0041
20/12/04 12:52:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 44857, None)
20/12/04 12:52:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 44857, None)
20/12/04 12:52:26 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:52:27 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 12:52:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 12:52:32 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:44342) with ID 1
20/12/04 12:52:32 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:46025 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 46025, None)
20/12/04 12:52:33 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:44346) with ID 2
20/12/04 12:52:33 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 12:52:33 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:39337 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 39337, None)
20/12/04 12:52:33 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 12:52:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 12:52:33 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 12:52:33 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:52:33 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:52:33 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:52:33 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:52:33 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:52:34 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:45235/files/darima.zip with timestamp 1607104354362
20/12/04 12:52:34 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/userFiles-bd97881c-4e77-4d70-a53b-17a72eaf27a1/darima.zip
20/12/04 12:52:36 INFO InMemoryFileIndex: It took 57 ms to list leaf files for 1 paths.
20/12/04 12:52:37 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
20/12/04 12:52:38 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 12:52:38 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 12:52:38 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 12:52:38 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 12:52:39 INFO CodeGenerator: Code generated in 282.734317 ms
20/12/04 12:52:39 INFO CodeGenerator: Code generated in 30.494481 ms
20/12/04 12:52:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 12:52:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 12:52:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:44857 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 12:52:39 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 12:52:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 12:52:39 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 12:52:39 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 12:52:39 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 12:52:39 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 12:52:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 12:52:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 12:52:39 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 12:52:39 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 12:52:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 12:52:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:44857 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 12:52:39 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 12:52:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 12:52:39 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 12:52:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 12:52:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:46025 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 12:52:41 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:46025 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 12:52:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3259 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 12:52:42 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 12:52:43 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.411 s
20/12/04 12:52:43 INFO DAGScheduler: looking for newly runnable stages
20/12/04 12:52:43 INFO DAGScheduler: running: Set()
20/12/04 12:52:43 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 12:52:43 INFO DAGScheduler: failed: Set()
20/12/04 12:52:43 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 12:52:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 12:52:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 12:52:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:44857 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 12:52:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 12:52:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 12:52:43 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 12:52:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:46025 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 12:52:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:44342
20/12/04 12:52:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 322 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 12:52:43 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 12:52:43 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.353 s
20/12/04 12:52:43 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 12:52:43 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 12:52:43 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.863007 s
20/12/04 12:52:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:46025 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 12:52:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:44857 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 12:52:45 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:46025 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 12:52:45 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:44857 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 12:52:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:44857 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 12:52:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:46025 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 12:52:45 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 12:52:46 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 12:52:46 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 12:52:46 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 12:52:46 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 12:52:46 INFO CodeGenerator: Code generated in 27.55021 ms
20/12/04 12:52:46 INFO CodeGenerator: Code generated in 16.920283 ms
20/12/04 12:52:46 INFO CodeGenerator: Code generated in 38.09793 ms
20/12/04 12:52:46 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 12:52:46 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 12:52:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:44857 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 12:52:46 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/userFiles-bd97881c-4e77-4d70-a53b-17a72eaf27a1/darima.zip/darima/dlsa.py:22
20/12/04 12:52:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 12:52:47 INFO SparkContext: Starting job: toPandas at /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/userFiles-bd97881c-4e77-4d70-a53b-17a72eaf27a1/darima.zip/darima/dlsa.py:22
20/12/04 12:52:47 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/userFiles-bd97881c-4e77-4d70-a53b-17a72eaf27a1/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 12:52:47 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/userFiles-bd97881c-4e77-4d70-a53b-17a72eaf27a1/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 12:52:47 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/userFiles-bd97881c-4e77-4d70-a53b-17a72eaf27a1/darima.zip/darima/dlsa.py:22)
20/12/04 12:52:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 12:52:47 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 12:52:47 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/userFiles-bd97881c-4e77-4d70-a53b-17a72eaf27a1/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 12:52:47 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 12:52:47 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 12:52:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:44857 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 12:52:47 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 12:52:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/userFiles-bd97881c-4e77-4d70-a53b-17a72eaf27a1/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 12:52:47 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 12:52:47 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 12:52:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:46025 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 12:52:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:46025 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 12:52:48 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1347 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 12:52:48 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 12:52:48 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 44497
20/12/04 12:52:48 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/userFiles-bd97881c-4e77-4d70-a53b-17a72eaf27a1/darima.zip/darima/dlsa.py:22) finished in 1.373 s
20/12/04 12:52:48 INFO DAGScheduler: looking for newly runnable stages
20/12/04 12:52:48 INFO DAGScheduler: running: Set()
20/12/04 12:52:48 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 12:52:48 INFO DAGScheduler: failed: Set()
20/12/04 12:52:48 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/userFiles-bd97881c-4e77-4d70-a53b-17a72eaf27a1/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 12:52:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 12:52:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 12:52:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:44857 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 12:52:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 12:52:48 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/userFiles-bd97881c-4e77-4d70-a53b-17a72eaf27a1/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 12:52:48 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 12:52:48 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:48 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:46025 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 12:52:49 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:44342
20/12/04 12:52:49 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:39337 (size: 131.5 KiB, free: 912.2 MiB)
20/12/04 12:52:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:44346
20/12/04 12:52:54 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:54 WARN TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 12:52:56 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:56 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 12:52:57 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:57 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 12:52:59 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:59 INFO TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 12:52:59 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 12:52:59 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 12:53:00 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 12:53:00 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 12:53:00 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 11, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 12:53:00 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 12:53:02 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 10) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/04 12:53:02 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 12:53:02 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 11) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/04 12:53:02 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 12:53:03 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 14, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 12:53:03 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 12) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 9]
20/12/04 12:53:03 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 12:53:03 INFO YarnScheduler: Cancelling stage 3
20/12/04 12:53:03 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 12:53:03 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 12:53:03 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/userFiles-bd97881c-4e77-4d70-a53b-17a72eaf27a1/darima.zip/darima/dlsa.py:22) failed in 15.291 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 12:53:03 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/userFiles-bd97881c-4e77-4d70-a53b-17a72eaf27a1/darima.zip/darima/dlsa.py:22, took 16.730455 s
20/12/04 12:53:04 WARN TaskSetManager: Lost task 3.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/04 12:53:04 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 12:53:04 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 12:53:04 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 12:53:04 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 14, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 12:53:04 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 12:53:04 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 12:53:04 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 12:53:04 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 12:53:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 12:53:04 INFO MemoryStore: MemoryStore cleared
20/12/04 12:53:04 INFO BlockManager: BlockManager stopped
20/12/04 12:53:04 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 12:53:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 12:53:04 INFO SparkContext: Successfully stopped SparkContext
20/12/04 12:53:04 INFO ShutdownHookManager: Shutdown hook called
20/12/04 12:53:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b
20/12/04 12:53:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-72772513-acc8-479c-8d9a-4f3f378ee737
20/12/04 12:53:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-a5b2990b-f236-44a7-a21c-549a29dcb31b/pyspark-99543770-55dc-48f7-ae78-659769a77ac9
20/12/04 12:53:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 12:53:07 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 12:53:07 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 12:53:07 INFO SecurityManager: Changing view acls groups to: 
20/12/04 12:53:07 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 12:53:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 12:53:08 INFO SparkContext: Running Spark version 3.0.1
20/12/04 12:53:08 INFO ResourceUtils: ==============================================================
20/12/04 12:53:08 INFO ResourceUtils: Resources for spark.driver:

20/12/04 12:53:08 INFO ResourceUtils: ==============================================================
20/12/04 12:53:08 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 12:53:08 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 12:53:08 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 12:53:08 INFO SecurityManager: Changing view acls groups to: 
20/12/04 12:53:08 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 12:53:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 12:53:08 INFO Utils: Successfully started service 'sparkDriver' on port 37857.
20/12/04 12:53:08 INFO SparkEnv: Registering MapOutputTracker
20/12/04 12:53:08 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 12:53:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 12:53:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 12:53:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 12:53:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-77cf10f2-2d8b-4751-b2bd-172eccbe7ef3
20/12/04 12:53:08 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 12:53:08 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 12:53:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 12:53:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 12:53:09 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 12:53:09 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 12:53:10 INFO Configuration: resource-types.xml not found
20/12/04 12:53:10 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 12:53:10 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 12:53:10 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 12:53:10 INFO Client: Setting up container launch context for our AM
20/12/04 12:53:10 INFO Client: Setting up the launch environment for our AM container
20/12/04 12:53:10 INFO Client: Preparing resources for our AM container
20/12/04 12:53:10 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 12:53:12 INFO Client: Uploading resource file:/tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/__spark_libs__12519258726550682911.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0042/__spark_libs__12519258726550682911.zip
20/12/04 12:53:13 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0042/pyspark.zip
20/12/04 12:53:13 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0042/py4j-0.10.9-src.zip
20/12/04 12:53:13 INFO Client: Uploading resource file:/tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/__spark_conf__3453041211386181085.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0042/__spark_conf__.zip
20/12/04 12:53:13 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 12:53:13 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 12:53:13 INFO SecurityManager: Changing view acls groups to: 
20/12/04 12:53:13 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 12:53:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 12:53:13 INFO Client: Submitting application application_1607015337794_0042 to ResourceManager
20/12/04 12:53:13 INFO YarnClientImpl: Submitted application application_1607015337794_0042
20/12/04 12:53:14 INFO Client: Application report for application_1607015337794_0042 (state: ACCEPTED)
20/12/04 12:53:14 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607104393707
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0042/
	 user: bsuconn
20/12/04 12:53:15 INFO Client: Application report for application_1607015337794_0042 (state: ACCEPTED)
20/12/04 12:53:16 INFO Client: Application report for application_1607015337794_0042 (state: ACCEPTED)
20/12/04 12:53:17 INFO Client: Application report for application_1607015337794_0042 (state: ACCEPTED)
20/12/04 12:53:18 INFO Client: Application report for application_1607015337794_0042 (state: RUNNING)
20/12/04 12:53:18 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607104393707
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0042/
	 user: bsuconn
20/12/04 12:53:18 INFO YarnClientSchedulerBackend: Application application_1607015337794_0042 has started running.
20/12/04 12:53:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41399.
20/12/04 12:53:18 INFO NettyBlockTransferService: Server created on 192.168.1.9:41399
20/12/04 12:53:18 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 12:53:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 41399, None)
20/12/04 12:53:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:41399 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 41399, None)
20/12/04 12:53:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 41399, None)
20/12/04 12:53:18 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 41399, None)
20/12/04 12:53:18 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0042), /proxy/application_1607015337794_0042
20/12/04 12:53:19 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:53:19 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 12:53:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 12:53:24 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:48698) with ID 1
20/12/04 12:53:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:35063 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 35063, None)
20/12/04 12:53:25 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:48702) with ID 2
20/12/04 12:53:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:35403 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 35403, None)
20/12/04 12:53:39 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
20/12/04 12:53:39 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 12:53:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 12:53:39 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 12:53:39 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:53:39 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:53:39 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:53:39 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:53:39 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 12:53:40 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:37857/files/darima.zip with timestamp 1607104420429
20/12/04 12:53:40 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/userFiles-113237a8-1916-4649-9b19-338c2cb0a080/darima.zip
20/12/04 12:53:42 INFO InMemoryFileIndex: It took 56 ms to list leaf files for 1 paths.
20/12/04 12:53:43 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
20/12/04 12:53:44 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 12:53:44 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 12:53:44 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 12:53:44 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 12:53:45 INFO CodeGenerator: Code generated in 257.599028 ms
20/12/04 12:53:45 INFO CodeGenerator: Code generated in 33.057594 ms
20/12/04 12:53:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 12:53:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 12:53:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:41399 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 12:53:45 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 12:53:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 12:53:45 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 12:53:45 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 12:53:45 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 12:53:45 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 12:53:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 12:53:45 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 12:53:45 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 12:53:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 12:53:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 12:53:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:41399 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 12:53:45 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 12:53:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 12:53:45 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 12:53:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 12:53:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:35403 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 12:53:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:35403 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 12:53:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3225 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 12:53:49 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 12:53:49 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.372 s
20/12/04 12:53:49 INFO DAGScheduler: looking for newly runnable stages
20/12/04 12:53:49 INFO DAGScheduler: running: Set()
20/12/04 12:53:49 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 12:53:49 INFO DAGScheduler: failed: Set()
20/12/04 12:53:49 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 12:53:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 12:53:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 12:53:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:41399 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 12:53:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 12:53:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 12:53:49 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 12:53:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 12:53:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:35403 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 12:53:49 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:48702
20/12/04 12:53:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 383 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 12:53:49 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.397 s
20/12/04 12:53:49 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 12:53:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 12:53:49 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 12:53:49 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.865137 s
20/12/04 12:53:51 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 12:53:52 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:35403 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 12:53:52 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:41399 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 12:53:52 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:41399 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 12:53:52 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:35403 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 12:53:52 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 12:53:52 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 12:53:52 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 12:53:52 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 12:53:52 INFO CodeGenerator: Code generated in 24.256989 ms
20/12/04 12:53:52 INFO CodeGenerator: Code generated in 16.713506 ms
20/12/04 12:53:53 INFO CodeGenerator: Code generated in 21.207678 ms
20/12/04 12:53:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 12:53:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 12:53:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:41399 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 12:53:53 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/userFiles-113237a8-1916-4649-9b19-338c2cb0a080/darima.zip/darima/dlsa.py:22
20/12/04 12:53:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 12:53:53 INFO SparkContext: Starting job: toPandas at /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/userFiles-113237a8-1916-4649-9b19-338c2cb0a080/darima.zip/darima/dlsa.py:22
20/12/04 12:53:53 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/userFiles-113237a8-1916-4649-9b19-338c2cb0a080/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 12:53:53 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/userFiles-113237a8-1916-4649-9b19-338c2cb0a080/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 12:53:53 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/userFiles-113237a8-1916-4649-9b19-338c2cb0a080/darima.zip/darima/dlsa.py:22)
20/12/04 12:53:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 12:53:53 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 12:53:53 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/userFiles-113237a8-1916-4649-9b19-338c2cb0a080/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 12:53:53 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 12:53:53 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 12:53:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:41399 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 12:53:53 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 12:53:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/userFiles-113237a8-1916-4649-9b19-338c2cb0a080/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 12:53:53 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 12:53:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 12:53:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:35403 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 12:53:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:35403 (size: 28.7 KiB, free: 912.2 MiB)
20/12/04 12:53:54 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1247 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 12:53:54 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 12:53:54 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 38395
20/12/04 12:53:54 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/userFiles-113237a8-1916-4649-9b19-338c2cb0a080/darima.zip/darima/dlsa.py:22) finished in 1.269 s
20/12/04 12:53:54 INFO DAGScheduler: looking for newly runnable stages
20/12/04 12:53:54 INFO DAGScheduler: running: Set()
20/12/04 12:53:54 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 12:53:54 INFO DAGScheduler: failed: Set()
20/12/04 12:53:54 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/userFiles-113237a8-1916-4649-9b19-338c2cb0a080/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 12:53:54 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 12:53:54 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 12:53:54 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:41399 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 12:53:54 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 12:53:54 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/userFiles-113237a8-1916-4649-9b19-338c2cb0a080/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 12:53:54 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 12:53:54 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 12:53:54 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 12:53:54 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:35403 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 12:53:55 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:48702
20/12/04 12:53:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:35063 (size: 131.5 KiB, free: 912.2 MiB)
20/12/04 12:53:57 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:48698
20/12/04 12:54:00 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 12:54:00 WARN TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 12:54:02 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 12:54:02 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 12:54:03 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 7, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 12:54:03 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 12:54:05 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 12:54:05 INFO TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 12:54:05 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 9, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 12:54:05 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 12:54:07 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 12:54:07 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 12:54:07 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 11, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 12:54:07 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 9) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 12:54:08 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 10) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/04 12:54:08 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 12:54:08 INFO TaskSetManager: Lost task 2.3 in stage 3.0 (TID 11) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/04 12:54:08 ERROR TaskSetManager: Task 2 in stage 3.0 failed 4 times; aborting job
20/12/04 12:54:08 INFO YarnScheduler: Cancelling stage 3
20/12/04 12:54:08 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 12:54:08 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 12:54:08 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/userFiles-113237a8-1916-4649-9b19-338c2cb0a080/darima.zip/darima/dlsa.py:22) failed in 14.155 s due to Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 11, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 12:54:08 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/userFiles-113237a8-1916-4649-9b19-338c2cb0a080/darima.zip/darima/dlsa.py:22, took 15.493692 s
20/12/04 12:54:09 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 12:54:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:41399 in memory (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 12:54:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:35403 in memory (size: 11.6 KiB, free: 912.1 MiB)
20/12/04 12:54:09 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 12:54:09 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 12:54:09 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 12:54:09 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 12:54:09 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 12:54:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 12:54:09 INFO MemoryStore: MemoryStore cleared
20/12/04 12:54:09 INFO BlockManager: BlockManager stopped
20/12/04 12:54:09 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 12:54:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 12:54:09 INFO SparkContext: Successfully stopped SparkContext
20/12/04 12:54:09 INFO ShutdownHookManager: Shutdown hook called
20/12/04 12:54:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1
20/12/04 12:54:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-caf84e3a-e9c4-490e-b2af-4c61c23e87bc
20/12/04 12:54:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-e1b2c4f3-c0a8-4638-bcad-b00d1bbae4c1/pyspark-97bdf399-76ee-43c6-9237-b99548cdc8b1
20/12/04 14:18:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 14:18:58 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:18:58 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:18:58 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:18:58 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:18:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:18:59 INFO SparkContext: Running Spark version 3.0.1
20/12/04 14:18:59 INFO ResourceUtils: ==============================================================
20/12/04 14:18:59 INFO ResourceUtils: Resources for spark.driver:

20/12/04 14:18:59 INFO ResourceUtils: ==============================================================
20/12/04 14:18:59 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 14:18:59 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:18:59 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:18:59 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:18:59 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:18:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:19:00 INFO Utils: Successfully started service 'sparkDriver' on port 42139.
20/12/04 14:19:00 INFO SparkEnv: Registering MapOutputTracker
20/12/04 14:19:00 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 14:19:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 14:19:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 14:19:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 14:19:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e3d67bae-e7ae-41ca-bfa8-916968ca5c48
20/12/04 14:19:00 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 14:19:00 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 14:19:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 14:19:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 14:19:01 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 14:19:01 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 14:19:02 INFO Configuration: resource-types.xml not found
20/12/04 14:19:02 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 14:19:02 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 14:19:02 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 14:19:02 INFO Client: Setting up container launch context for our AM
20/12/04 14:19:02 INFO Client: Setting up the launch environment for our AM container
20/12/04 14:19:02 INFO Client: Preparing resources for our AM container
20/12/04 14:19:02 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 14:19:05 INFO Client: Uploading resource file:/tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/__spark_libs__11790785151323165502.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0043/__spark_libs__11790785151323165502.zip
20/12/04 14:19:06 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0043/pyspark.zip
20/12/04 14:19:06 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0043/py4j-0.10.9-src.zip
20/12/04 14:19:06 INFO Client: Uploading resource file:/tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/__spark_conf__3427902319101877650.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0043/__spark_conf__.zip
20/12/04 14:19:06 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:19:06 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:19:06 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:19:06 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:19:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:19:06 INFO Client: Submitting application application_1607015337794_0043 to ResourceManager
20/12/04 14:19:06 INFO YarnClientImpl: Submitted application application_1607015337794_0043
20/12/04 14:19:07 INFO Client: Application report for application_1607015337794_0043 (state: ACCEPTED)
20/12/04 14:19:07 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607109546808
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0043/
	 user: bsuconn
20/12/04 14:19:08 INFO Client: Application report for application_1607015337794_0043 (state: ACCEPTED)
20/12/04 14:19:09 INFO Client: Application report for application_1607015337794_0043 (state: ACCEPTED)
20/12/04 14:19:10 INFO Client: Application report for application_1607015337794_0043 (state: ACCEPTED)
20/12/04 14:19:11 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0043), /proxy/application_1607015337794_0043
20/12/04 14:19:11 INFO Client: Application report for application_1607015337794_0043 (state: RUNNING)
20/12/04 14:19:11 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607109546808
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0043/
	 user: bsuconn
20/12/04 14:19:11 INFO YarnClientSchedulerBackend: Application application_1607015337794_0043 has started running.
20/12/04 14:19:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43397.
20/12/04 14:19:11 INFO NettyBlockTransferService: Server created on 192.168.1.9:43397
20/12/04 14:19:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 14:19:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 43397, None)
20/12/04 14:19:11 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:43397 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 43397, None)
20/12/04 14:19:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 43397, None)
20/12/04 14:19:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 43397, None)
20/12/04 14:19:12 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:19:12 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 14:19:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 14:19:16 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:36136) with ID 1
20/12/04 14:19:16 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 14:19:16 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:46473 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 46473, None)
20/12/04 14:19:16 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 14:19:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 14:19:16 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 14:19:16 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:19:16 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:19:16 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:19:16 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:19:16 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:19:17 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:42139/files/darima.zip with timestamp 1607109557699
20/12/04 14:19:17 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/userFiles-35986533-2271-4cc0-85a3-c84ee301c43b/darima.zip
20/12/04 14:19:19 INFO InMemoryFileIndex: It took 61 ms to list leaf files for 1 paths.
20/12/04 14:19:21 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
20/12/04 14:19:21 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:19:21 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:19:21 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:19:21 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:19:22 INFO CodeGenerator: Code generated in 310.012803 ms
20/12/04 14:19:22 INFO CodeGenerator: Code generated in 29.424728 ms
20/12/04 14:19:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:19:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:19:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:43397 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:19:22 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 14:19:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:19:23 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 14:19:23 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 14:19:23 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 14:19:23 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 14:19:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 14:19:23 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 14:19:23 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:19:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 14:19:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 14:19:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:43397 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:19:23 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 14:19:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:19:23 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 14:19:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:19:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:46473 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:19:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:46473 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:19:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3213 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:19:26 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 14:19:26 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.369 s
20/12/04 14:19:26 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:19:26 INFO DAGScheduler: running: Set()
20/12/04 14:19:26 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 14:19:26 INFO DAGScheduler: failed: Set()
20/12/04 14:19:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:19:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 14:19:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 14:19:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:43397 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:19:26 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 14:19:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:19:26 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 14:19:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:19:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:46473 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:19:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:36136
20/12/04 14:19:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 358 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:19:26 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 14:19:26 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.374 s
20/12/04 14:19:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 14:19:26 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 14:19:26 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.832283 s
20/12/04 14:19:29 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:46473 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:19:29 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:43397 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:19:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:43397 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:19:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:46473 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:19:29 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 14:19:29 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:46473 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:19:29 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:43397 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:19:30 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:19:30 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:19:30 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:19:30 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:19:30 INFO CodeGenerator: Code generated in 27.209098 ms
20/12/04 14:19:30 INFO CodeGenerator: Code generated in 20.260264 ms
20/12/04 14:19:30 INFO CodeGenerator: Code generated in 26.006757 ms
20/12/04 14:19:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:19:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:19:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:43397 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:19:30 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/userFiles-35986533-2271-4cc0-85a3-c84ee301c43b/darima.zip/darima/dlsa.py:22
20/12/04 14:19:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:19:30 INFO SparkContext: Starting job: toPandas at /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/userFiles-35986533-2271-4cc0-85a3-c84ee301c43b/darima.zip/darima/dlsa.py:22
20/12/04 14:19:30 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/userFiles-35986533-2271-4cc0-85a3-c84ee301c43b/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 14:19:30 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/userFiles-35986533-2271-4cc0-85a3-c84ee301c43b/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 14:19:30 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/userFiles-35986533-2271-4cc0-85a3-c84ee301c43b/darima.zip/darima/dlsa.py:22)
20/12/04 14:19:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 14:19:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 14:19:30 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/userFiles-35986533-2271-4cc0-85a3-c84ee301c43b/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:19:30 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 14:19:30 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 14:19:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:43397 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 14:19:30 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 14:19:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/userFiles-35986533-2271-4cc0-85a3-c84ee301c43b/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 14:19:30 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 14:19:30 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:19:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:46473 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 14:19:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:46473 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:19:32 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1276 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:19:32 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 14:19:32 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 47943
20/12/04 14:19:32 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/userFiles-35986533-2271-4cc0-85a3-c84ee301c43b/darima.zip/darima/dlsa.py:22) finished in 1.302 s
20/12/04 14:19:32 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:19:32 INFO DAGScheduler: running: Set()
20/12/04 14:19:32 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 14:19:32 INFO DAGScheduler: failed: Set()
20/12/04 14:19:32 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/userFiles-35986533-2271-4cc0-85a3-c84ee301c43b/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:19:32 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 14:19:32 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 14:19:32 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:43397 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 14:19:32 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 14:19:32 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/userFiles-35986533-2271-4cc0-85a3-c84ee301c43b/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 14:19:32 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 14:19:32 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:19:32 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:46473 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 14:19:32 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:36136
20/12/04 14:19:35 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:19:35 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 14:19:36 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:19:36 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 14:19:37 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:19:37 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 14:19:38 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:19:38 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 14:19:39 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:19:39 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 14:19:40 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:19:40 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 14:19:40 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:19:40 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 14:19:40 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 14:19:40 INFO YarnScheduler: Cancelling stage 3
20/12/04 14:19:40 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 14:19:40 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 14:19:40 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/userFiles-35986533-2271-4cc0-85a3-c84ee301c43b/darima.zip/darima/dlsa.py:22) failed in 8.635 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 14:19:40 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/userFiles-35986533-2271-4cc0-85a3-c84ee301c43b/darima.zip/darima/dlsa.py:22, took 9.989200 s
20/12/04 14:19:40 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 14:19:40 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 14:19:41 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 14:19:41 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 14:19:41 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 14:19:41 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 14:19:41 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 14:19:41 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 14:19:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 14:19:41 INFO MemoryStore: MemoryStore cleared
20/12/04 14:19:41 INFO BlockManager: BlockManager stopped
20/12/04 14:19:41 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 14:19:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 14:19:41 INFO SparkContext: Successfully stopped SparkContext
20/12/04 14:19:41 INFO ShutdownHookManager: Shutdown hook called
20/12/04 14:19:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f
20/12/04 14:19:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-5bbd14e4-2068-4469-90ff-225d4858a412
20/12/04 14:19:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-19ae2136-7595-41d6-bdaa-e20a1bbfe55f/pyspark-ded41b28-5b45-48c1-a62b-f64e9a4a7bf9
20/12/04 14:19:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 14:19:44 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:19:44 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:19:44 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:19:44 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:19:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:19:45 INFO SparkContext: Running Spark version 3.0.1
20/12/04 14:19:45 INFO ResourceUtils: ==============================================================
20/12/04 14:19:45 INFO ResourceUtils: Resources for spark.driver:

20/12/04 14:19:45 INFO ResourceUtils: ==============================================================
20/12/04 14:19:45 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 14:19:45 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:19:45 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:19:45 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:19:45 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:19:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:19:45 INFO Utils: Successfully started service 'sparkDriver' on port 37391.
20/12/04 14:19:46 INFO SparkEnv: Registering MapOutputTracker
20/12/04 14:19:46 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 14:19:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 14:19:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 14:19:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 14:19:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b86b783d-6620-49e6-84c9-734131575cd3
20/12/04 14:19:46 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 14:19:46 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 14:19:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 14:19:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 14:19:47 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 14:19:47 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 14:19:47 INFO Configuration: resource-types.xml not found
20/12/04 14:19:47 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 14:19:47 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 14:19:47 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 14:19:47 INFO Client: Setting up container launch context for our AM
20/12/04 14:19:47 INFO Client: Setting up the launch environment for our AM container
20/12/04 14:19:47 INFO Client: Preparing resources for our AM container
20/12/04 14:19:47 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 14:19:49 INFO Client: Uploading resource file:/tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/__spark_libs__10363150676316104122.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0044/__spark_libs__10363150676316104122.zip
20/12/04 14:19:51 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0044/pyspark.zip
20/12/04 14:19:51 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0044/py4j-0.10.9-src.zip
20/12/04 14:19:51 INFO Client: Uploading resource file:/tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/__spark_conf__3087609651614921646.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0044/__spark_conf__.zip
20/12/04 14:19:51 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:19:51 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:19:51 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:19:51 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:19:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:19:52 INFO Client: Submitting application application_1607015337794_0044 to ResourceManager
20/12/04 14:19:52 INFO YarnClientImpl: Submitted application application_1607015337794_0044
20/12/04 14:19:53 INFO Client: Application report for application_1607015337794_0044 (state: ACCEPTED)
20/12/04 14:19:53 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607109592021
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0044/
	 user: bsuconn
20/12/04 14:19:54 INFO Client: Application report for application_1607015337794_0044 (state: ACCEPTED)
20/12/04 14:19:55 INFO Client: Application report for application_1607015337794_0044 (state: ACCEPTED)
20/12/04 14:19:56 INFO Client: Application report for application_1607015337794_0044 (state: ACCEPTED)
20/12/04 14:19:57 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0044), /proxy/application_1607015337794_0044
20/12/04 14:19:57 INFO Client: Application report for application_1607015337794_0044 (state: RUNNING)
20/12/04 14:19:57 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607109592021
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0044/
	 user: bsuconn
20/12/04 14:19:57 INFO YarnClientSchedulerBackend: Application application_1607015337794_0044 has started running.
20/12/04 14:19:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33501.
20/12/04 14:19:57 INFO NettyBlockTransferService: Server created on 192.168.1.9:33501
20/12/04 14:19:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 14:19:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 33501, None)
20/12/04 14:19:57 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:33501 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 33501, None)
20/12/04 14:19:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 33501, None)
20/12/04 14:19:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 33501, None)
20/12/04 14:19:57 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:19:57 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 14:20:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 14:20:02 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:37742) with ID 1
20/12/04 14:20:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40743 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 40743, None)
20/12/04 14:20:03 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:37746) with ID 2
20/12/04 14:20:03 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 14:20:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:44865 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 44865, None)
20/12/04 14:20:04 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 14:20:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 14:20:04 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 14:20:04 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:20:04 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:20:04 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:20:04 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:20:04 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:20:04 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:37391/files/darima.zip with timestamp 1607109604973
20/12/04 14:20:04 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/userFiles-fc7e9294-38b0-44d4-a6b9-732953b9c361/darima.zip
20/12/04 14:20:06 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
20/12/04 14:20:08 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
20/12/04 14:20:09 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:20:09 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:20:09 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:20:09 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:20:09 INFO CodeGenerator: Code generated in 306.21436 ms
20/12/04 14:20:10 INFO CodeGenerator: Code generated in 28.899188 ms
20/12/04 14:20:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:20:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:20:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:33501 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:20:10 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 14:20:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:20:10 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 14:20:10 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 14:20:10 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 14:20:10 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 14:20:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 14:20:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 14:20:10 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:20:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 14:20:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 14:20:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:33501 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:20:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 14:20:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:20:10 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 14:20:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:20:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:44865 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:20:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:44865 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:20:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3209 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 14:20:13 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 14:20:13 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.357 s
20/12/04 14:20:13 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:20:13 INFO DAGScheduler: running: Set()
20/12/04 14:20:13 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 14:20:13 INFO DAGScheduler: failed: Set()
20/12/04 14:20:13 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:20:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 14:20:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 14:20:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:33501 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:20:13 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 14:20:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:20:13 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 14:20:13 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:20:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:44865 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:20:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:37746
20/12/04 14:20:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 381 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 14:20:14 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.399 s
20/12/04 14:20:14 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 14:20:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 14:20:14 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 14:20:14 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.865675 s
20/12/04 14:20:16 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:33501 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:20:16 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:44865 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:20:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:33501 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:20:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:44865 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:20:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:44865 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:20:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:33501 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:20:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 14:20:17 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:20:17 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:20:17 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:20:17 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:20:17 INFO CodeGenerator: Code generated in 55.352027 ms
20/12/04 14:20:17 INFO CodeGenerator: Code generated in 15.36809 ms
20/12/04 14:20:17 INFO CodeGenerator: Code generated in 19.640601 ms
20/12/04 14:20:17 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:20:17 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:20:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:33501 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:20:17 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/userFiles-fc7e9294-38b0-44d4-a6b9-732953b9c361/darima.zip/darima/dlsa.py:22
20/12/04 14:20:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:20:17 INFO SparkContext: Starting job: toPandas at /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/userFiles-fc7e9294-38b0-44d4-a6b9-732953b9c361/darima.zip/darima/dlsa.py:22
20/12/04 14:20:17 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/userFiles-fc7e9294-38b0-44d4-a6b9-732953b9c361/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 14:20:17 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/userFiles-fc7e9294-38b0-44d4-a6b9-732953b9c361/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 14:20:17 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/userFiles-fc7e9294-38b0-44d4-a6b9-732953b9c361/darima.zip/darima/dlsa.py:22)
20/12/04 14:20:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 14:20:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 14:20:17 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/userFiles-fc7e9294-38b0-44d4-a6b9-732953b9c361/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:20:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 14:20:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 14:20:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:33501 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 14:20:17 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 14:20:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/userFiles-fc7e9294-38b0-44d4-a6b9-732953b9c361/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 14:20:17 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 14:20:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:20:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:44865 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 14:20:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:44865 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:20:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1257 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 14:20:19 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 14:20:19 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 36777
20/12/04 14:20:19 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/userFiles-fc7e9294-38b0-44d4-a6b9-732953b9c361/darima.zip/darima/dlsa.py:22) finished in 1.282 s
20/12/04 14:20:19 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:20:19 INFO DAGScheduler: running: Set()
20/12/04 14:20:19 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 14:20:19 INFO DAGScheduler: failed: Set()
20/12/04 14:20:19 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/userFiles-fc7e9294-38b0-44d4-a6b9-732953b9c361/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:20:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 14:20:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 14:20:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:33501 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 14:20:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 14:20:19 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/userFiles-fc7e9294-38b0-44d4-a6b9-732953b9c361/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 14:20:19 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 14:20:19 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:20:19 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:20:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:44865 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 14:20:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:37746
20/12/04 14:20:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:40743 (size: 131.5 KiB, free: 912.2 MiB)
20/12/04 14:20:22 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:37742
20/12/04 14:20:25 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:20:25 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 14:20:28 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 6, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:20:28 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 14:20:29 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 7, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:20:29 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 14:20:31 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 8, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:20:31 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 14:20:31 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:20:31 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 14:20:32 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 10, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:20:32 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 14:20:33 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 14:20:33 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 11, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:20:34 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 12, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:20:34 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 10) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/04 14:20:35 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 13, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:20:35 INFO TaskSetManager: Lost task 3.3 in stage 3.0 (TID 11) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/04 14:20:35 ERROR TaskSetManager: Task 3 in stage 3.0 failed 4 times; aborting job
20/12/04 14:20:35 INFO YarnScheduler: Cancelling stage 3
20/12/04 14:20:35 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 14:20:35 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 14:20:35 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/userFiles-fc7e9294-38b0-44d4-a6b9-732953b9c361/darima.zip/darima/dlsa.py:22) failed in 15.998 s due to Job aborted due to stage failure: Task 3 in stage 3.0 failed 4 times, most recent failure: Lost task 3.3 in stage 3.0 (TID 11, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 14:20:35 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/userFiles-fc7e9294-38b0-44d4-a6b9-732953b9c361/darima.zip/darima/dlsa.py:22, took 17.332817 s
20/12/04 14:20:35 WARN TaskSetManager: Lost task 2.2 in stage 3.0 (TID 13, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 14:20:35 WARN TaskSetManager: Lost task 0.3 in stage 3.0 (TID 12, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/04 14:20:35 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 14:20:35 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 14:20:35 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 14:20:36 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 14:20:36 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 14:20:36 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 14:20:36 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 14:20:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 14:20:36 INFO MemoryStore: MemoryStore cleared
20/12/04 14:20:36 INFO BlockManager: BlockManager stopped
20/12/04 14:20:36 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 14:20:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 14:20:36 INFO SparkContext: Successfully stopped SparkContext
20/12/04 14:20:36 INFO ShutdownHookManager: Shutdown hook called
20/12/04 14:20:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7
20/12/04 14:20:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-eac59d98-856b-40f2-8b04-9a82dbc30dd7/pyspark-802bdb8a-1d09-42c8-b127-588bdb997c6f
20/12/04 14:20:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-50d61de3-c339-4784-9414-f80c2026d27f
20/12/04 14:20:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 14:20:39 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:20:39 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:20:39 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:20:39 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:20:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:20:39 INFO SparkContext: Running Spark version 3.0.1
20/12/04 14:20:39 INFO ResourceUtils: ==============================================================
20/12/04 14:20:39 INFO ResourceUtils: Resources for spark.driver:

20/12/04 14:20:39 INFO ResourceUtils: ==============================================================
20/12/04 14:20:39 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 14:20:39 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:20:39 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:20:39 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:20:39 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:20:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:20:40 INFO Utils: Successfully started service 'sparkDriver' on port 45427.
20/12/04 14:20:40 INFO SparkEnv: Registering MapOutputTracker
20/12/04 14:20:40 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 14:20:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 14:20:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 14:20:40 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 14:20:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f2a197ea-73ba-46db-bbad-fb352c7d9933
20/12/04 14:20:40 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 14:20:40 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 14:20:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 14:20:40 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 14:20:41 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 14:20:41 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 14:20:42 INFO Configuration: resource-types.xml not found
20/12/04 14:20:42 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 14:20:42 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 14:20:42 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 14:20:42 INFO Client: Setting up container launch context for our AM
20/12/04 14:20:42 INFO Client: Setting up the launch environment for our AM container
20/12/04 14:20:42 INFO Client: Preparing resources for our AM container
20/12/04 14:20:42 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 14:20:44 INFO DiskBlockManager: Shutdown hook called
20/12/04 14:20:44 INFO ShutdownHookManager: Shutdown hook called
20/12/04 14:20:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4b170ca-0372-48a5-831f-306905e013fd/userFiles-afd3638d-c1d4-4b38-9f97-7bbc36d0d36b
20/12/04 14:20:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-5861cce4-dd65-4bea-a2de-c7a00ccd69b1
20/12/04 14:20:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4b170ca-0372-48a5-831f-306905e013fd
20/12/04 14:20:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 14:20:59 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:20:59 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:20:59 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:20:59 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:20:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:21:00 INFO SparkContext: Running Spark version 3.0.1
20/12/04 14:21:00 INFO ResourceUtils: ==============================================================
20/12/04 14:21:00 INFO ResourceUtils: Resources for spark.driver:

20/12/04 14:21:00 INFO ResourceUtils: ==============================================================
20/12/04 14:21:00 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 14:21:00 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:21:00 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:21:00 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:21:00 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:21:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:21:01 INFO Utils: Successfully started service 'sparkDriver' on port 42175.
20/12/04 14:21:01 INFO SparkEnv: Registering MapOutputTracker
20/12/04 14:21:01 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 14:21:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 14:21:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 14:21:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 14:21:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-59d24069-ebc2-4177-865b-7696fda77263
20/12/04 14:21:01 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 14:21:01 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 14:21:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 14:21:01 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 14:21:02 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 14:21:02 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 14:21:02 INFO Configuration: resource-types.xml not found
20/12/04 14:21:02 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 14:21:03 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 14:21:03 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 14:21:03 INFO Client: Setting up container launch context for our AM
20/12/04 14:21:03 INFO Client: Setting up the launch environment for our AM container
20/12/04 14:21:03 INFO Client: Preparing resources for our AM container
20/12/04 14:21:03 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 14:21:05 INFO Client: Uploading resource file:/tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/__spark_libs__1894225772783981922.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0046/__spark_libs__1894225772783981922.zip
20/12/04 14:21:06 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0046/pyspark.zip
20/12/04 14:21:06 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0046/py4j-0.10.9-src.zip
20/12/04 14:21:07 INFO Client: Uploading resource file:/tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/__spark_conf__10994605173897392708.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0046/__spark_conf__.zip
20/12/04 14:21:07 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:21:07 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:21:07 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:21:07 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:21:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:21:07 INFO Client: Submitting application application_1607015337794_0046 to ResourceManager
20/12/04 14:21:07 INFO YarnClientImpl: Submitted application application_1607015337794_0046
20/12/04 14:21:08 INFO Client: Application report for application_1607015337794_0046 (state: ACCEPTED)
20/12/04 14:21:08 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607109667373
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0046/
	 user: bsuconn
20/12/04 14:21:09 INFO Client: Application report for application_1607015337794_0046 (state: ACCEPTED)
20/12/04 14:21:10 INFO Client: Application report for application_1607015337794_0046 (state: ACCEPTED)
20/12/04 14:21:11 INFO Client: Application report for application_1607015337794_0046 (state: ACCEPTED)
20/12/04 14:21:12 INFO Client: Application report for application_1607015337794_0046 (state: ACCEPTED)
20/12/04 14:21:13 INFO Client: Application report for application_1607015337794_0046 (state: RUNNING)
20/12/04 14:21:13 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607109667373
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0046/
	 user: bsuconn
20/12/04 14:21:13 INFO YarnClientSchedulerBackend: Application application_1607015337794_0046 has started running.
20/12/04 14:21:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33303.
20/12/04 14:21:13 INFO NettyBlockTransferService: Server created on 192.168.1.9:33303
20/12/04 14:21:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 14:21:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 33303, None)
20/12/04 14:21:13 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:33303 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 33303, None)
20/12/04 14:21:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 33303, None)
20/12/04 14:21:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 33303, None)
20/12/04 14:21:13 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0046), /proxy/application_1607015337794_0046
20/12/04 14:21:14 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:21:14 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 14:21:18 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 14:21:19 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:43100) with ID 1
20/12/04 14:21:19 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 14:21:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:37361 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 37361, None)
20/12/04 14:21:19 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 14:21:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 14:21:19 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 14:21:19 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:21:19 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:21:19 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:21:19 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:21:19 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:21:20 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:42175/files/darima.zip with timestamp 1607109680439
20/12/04 14:21:20 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/userFiles-f54a36b8-cbb3-4316-bdeb-3e9a4d5161da/darima.zip
20/12/04 14:21:22 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
20/12/04 14:21:23 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
20/12/04 14:21:24 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:21:24 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:21:24 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:21:24 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:21:25 INFO CodeGenerator: Code generated in 275.591766 ms
20/12/04 14:21:25 INFO CodeGenerator: Code generated in 30.284624 ms
20/12/04 14:21:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:21:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:21:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:33303 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:21:25 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 14:21:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:21:25 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 14:21:25 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 14:21:25 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 14:21:25 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 14:21:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 14:21:25 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 14:21:25 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:21:25 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 14:21:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 14:21:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:33303 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:21:25 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 14:21:25 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:21:25 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 14:21:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:21:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:37361 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:21:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:37361 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:21:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3446 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:21:29 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 14:21:29 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.645 s
20/12/04 14:21:29 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:21:29 INFO DAGScheduler: running: Set()
20/12/04 14:21:29 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 14:21:29 INFO DAGScheduler: failed: Set()
20/12/04 14:21:29 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:21:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 14:21:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 14:21:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:33303 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:21:29 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 14:21:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:21:29 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 14:21:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:21:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:37361 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:21:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:43100
20/12/04 14:21:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 365 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:21:29 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 14:21:29 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.382 s
20/12/04 14:21:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 14:21:29 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 14:21:29 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 4.121540 s
20/12/04 14:21:31 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:37361 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:21:31 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:33303 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:21:31 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:33303 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:21:31 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:37361 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:21:32 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 14:21:33 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:21:33 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:21:33 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:21:33 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:21:33 INFO CodeGenerator: Code generated in 55.141099 ms
20/12/04 14:21:33 INFO CodeGenerator: Code generated in 20.18916 ms
20/12/04 14:21:33 INFO CodeGenerator: Code generated in 20.704816 ms
20/12/04 14:21:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:21:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:21:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:33303 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:21:33 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/userFiles-f54a36b8-cbb3-4316-bdeb-3e9a4d5161da/darima.zip/darima/dlsa.py:22
20/12/04 14:21:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:21:33 INFO SparkContext: Starting job: toPandas at /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/userFiles-f54a36b8-cbb3-4316-bdeb-3e9a4d5161da/darima.zip/darima/dlsa.py:22
20/12/04 14:21:33 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/userFiles-f54a36b8-cbb3-4316-bdeb-3e9a4d5161da/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 14:21:33 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/userFiles-f54a36b8-cbb3-4316-bdeb-3e9a4d5161da/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 14:21:33 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/userFiles-f54a36b8-cbb3-4316-bdeb-3e9a4d5161da/darima.zip/darima/dlsa.py:22)
20/12/04 14:21:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 14:21:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 14:21:33 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/userFiles-f54a36b8-cbb3-4316-bdeb-3e9a4d5161da/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:21:33 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.2 KiB, free 5.8 GiB)
20/12/04 14:21:33 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 14:21:33 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:33303 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 14:21:33 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 14:21:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/userFiles-f54a36b8-cbb3-4316-bdeb-3e9a4d5161da/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 14:21:33 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 14:21:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:21:33 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:37361 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 14:21:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:37361 (size: 28.7 KiB, free: 912.2 MiB)
20/12/04 14:21:34 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1300 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:21:34 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 14:21:34 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 48067
20/12/04 14:21:34 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/userFiles-f54a36b8-cbb3-4316-bdeb-3e9a4d5161da/darima.zip/darima/dlsa.py:22) finished in 1.325 s
20/12/04 14:21:34 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:21:34 INFO DAGScheduler: running: Set()
20/12/04 14:21:34 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 14:21:34 INFO DAGScheduler: failed: Set()
20/12/04 14:21:34 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/userFiles-f54a36b8-cbb3-4316-bdeb-3e9a4d5161da/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:21:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.0 KiB, free 5.8 GiB)
20/12/04 14:21:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.4 KiB, free 5.8 GiB)
20/12/04 14:21:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:33303 (size: 131.4 KiB, free: 5.8 GiB)
20/12/04 14:21:35 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 14:21:35 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/userFiles-f54a36b8-cbb3-4316-bdeb-3e9a4d5161da/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 14:21:35 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 14:21:35 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:21:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:37361 (size: 131.4 KiB, free: 912.1 MiB)
20/12/04 14:21:35 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:43100
20/12/04 14:21:38 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:21:38 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 14:21:39 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:21:39 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 14:21:40 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:21:40 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 14:21:41 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:21:41 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 14:21:42 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:21:42 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 14:21:42 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:21:42 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 14:21:43 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:21:43 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 14:21:43 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 14:21:43 INFO YarnScheduler: Cancelling stage 3
20/12/04 14:21:43 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 14:21:43 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 14:21:43 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/userFiles-f54a36b8-cbb3-4316-bdeb-3e9a4d5161da/darima.zip/darima/dlsa.py:22) failed in 8.649 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 14:21:43 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/userFiles-f54a36b8-cbb3-4316-bdeb-3e9a4d5161da/darima.zip/darima/dlsa.py:22, took 10.039643 s
20/12/04 14:21:43 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 14:21:43 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 14:21:44 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 14:21:44 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 14:21:44 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 14:21:44 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 14:21:44 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 14:21:44 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 14:21:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 14:21:44 INFO MemoryStore: MemoryStore cleared
20/12/04 14:21:44 INFO BlockManager: BlockManager stopped
20/12/04 14:21:44 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 14:21:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 14:21:44 INFO SparkContext: Successfully stopped SparkContext
20/12/04 14:21:44 INFO ShutdownHookManager: Shutdown hook called
20/12/04 14:21:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-0f69c6fd-e873-4d80-a123-7730d536e83f
20/12/04 14:21:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84
20/12/04 14:21:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-264647f5-097d-454c-aad3-0b2830c1eb84/pyspark-10d6e6d1-48b9-4bf1-87e2-fbcf35251639
20/12/04 14:21:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 14:21:47 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:21:47 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:21:47 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:21:47 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:21:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:21:47 INFO SparkContext: Running Spark version 3.0.1
20/12/04 14:21:47 INFO ResourceUtils: ==============================================================
20/12/04 14:21:47 INFO ResourceUtils: Resources for spark.driver:

20/12/04 14:21:47 INFO ResourceUtils: ==============================================================
20/12/04 14:21:47 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 14:21:47 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:21:47 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:21:47 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:21:47 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:21:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:21:48 INFO Utils: Successfully started service 'sparkDriver' on port 46229.
20/12/04 14:21:48 INFO SparkEnv: Registering MapOutputTracker
20/12/04 14:21:48 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 14:21:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 14:21:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 14:21:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 14:21:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4715bcfe-c7b5-48fc-bf91-0cda6eaa0c45
20/12/04 14:21:48 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 14:21:48 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 14:21:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 14:21:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 14:21:49 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 14:21:49 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 14:21:49 INFO Configuration: resource-types.xml not found
20/12/04 14:21:49 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 14:21:49 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 14:21:49 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 14:21:49 INFO Client: Setting up container launch context for our AM
20/12/04 14:21:49 INFO Client: Setting up the launch environment for our AM container
20/12/04 14:21:49 INFO Client: Preparing resources for our AM container
20/12/04 14:21:50 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 14:21:52 INFO Client: Uploading resource file:/tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/__spark_libs__10710538499091131052.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0047/__spark_libs__10710538499091131052.zip
20/12/04 14:21:53 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0047/pyspark.zip
20/12/04 14:21:53 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0047/py4j-0.10.9-src.zip
20/12/04 14:21:54 INFO Client: Uploading resource file:/tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/__spark_conf__1773961149035431979.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0047/__spark_conf__.zip
20/12/04 14:21:54 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:21:54 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:21:54 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:21:54 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:21:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:21:54 INFO Client: Submitting application application_1607015337794_0047 to ResourceManager
20/12/04 14:21:54 INFO YarnClientImpl: Submitted application application_1607015337794_0047
20/12/04 14:21:55 INFO Client: Application report for application_1607015337794_0047 (state: ACCEPTED)
20/12/04 14:21:55 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607109714333
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0047/
	 user: bsuconn
20/12/04 14:21:56 INFO Client: Application report for application_1607015337794_0047 (state: ACCEPTED)
20/12/04 14:21:57 INFO Client: Application report for application_1607015337794_0047 (state: ACCEPTED)
20/12/04 14:21:58 INFO Client: Application report for application_1607015337794_0047 (state: ACCEPTED)
20/12/04 14:21:59 INFO Client: Application report for application_1607015337794_0047 (state: RUNNING)
20/12/04 14:21:59 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607109714333
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0047/
	 user: bsuconn
20/12/04 14:21:59 INFO YarnClientSchedulerBackend: Application application_1607015337794_0047 has started running.
20/12/04 14:21:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46739.
20/12/04 14:21:59 INFO NettyBlockTransferService: Server created on 192.168.1.9:46739
20/12/04 14:21:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 14:21:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 46739, None)
20/12/04 14:21:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:46739 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 46739, None)
20/12/04 14:21:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 46739, None)
20/12/04 14:21:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 46739, None)
20/12/04 14:21:59 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0047), /proxy/application_1607015337794_0047
20/12/04 14:22:00 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 14:22:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 14:22:05 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:44896) with ID 1
20/12/04 14:22:05 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:38407 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 38407, None)
20/12/04 14:22:06 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:44900) with ID 2
20/12/04 14:22:06 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 14:22:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:38223 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 38223, None)
20/12/04 14:22:07 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 14:22:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 14:22:07 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 14:22:07 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:22:07 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:22:07 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:22:07 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:22:07 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:22:07 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:46229/files/darima.zip with timestamp 1607109727735
20/12/04 14:22:07 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/userFiles-8a2f71c8-f7bf-4dfb-8613-bf794bdcf14e/darima.zip
20/12/04 14:22:09 INFO InMemoryFileIndex: It took 60 ms to list leaf files for 1 paths.
20/12/04 14:22:11 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/12/04 14:22:11 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:22:11 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:22:11 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:22:11 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:22:12 INFO CodeGenerator: Code generated in 278.42964 ms
20/12/04 14:22:12 INFO CodeGenerator: Code generated in 31.962495 ms
20/12/04 14:22:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:22:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:22:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:46739 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:22:12 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 14:22:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:22:13 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 14:22:13 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 14:22:13 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 14:22:13 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 14:22:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 14:22:13 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 14:22:13 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:22:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 14:22:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 14:22:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:46739 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:22:13 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 14:22:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:22:13 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 14:22:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:22:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:38407 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:22:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:38407 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:22:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4136 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:22:17 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 14:22:17 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 4.342 s
20/12/04 14:22:17 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:22:17 INFO DAGScheduler: running: Set()
20/12/04 14:22:17 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 14:22:17 INFO DAGScheduler: failed: Set()
20/12/04 14:22:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:22:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 14:22:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 14:22:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:46739 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:22:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 14:22:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:22:17 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 14:22:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:22:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:38407 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:22:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:44896
20/12/04 14:22:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 378 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:22:17 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.396 s
20/12/04 14:22:17 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 14:22:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 14:22:17 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 14:22:17 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 4.834171 s
20/12/04 14:22:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:46739 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:22:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:38407 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:22:19 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:38407 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:22:19 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:46739 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:22:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 14:22:21 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:22:21 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:22:21 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:22:21 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:22:21 INFO CodeGenerator: Code generated in 27.945715 ms
20/12/04 14:22:21 INFO CodeGenerator: Code generated in 16.879621 ms
20/12/04 14:22:21 INFO CodeGenerator: Code generated in 45.175844 ms
20/12/04 14:22:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:22:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:22:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:46739 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:22:21 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/userFiles-8a2f71c8-f7bf-4dfb-8613-bf794bdcf14e/darima.zip/darima/dlsa.py:22
20/12/04 14:22:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:22:21 INFO SparkContext: Starting job: toPandas at /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/userFiles-8a2f71c8-f7bf-4dfb-8613-bf794bdcf14e/darima.zip/darima/dlsa.py:22
20/12/04 14:22:21 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/userFiles-8a2f71c8-f7bf-4dfb-8613-bf794bdcf14e/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 14:22:21 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/userFiles-8a2f71c8-f7bf-4dfb-8613-bf794bdcf14e/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 14:22:21 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/userFiles-8a2f71c8-f7bf-4dfb-8613-bf794bdcf14e/darima.zip/darima/dlsa.py:22)
20/12/04 14:22:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 14:22:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 14:22:21 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/userFiles-8a2f71c8-f7bf-4dfb-8613-bf794bdcf14e/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:22:21 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 14:22:21 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 14:22:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:46739 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 14:22:21 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 14:22:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/userFiles-8a2f71c8-f7bf-4dfb-8613-bf794bdcf14e/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 14:22:21 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 14:22:21 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:22:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:38223 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 14:22:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:38223 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:22:26 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 4221 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 14:22:26 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 14:22:26 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 48299
20/12/04 14:22:26 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/userFiles-8a2f71c8-f7bf-4dfb-8613-bf794bdcf14e/darima.zip/darima/dlsa.py:22) finished in 4.245 s
20/12/04 14:22:26 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:22:26 INFO DAGScheduler: running: Set()
20/12/04 14:22:26 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 14:22:26 INFO DAGScheduler: failed: Set()
20/12/04 14:22:26 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/userFiles-8a2f71c8-f7bf-4dfb-8613-bf794bdcf14e/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:22:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 14:22:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 14:22:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:46739 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 14:22:26 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 14:22:26 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/userFiles-8a2f71c8-f7bf-4dfb-8613-bf794bdcf14e/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 14:22:26 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 14:22:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:22:26 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:22:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:38407 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 14:22:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:38223 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 14:22:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:44896
20/12/04 14:22:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:44900
20/12/04 14:22:33 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:22:33 WARN TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 14:22:33 INFO TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 14:22:33 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 6, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:22:34 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:22:34 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 14:22:35 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 8, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:22:35 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 14:22:36 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:22:36 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 14:22:36 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 10, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:22:36 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 8) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 14:22:37 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 11, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:22:37 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 14:22:38 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 12, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:22:38 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 10) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/04 14:22:39 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:22:39 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 12) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/04 14:22:39 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 14:22:39 INFO YarnScheduler: Cancelling stage 3
20/12/04 14:22:39 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 14:22:39 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 14:22:39 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 11) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 9]
20/12/04 14:22:39 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/userFiles-8a2f71c8-f7bf-4dfb-8613-bf794bdcf14e/darima.zip/darima/dlsa.py:22) failed in 13.284 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 12, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 14:22:39 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/userFiles-8a2f71c8-f7bf-4dfb-8613-bf794bdcf14e/darima.zip/darima/dlsa.py:22, took 17.593541 s
20/12/04 14:22:39 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/04 14:22:39 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 14:22:39 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 14:22:39 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 14:22:39 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 14:22:39 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 14:22:39 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 14:22:39 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 14:22:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 14:22:39 INFO MemoryStore: MemoryStore cleared
20/12/04 14:22:40 INFO BlockManager: BlockManager stopped
20/12/04 14:22:40 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 14:22:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 14:22:40 INFO SparkContext: Successfully stopped SparkContext
20/12/04 14:22:40 INFO ShutdownHookManager: Shutdown hook called
20/12/04 14:22:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-0fe50eec-4f59-4a20-880f-41709b9b3111
20/12/04 14:22:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac
20/12/04 14:22:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-546ba39f-e9f7-475f-a26e-4752acecc3ac/pyspark-97fc4447-a03e-425a-bd8b-0765a6cb8693
20/12/04 14:22:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 14:22:42 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:22:42 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:22:42 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:22:42 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:22:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:22:43 INFO SparkContext: Running Spark version 3.0.1
20/12/04 14:22:43 INFO ResourceUtils: ==============================================================
20/12/04 14:22:43 INFO ResourceUtils: Resources for spark.driver:

20/12/04 14:22:43 INFO ResourceUtils: ==============================================================
20/12/04 14:22:43 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 14:22:43 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:22:43 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:22:43 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:22:43 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:22:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:22:43 INFO Utils: Successfully started service 'sparkDriver' on port 39739.
20/12/04 14:22:43 INFO SparkEnv: Registering MapOutputTracker
20/12/04 14:22:44 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 14:22:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 14:22:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 14:22:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 14:22:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0b1e3b3d-43dc-46b5-a79a-18cabfd51f8c
20/12/04 14:22:44 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 14:22:44 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 14:22:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 14:22:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 14:22:44 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 14:22:45 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 14:22:45 INFO Configuration: resource-types.xml not found
20/12/04 14:22:45 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 14:22:45 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 14:22:45 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 14:22:45 INFO Client: Setting up container launch context for our AM
20/12/04 14:22:45 INFO Client: Setting up the launch environment for our AM container
20/12/04 14:22:45 INFO Client: Preparing resources for our AM container
20/12/04 14:22:45 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 14:22:47 INFO Client: Uploading resource file:/tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/__spark_libs__15398028777996721548.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0048/__spark_libs__15398028777996721548.zip
20/12/04 14:22:48 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0048/pyspark.zip
20/12/04 14:22:48 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0048/py4j-0.10.9-src.zip
20/12/04 14:22:48 INFO Client: Uploading resource file:/tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/__spark_conf__13054429155057599957.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0048/__spark_conf__.zip
20/12/04 14:22:48 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:22:48 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:22:48 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:22:48 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:22:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:22:48 INFO Client: Submitting application application_1607015337794_0048 to ResourceManager
20/12/04 14:22:48 INFO YarnClientImpl: Submitted application application_1607015337794_0048
20/12/04 14:22:49 INFO Client: Application report for application_1607015337794_0048 (state: ACCEPTED)
20/12/04 14:22:50 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607109768972
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0048/
	 user: bsuconn
20/12/04 14:22:51 INFO Client: Application report for application_1607015337794_0048 (state: ACCEPTED)
20/12/04 14:22:52 INFO Client: Application report for application_1607015337794_0048 (state: ACCEPTED)
20/12/04 14:22:53 INFO Client: Application report for application_1607015337794_0048 (state: ACCEPTED)
20/12/04 14:22:54 INFO Client: Application report for application_1607015337794_0048 (state: RUNNING)
20/12/04 14:22:54 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607109768972
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0048/
	 user: bsuconn
20/12/04 14:22:54 INFO YarnClientSchedulerBackend: Application application_1607015337794_0048 has started running.
20/12/04 14:22:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45551.
20/12/04 14:22:54 INFO NettyBlockTransferService: Server created on 192.168.1.9:45551
20/12/04 14:22:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 14:22:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 45551, None)
20/12/04 14:22:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:45551 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 45551, None)
20/12/04 14:22:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 45551, None)
20/12/04 14:22:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 45551, None)
20/12/04 14:22:54 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0048), /proxy/application_1607015337794_0048
20/12/04 14:22:55 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 14:22:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 14:23:00 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:59370) with ID 1
20/12/04 14:23:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:44339 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 44339, None)
20/12/04 14:23:01 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:59376) with ID 2
20/12/04 14:23:01 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:35481 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 35481, None)
20/12/04 14:23:14 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
20/12/04 14:23:15 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 14:23:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 14:23:15 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 14:23:15 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:23:15 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:23:15 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:23:15 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:23:15 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:23:15 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:39739/files/darima.zip with timestamp 1607109795882
20/12/04 14:23:15 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/userFiles-8cb7def6-5794-4369-a7e5-664591aec597/darima.zip
20/12/04 14:23:17 INFO InMemoryFileIndex: It took 83 ms to list leaf files for 1 paths.
20/12/04 14:23:19 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/12/04 14:23:19 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:23:19 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:23:19 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:23:19 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:23:20 INFO CodeGenerator: Code generated in 357.103686 ms
20/12/04 14:23:20 INFO CodeGenerator: Code generated in 29.599369 ms
20/12/04 14:23:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:23:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:23:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:45551 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:23:20 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 14:23:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:23:21 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 14:23:21 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 14:23:21 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 14:23:21 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 14:23:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 14:23:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 14:23:21 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:23:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 14:23:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 14:23:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:45551 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:23:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 14:23:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:23:21 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 14:23:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:23:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:35481 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:23:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:35481 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:23:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3344 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 14:23:24 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 14:23:24 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.528 s
20/12/04 14:23:24 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:23:24 INFO DAGScheduler: running: Set()
20/12/04 14:23:24 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 14:23:24 INFO DAGScheduler: failed: Set()
20/12/04 14:23:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:23:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 14:23:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 14:23:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:45551 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:23:24 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 14:23:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:23:24 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 14:23:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:23:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:44339 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:23:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:59370
20/12/04 14:23:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1840 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:23:26 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 14:23:26 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 1.855 s
20/12/04 14:23:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 14:23:26 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 14:23:26 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 5.474991 s
20/12/04 14:23:28 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:35481 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:23:28 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:45551 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:23:29 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:45551 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:23:29 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:44339 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:23:29 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 14:23:29 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:23:29 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:23:29 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:23:29 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:23:30 INFO CodeGenerator: Code generated in 27.362509 ms
20/12/04 14:23:30 INFO CodeGenerator: Code generated in 17.772399 ms
20/12/04 14:23:30 INFO CodeGenerator: Code generated in 20.670753 ms
20/12/04 14:23:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:23:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:23:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:45551 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:23:30 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/userFiles-8cb7def6-5794-4369-a7e5-664591aec597/darima.zip/darima/dlsa.py:22
20/12/04 14:23:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:23:30 INFO SparkContext: Starting job: toPandas at /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/userFiles-8cb7def6-5794-4369-a7e5-664591aec597/darima.zip/darima/dlsa.py:22
20/12/04 14:23:30 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/userFiles-8cb7def6-5794-4369-a7e5-664591aec597/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 14:23:30 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/userFiles-8cb7def6-5794-4369-a7e5-664591aec597/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 14:23:30 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/userFiles-8cb7def6-5794-4369-a7e5-664591aec597/darima.zip/darima/dlsa.py:22)
20/12/04 14:23:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 14:23:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 14:23:30 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/userFiles-8cb7def6-5794-4369-a7e5-664591aec597/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:23:30 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 14:23:30 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 14:23:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:45551 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 14:23:30 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 14:23:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/userFiles-8cb7def6-5794-4369-a7e5-664591aec597/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 14:23:30 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 14:23:30 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:23:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:44339 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 14:23:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:44339 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:23:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2703 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:23:33 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 14:23:33 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 58105
20/12/04 14:23:33 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/userFiles-8cb7def6-5794-4369-a7e5-664591aec597/darima.zip/darima/dlsa.py:22) finished in 2.736 s
20/12/04 14:23:33 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:23:33 INFO DAGScheduler: running: Set()
20/12/04 14:23:33 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 14:23:33 INFO DAGScheduler: failed: Set()
20/12/04 14:23:33 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/userFiles-8cb7def6-5794-4369-a7e5-664591aec597/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:23:33 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 14:23:33 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 14:23:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:45551 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 14:23:33 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 14:23:33 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/userFiles-8cb7def6-5794-4369-a7e5-664591aec597/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 14:23:33 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 14:23:33 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:23:33 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:23:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:44339 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 14:23:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:35481 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 14:23:33 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:59370
20/12/04 14:23:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:59376
20/12/04 14:23:39 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:23:39 WARN TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 14:23:41 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:23:41 INFO TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 14:23:41 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:23:41 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 14:23:42 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 8, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:23:42 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 14:23:43 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:23:43 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 14:23:44 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 8) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 14:23:44 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 10, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:23:44 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 11, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:23:44 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 14:23:46 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 12, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:23:46 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 10) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/04 14:23:46 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 11) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/04 14:23:46 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 13, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:23:47 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 14, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:23:47 INFO TaskSetManager: Lost task 2.3 in stage 3.0 (TID 12) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 9]
20/12/04 14:23:47 ERROR TaskSetManager: Task 2 in stage 3.0 failed 4 times; aborting job
20/12/04 14:23:47 INFO YarnScheduler: Cancelling stage 3
20/12/04 14:23:47 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 14:23:47 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 14:23:47 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/userFiles-8cb7def6-5794-4369-a7e5-664591aec597/darima.zip/darima/dlsa.py:22) failed in 14.684 s due to Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 12, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 14:23:47 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/userFiles-8cb7def6-5794-4369-a7e5-664591aec597/darima.zip/darima/dlsa.py:22, took 17.526563 s
20/12/04 14:23:48 WARN TaskSetManager: Lost task 3.3 in stage 3.0 (TID 14, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/04 14:23:48 WARN TaskSetManager: Lost task 0.3 in stage 3.0 (TID 13, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 14:23:48 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 14:23:48 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 14:23:48 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 14:23:48 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 14:23:48 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 14:23:48 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 14:23:48 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 14:23:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 14:23:48 INFO MemoryStore: MemoryStore cleared
20/12/04 14:23:48 INFO BlockManager: BlockManager stopped
20/12/04 14:23:48 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 14:23:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 14:23:48 INFO SparkContext: Successfully stopped SparkContext
20/12/04 14:23:48 INFO ShutdownHookManager: Shutdown hook called
20/12/04 14:23:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65
20/12/04 14:23:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-b1e8561c-273c-46ae-97c8-043f938312ee
20/12/04 14:23:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-01ff1930-3905-45c6-9525-744116a25b65/pyspark-b2041cad-15c3-4c5f-8c63-21c944adfa81
20/12/04 14:26:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 14:26:33 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:26:33 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:26:33 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:26:33 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:26:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:26:34 INFO SparkContext: Running Spark version 3.0.1
20/12/04 14:26:34 INFO ResourceUtils: ==============================================================
20/12/04 14:26:34 INFO ResourceUtils: Resources for spark.driver:

20/12/04 14:26:34 INFO ResourceUtils: ==============================================================
20/12/04 14:26:34 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 14:26:34 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:26:34 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:26:34 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:26:34 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:26:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:26:34 INFO Utils: Successfully started service 'sparkDriver' on port 35239.
20/12/04 14:26:34 INFO SparkEnv: Registering MapOutputTracker
20/12/04 14:26:34 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 14:26:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 14:26:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 14:26:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 14:26:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-83c6bf8c-b866-465d-b04d-59a8edc5043c
20/12/04 14:26:35 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 14:26:35 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 14:26:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 14:26:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 14:26:35 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 14:26:36 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 14:26:36 INFO Configuration: resource-types.xml not found
20/12/04 14:26:36 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 14:26:36 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 14:26:36 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 14:26:36 INFO Client: Setting up container launch context for our AM
20/12/04 14:26:36 INFO Client: Setting up the launch environment for our AM container
20/12/04 14:26:36 INFO Client: Preparing resources for our AM container
20/12/04 14:26:36 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 14:26:39 INFO Client: Uploading resource file:/tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/__spark_libs__3346629874439069171.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0049/__spark_libs__3346629874439069171.zip
20/12/04 14:26:40 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0049/pyspark.zip
20/12/04 14:26:40 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0049/py4j-0.10.9-src.zip
20/12/04 14:26:40 INFO Client: Uploading resource file:/tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/__spark_conf__4720104968012569396.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0049/__spark_conf__.zip
20/12/04 14:26:41 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:26:41 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:26:41 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:26:41 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:26:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:26:41 INFO Client: Submitting application application_1607015337794_0049 to ResourceManager
20/12/04 14:26:41 INFO YarnClientImpl: Submitted application application_1607015337794_0049
20/12/04 14:26:42 INFO Client: Application report for application_1607015337794_0049 (state: ACCEPTED)
20/12/04 14:26:42 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607110001045
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0049/
	 user: bsuconn
20/12/04 14:26:43 INFO Client: Application report for application_1607015337794_0049 (state: ACCEPTED)
20/12/04 14:26:44 INFO Client: Application report for application_1607015337794_0049 (state: ACCEPTED)
20/12/04 14:26:45 INFO Client: Application report for application_1607015337794_0049 (state: ACCEPTED)
20/12/04 14:26:46 INFO Client: Application report for application_1607015337794_0049 (state: RUNNING)
20/12/04 14:26:46 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607110001045
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0049/
	 user: bsuconn
20/12/04 14:26:46 INFO YarnClientSchedulerBackend: Application application_1607015337794_0049 has started running.
20/12/04 14:26:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38559.
20/12/04 14:26:46 INFO NettyBlockTransferService: Server created on 192.168.1.9:38559
20/12/04 14:26:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 14:26:46 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0049), /proxy/application_1607015337794_0049
20/12/04 14:26:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 38559, None)
20/12/04 14:26:46 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:38559 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 38559, None)
20/12/04 14:26:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 38559, None)
20/12/04 14:26:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 38559, None)
20/12/04 14:26:46 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:26:47 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 14:26:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 14:26:51 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:48198) with ID 1
20/12/04 14:26:51 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 14:26:51 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:41013 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 41013, None)
20/12/04 14:26:51 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 14:26:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 14:26:51 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 14:26:51 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:26:51 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:26:51 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:26:51 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:26:51 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:26:52 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:35239/files/darima.zip with timestamp 1607110012300
20/12/04 14:26:52 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/userFiles-33c70789-fbdd-44e8-b8cc-231dac4d4321/darima.zip
20/12/04 14:26:54 INFO InMemoryFileIndex: It took 71 ms to list leaf files for 1 paths.
20/12/04 14:26:56 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/12/04 14:26:57 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:26:57 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:26:57 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:26:57 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:26:57 INFO CodeGenerator: Code generated in 270.492913 ms
20/12/04 14:26:58 INFO CodeGenerator: Code generated in 34.017804 ms
20/12/04 14:26:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:26:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:26:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:38559 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:26:58 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 14:26:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:26:58 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 14:26:58 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 14:26:58 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 14:26:58 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 14:26:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 14:26:58 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 14:26:58 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:26:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 14:26:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 14:26:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:38559 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:26:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 14:26:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:26:58 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 14:26:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:26:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:41013 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:27:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:41013 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:27:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3345 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:27:01 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 14:27:02 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.534 s
20/12/04 14:27:02 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:27:02 INFO DAGScheduler: running: Set()
20/12/04 14:27:02 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 14:27:02 INFO DAGScheduler: failed: Set()
20/12/04 14:27:02 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:27:02 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 14:27:02 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 14:27:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:38559 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:27:02 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 14:27:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:27:02 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 14:27:02 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:27:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:41013 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:27:02 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:48198
20/12/04 14:27:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 363 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:27:02 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 14:27:02 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.377 s
20/12/04 14:27:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 14:27:02 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 14:27:02 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.997855 s
20/12/04 14:27:04 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:38559 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:27:04 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:41013 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:27:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:38559 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:27:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:41013 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:27:05 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 14:27:05 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:27:05 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:27:05 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:27:05 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:27:06 INFO CodeGenerator: Code generated in 26.406069 ms
20/12/04 14:27:06 INFO CodeGenerator: Code generated in 17.80518 ms
20/12/04 14:27:06 INFO CodeGenerator: Code generated in 25.603783 ms
20/12/04 14:27:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:27:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:27:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:38559 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:27:06 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/userFiles-33c70789-fbdd-44e8-b8cc-231dac4d4321/darima.zip/darima/dlsa.py:22
20/12/04 14:27:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:27:06 INFO SparkContext: Starting job: toPandas at /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/userFiles-33c70789-fbdd-44e8-b8cc-231dac4d4321/darima.zip/darima/dlsa.py:22
20/12/04 14:27:06 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/userFiles-33c70789-fbdd-44e8-b8cc-231dac4d4321/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 14:27:06 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/userFiles-33c70789-fbdd-44e8-b8cc-231dac4d4321/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 14:27:06 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/userFiles-33c70789-fbdd-44e8-b8cc-231dac4d4321/darima.zip/darima/dlsa.py:22)
20/12/04 14:27:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 14:27:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 14:27:06 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/userFiles-33c70789-fbdd-44e8-b8cc-231dac4d4321/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:27:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 14:27:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 14:27:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:38559 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 14:27:06 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 14:27:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/userFiles-33c70789-fbdd-44e8-b8cc-231dac4d4321/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 14:27:06 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 14:27:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:27:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:41013 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 14:27:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:41013 (size: 28.7 KiB, free: 912.2 MiB)
20/12/04 14:27:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1370 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:27:07 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 14:27:07 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 51193
20/12/04 14:27:07 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/userFiles-33c70789-fbdd-44e8-b8cc-231dac4d4321/darima.zip/darima/dlsa.py:22) finished in 1.391 s
20/12/04 14:27:07 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:27:07 INFO DAGScheduler: running: Set()
20/12/04 14:27:07 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 14:27:07 INFO DAGScheduler: failed: Set()
20/12/04 14:27:07 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/userFiles-33c70789-fbdd-44e8-b8cc-231dac4d4321/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:27:07 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 14:27:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 14:27:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:38559 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 14:27:07 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 14:27:07 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/userFiles-33c70789-fbdd-44e8-b8cc-231dac4d4321/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 14:27:07 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 14:27:07 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:27:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:41013 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 14:27:08 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:48198
20/12/04 14:27:11 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:27:11 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 14:27:12 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:27:12 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 14:27:13 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:27:13 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 14:27:14 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:27:14 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 14:27:15 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:27:15 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 14:27:15 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:27:15 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 14:27:17 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:27:17 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 14:27:17 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 14:27:17 INFO YarnScheduler: Cancelling stage 3
20/12/04 14:27:17 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 14:27:17 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 14:27:17 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/userFiles-33c70789-fbdd-44e8-b8cc-231dac4d4321/darima.zip/darima/dlsa.py:22) failed in 9.337 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 14:27:17 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/userFiles-33c70789-fbdd-44e8-b8cc-231dac4d4321/darima.zip/darima/dlsa.py:22, took 10.773819 s
20/12/04 14:27:17 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 14:27:17 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 14:27:17 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 14:27:17 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 14:27:17 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 14:27:17 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 14:27:17 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 14:27:17 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 14:27:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 14:27:17 INFO MemoryStore: MemoryStore cleared
20/12/04 14:27:17 INFO BlockManager: BlockManager stopped
20/12/04 14:27:17 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 14:27:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 14:27:18 INFO SparkContext: Successfully stopped SparkContext
20/12/04 14:27:18 INFO ShutdownHookManager: Shutdown hook called
20/12/04 14:27:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd/pyspark-6b3e381f-1e3b-4f95-81ac-d3328c544551
20/12/04 14:27:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-4ced3f6a-fd1e-49e3-b659-1f28f2665acd
20/12/04 14:27:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-50f839b8-77ca-44a3-a3ee-28260e4cb568
20/12/04 14:27:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 14:27:20 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:27:20 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:27:20 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:27:20 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:27:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:27:21 INFO SparkContext: Running Spark version 3.0.1
20/12/04 14:27:21 INFO ResourceUtils: ==============================================================
20/12/04 14:27:21 INFO ResourceUtils: Resources for spark.driver:

20/12/04 14:27:21 INFO ResourceUtils: ==============================================================
20/12/04 14:27:21 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 14:27:21 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:27:21 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:27:21 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:27:21 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:27:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:27:21 INFO Utils: Successfully started service 'sparkDriver' on port 43133.
20/12/04 14:27:21 INFO SparkEnv: Registering MapOutputTracker
20/12/04 14:27:21 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 14:27:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 14:27:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 14:27:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 14:27:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f653db5b-c4ba-4e91-8eed-e41786af89ec
20/12/04 14:27:21 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 14:27:22 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 14:27:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 14:27:22 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 14:27:22 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 14:27:23 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 14:27:23 INFO Configuration: resource-types.xml not found
20/12/04 14:27:23 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 14:27:23 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 14:27:23 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 14:27:23 INFO Client: Setting up container launch context for our AM
20/12/04 14:27:23 INFO Client: Setting up the launch environment for our AM container
20/12/04 14:27:23 INFO Client: Preparing resources for our AM container
20/12/04 14:27:23 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 14:27:26 INFO Client: Uploading resource file:/tmp/spark-c2689ed7-e294-44cb-a77e-37fd2842ddc9/__spark_libs__10667095677734909440.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0050/__spark_libs__10667095677734909440.zip
20/12/04 14:27:27 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0050/pyspark.zip
20/12/04 14:27:28 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0050/py4j-0.10.9-src.zip
20/12/04 14:27:29 INFO Client: Uploading resource file:/tmp/spark-c2689ed7-e294-44cb-a77e-37fd2842ddc9/__spark_conf__2307817579406030029.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0050/__spark_conf__.zip
20/12/04 14:27:29 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:27:29 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:27:29 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:27:29 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:27:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:27:29 INFO Client: Submitting application application_1607015337794_0050 to ResourceManager
20/12/04 14:27:29 INFO YarnClientImpl: Submitted application application_1607015337794_0050
20/12/04 14:27:30 INFO Client: Application report for application_1607015337794_0050 (state: ACCEPTED)
20/12/04 14:27:30 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607110049144
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0050/
	 user: bsuconn
20/12/04 14:27:31 INFO Client: Application report for application_1607015337794_0050 (state: ACCEPTED)
20/12/04 14:27:32 INFO Client: Application report for application_1607015337794_0050 (state: ACCEPTED)
20/12/04 14:27:33 INFO Client: Application report for application_1607015337794_0050 (state: ACCEPTED)
20/12/04 14:27:34 INFO Client: Application report for application_1607015337794_0050 (state: ACCEPTED)
20/12/04 14:27:34 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0050), /proxy/application_1607015337794_0050
20/12/04 14:27:35 INFO Client: Application report for application_1607015337794_0050 (state: RUNNING)
20/12/04 14:27:35 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607110049144
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0050/
	 user: bsuconn
20/12/04 14:27:35 INFO YarnClientSchedulerBackend: Application application_1607015337794_0050 has started running.
20/12/04 14:27:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39255.
20/12/04 14:27:35 INFO NettyBlockTransferService: Server created on 192.168.1.9:39255
20/12/04 14:27:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 14:27:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 39255, None)
20/12/04 14:27:35 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:39255 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 39255, None)
20/12/04 14:27:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 39255, None)
20/12/04 14:27:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 39255, None)
20/12/04 14:27:35 INFO DiskBlockManager: Shutdown hook called
20/12/04 14:27:35 INFO ShutdownHookManager: Shutdown hook called
20/12/04 14:27:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-c2689ed7-e294-44cb-a77e-37fd2842ddc9
20/12/04 14:27:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-4ee39b4b-08f5-4ef0-9f9d-14130008562d
20/12/04 14:27:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-c2689ed7-e294-44cb-a77e-37fd2842ddc9/userFiles-b9f5a2c3-3150-4398-9406-5cb801b1ed14
20/12/04 14:44:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 14:44:45 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:44:45 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:44:45 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:44:45 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:44:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:44:46 INFO SparkContext: Running Spark version 3.0.1
20/12/04 14:44:46 INFO ResourceUtils: ==============================================================
20/12/04 14:44:46 INFO ResourceUtils: Resources for spark.driver:

20/12/04 14:44:46 INFO ResourceUtils: ==============================================================
20/12/04 14:44:46 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 14:44:46 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:44:46 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:44:46 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:44:46 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:44:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:44:46 INFO Utils: Successfully started service 'sparkDriver' on port 33653.
20/12/04 14:44:46 INFO SparkEnv: Registering MapOutputTracker
20/12/04 14:44:46 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 14:44:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 14:44:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 14:44:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 14:44:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-22c2b082-6d5b-4c42-bfa5-f86d1450f0fa
20/12/04 14:44:46 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 14:44:46 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 14:44:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 14:44:47 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 14:44:47 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 14:44:47 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 14:44:48 INFO Configuration: resource-types.xml not found
20/12/04 14:44:48 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 14:44:48 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 14:44:48 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 14:44:48 INFO Client: Setting up container launch context for our AM
20/12/04 14:44:48 INFO Client: Setting up the launch environment for our AM container
20/12/04 14:44:48 INFO Client: Preparing resources for our AM container
20/12/04 14:44:48 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 14:44:50 INFO Client: Uploading resource file:/tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/__spark_libs__14938677184310982945.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0051/__spark_libs__14938677184310982945.zip
20/12/04 14:44:51 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0051/pyspark.zip
20/12/04 14:44:51 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0051/py4j-0.10.9-src.zip
20/12/04 14:44:51 INFO Client: Uploading resource file:/tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/__spark_conf__6936332492300387161.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0051/__spark_conf__.zip
20/12/04 14:44:51 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:44:51 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:44:51 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:44:51 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:44:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:44:51 INFO Client: Submitting application application_1607015337794_0051 to ResourceManager
20/12/04 14:44:51 INFO YarnClientImpl: Submitted application application_1607015337794_0051
20/12/04 14:44:52 INFO Client: Application report for application_1607015337794_0051 (state: ACCEPTED)
20/12/04 14:44:52 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607111091911
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0051/
	 user: bsuconn
20/12/04 14:44:53 INFO Client: Application report for application_1607015337794_0051 (state: ACCEPTED)
20/12/04 14:44:54 INFO Client: Application report for application_1607015337794_0051 (state: ACCEPTED)
20/12/04 14:44:55 INFO Client: Application report for application_1607015337794_0051 (state: ACCEPTED)
20/12/04 14:44:56 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0051), /proxy/application_1607015337794_0051
20/12/04 14:44:56 INFO Client: Application report for application_1607015337794_0051 (state: RUNNING)
20/12/04 14:44:56 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607111091911
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0051/
	 user: bsuconn
20/12/04 14:44:56 INFO YarnClientSchedulerBackend: Application application_1607015337794_0051 has started running.
20/12/04 14:44:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43151.
20/12/04 14:44:56 INFO NettyBlockTransferService: Server created on 192.168.1.9:43151
20/12/04 14:44:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 14:44:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 43151, None)
20/12/04 14:44:57 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:43151 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 43151, None)
20/12/04 14:44:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 43151, None)
20/12/04 14:44:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 43151, None)
20/12/04 14:44:57 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:44:57 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 14:45:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 14:45:01 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:52128) with ID 1
20/12/04 14:45:01 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 14:45:01 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:45641 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 45641, None)
20/12/04 14:45:01 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 14:45:01 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 14:45:01 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 14:45:01 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:45:01 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:45:01 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:45:01 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:45:01 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:45:02 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:33653/files/darima.zip with timestamp 1607111102586
20/12/04 14:45:02 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/userFiles-dda4ce2b-f1bd-4a16-bc1d-a717d09158a0/darima.zip
20/12/04 14:45:04 INFO InMemoryFileIndex: It took 58 ms to list leaf files for 1 paths.
20/12/04 14:45:05 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/12/04 14:45:06 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:45:06 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:45:06 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:45:06 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:45:07 INFO CodeGenerator: Code generated in 346.564083 ms
20/12/04 14:45:07 INFO CodeGenerator: Code generated in 27.851006 ms
20/12/04 14:45:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:45:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:45:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:43151 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:45:07 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 14:45:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:45:07 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 14:45:07 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 14:45:07 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 14:45:07 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 14:45:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 14:45:07 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 14:45:07 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:45:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 14:45:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 14:45:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:43151 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:45:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 14:45:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:45:07 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 14:45:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:45:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:45641 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:45:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:45641 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:45:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3231 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:45:11 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 14:45:11 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.366 s
20/12/04 14:45:11 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:45:11 INFO DAGScheduler: running: Set()
20/12/04 14:45:11 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 14:45:11 INFO DAGScheduler: failed: Set()
20/12/04 14:45:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:45:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 14:45:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 14:45:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:43151 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:45:11 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 14:45:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:45:11 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 14:45:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:45:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:45641 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:45:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:52128
20/12/04 14:45:11 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 381 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:45:11 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 14:45:11 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.397 s
20/12/04 14:45:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 14:45:11 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 14:45:11 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.853647 s
20/12/04 14:45:13 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:45641 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:45:13 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:43151 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:45:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:43151 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:45:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:45641 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:45:13 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:43151 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:45:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:45641 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:45:14 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 14:45:14 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:45:14 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:45:14 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:45:14 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:45:15 INFO CodeGenerator: Code generated in 33.518729 ms
20/12/04 14:45:15 INFO CodeGenerator: Code generated in 17.503929 ms
20/12/04 14:45:15 INFO CodeGenerator: Code generated in 27.468927 ms
20/12/04 14:45:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:45:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:45:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:43151 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:45:15 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/userFiles-dda4ce2b-f1bd-4a16-bc1d-a717d09158a0/darima.zip/darima/dlsa.py:22
20/12/04 14:45:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:45:15 INFO SparkContext: Starting job: toPandas at /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/userFiles-dda4ce2b-f1bd-4a16-bc1d-a717d09158a0/darima.zip/darima/dlsa.py:22
20/12/04 14:45:15 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/userFiles-dda4ce2b-f1bd-4a16-bc1d-a717d09158a0/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 14:45:15 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/userFiles-dda4ce2b-f1bd-4a16-bc1d-a717d09158a0/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 14:45:15 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/userFiles-dda4ce2b-f1bd-4a16-bc1d-a717d09158a0/darima.zip/darima/dlsa.py:22)
20/12/04 14:45:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 14:45:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 14:45:15 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/userFiles-dda4ce2b-f1bd-4a16-bc1d-a717d09158a0/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:45:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 14:45:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 14:45:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:43151 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 14:45:15 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 14:45:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/userFiles-dda4ce2b-f1bd-4a16-bc1d-a717d09158a0/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 14:45:15 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 14:45:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:45:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:45641 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 14:45:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:45641 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:45:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1338 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:45:16 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 14:45:16 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 34035
20/12/04 14:45:16 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/userFiles-dda4ce2b-f1bd-4a16-bc1d-a717d09158a0/darima.zip/darima/dlsa.py:22) finished in 1.362 s
20/12/04 14:45:16 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:45:16 INFO DAGScheduler: running: Set()
20/12/04 14:45:16 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 14:45:16 INFO DAGScheduler: failed: Set()
20/12/04 14:45:16 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/userFiles-dda4ce2b-f1bd-4a16-bc1d-a717d09158a0/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:45:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 14:45:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 14:45:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:43151 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 14:45:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 14:45:16 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/userFiles-dda4ce2b-f1bd-4a16-bc1d-a717d09158a0/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 14:45:16 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 14:45:16 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:45:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:45641 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 14:45:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:52128
20/12/04 14:45:19 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:45:19 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 14:45:20 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:45:20 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 14:45:21 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:45:21 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 14:45:22 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:45:22 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 14:45:23 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:45:23 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 14:45:23 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:45:23 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 14:45:24 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:45:24 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 14:45:24 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 14:45:24 INFO YarnScheduler: Cancelling stage 3
20/12/04 14:45:24 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 14:45:24 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 14:45:24 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/userFiles-dda4ce2b-f1bd-4a16-bc1d-a717d09158a0/darima.zip/darima/dlsa.py:22) failed in 8.057 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 14:45:24 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/userFiles-dda4ce2b-f1bd-4a16-bc1d-a717d09158a0/darima.zip/darima/dlsa.py:22, took 9.480492 s
20/12/04 14:45:25 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 14:45:25 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 14:45:25 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 14:45:25 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 14:45:25 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 14:45:25 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 14:45:25 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 14:45:25 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 14:45:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 14:45:25 INFO MemoryStore: MemoryStore cleared
20/12/04 14:45:25 INFO BlockManager: BlockManager stopped
20/12/04 14:45:25 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 14:45:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 14:45:25 INFO SparkContext: Successfully stopped SparkContext
20/12/04 14:45:25 INFO ShutdownHookManager: Shutdown hook called
20/12/04 14:45:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653
20/12/04 14:45:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-d5c885cb-889d-40a2-a0ff-3e652c765653/pyspark-615a63a5-a5f4-4569-9036-b09b12769c6e
20/12/04 14:45:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-19909538-eda8-414a-8efb-bfe7ef3a4492
20/12/04 14:45:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 14:45:28 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:45:28 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:45:28 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:45:28 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:45:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:45:28 INFO SparkContext: Running Spark version 3.0.1
20/12/04 14:45:29 INFO ResourceUtils: ==============================================================
20/12/04 14:45:29 INFO ResourceUtils: Resources for spark.driver:

20/12/04 14:45:29 INFO ResourceUtils: ==============================================================
20/12/04 14:45:29 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 14:45:29 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:45:29 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:45:29 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:45:29 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:45:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:45:29 INFO Utils: Successfully started service 'sparkDriver' on port 42041.
20/12/04 14:45:29 INFO SparkEnv: Registering MapOutputTracker
20/12/04 14:45:29 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 14:45:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 14:45:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 14:45:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 14:45:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b8f4670c-1206-43fc-8f24-2d9203f79a1c
20/12/04 14:45:29 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 14:45:29 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 14:45:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 14:45:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 14:45:30 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 14:45:30 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 14:45:31 INFO Configuration: resource-types.xml not found
20/12/04 14:45:31 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 14:45:31 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 14:45:31 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 14:45:31 INFO Client: Setting up container launch context for our AM
20/12/04 14:45:31 INFO Client: Setting up the launch environment for our AM container
20/12/04 14:45:31 INFO Client: Preparing resources for our AM container
20/12/04 14:45:31 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 14:45:33 INFO Client: Uploading resource file:/tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/__spark_libs__15398563663135907831.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0052/__spark_libs__15398563663135907831.zip
20/12/04 14:45:34 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0052/pyspark.zip
20/12/04 14:45:34 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0052/py4j-0.10.9-src.zip
20/12/04 14:45:34 INFO Client: Uploading resource file:/tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/__spark_conf__10007412450118473130.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0052/__spark_conf__.zip
20/12/04 14:45:34 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:45:34 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:45:34 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:45:34 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:45:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:45:34 INFO Client: Submitting application application_1607015337794_0052 to ResourceManager
20/12/04 14:45:34 INFO YarnClientImpl: Submitted application application_1607015337794_0052
20/12/04 14:45:35 INFO Client: Application report for application_1607015337794_0052 (state: ACCEPTED)
20/12/04 14:45:35 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607111134906
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0052/
	 user: bsuconn
20/12/04 14:45:36 INFO Client: Application report for application_1607015337794_0052 (state: ACCEPTED)
20/12/04 14:45:37 INFO Client: Application report for application_1607015337794_0052 (state: ACCEPTED)
20/12/04 14:45:38 INFO Client: Application report for application_1607015337794_0052 (state: ACCEPTED)
20/12/04 14:45:39 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0052), /proxy/application_1607015337794_0052
20/12/04 14:45:39 INFO Client: Application report for application_1607015337794_0052 (state: RUNNING)
20/12/04 14:45:39 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607111134906
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0052/
	 user: bsuconn
20/12/04 14:45:39 INFO YarnClientSchedulerBackend: Application application_1607015337794_0052 has started running.
20/12/04 14:45:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34033.
20/12/04 14:45:39 INFO NettyBlockTransferService: Server created on 192.168.1.9:34033
20/12/04 14:45:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 14:45:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 34033, None)
20/12/04 14:45:39 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:34033 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 34033, None)
20/12/04 14:45:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 34033, None)
20/12/04 14:45:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 34033, None)
20/12/04 14:45:40 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:45:40 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 14:45:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 14:45:45 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:54750) with ID 1
20/12/04 14:45:45 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:37829 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 37829, None)
20/12/04 14:45:46 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:54754) with ID 2
20/12/04 14:45:46 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 14:45:47 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:34471 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 34471, None)
20/12/04 14:45:47 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 14:45:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 14:45:47 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 14:45:47 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:45:47 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:45:47 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:45:47 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:45:47 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:45:47 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:42041/files/darima.zip with timestamp 1607111147978
20/12/04 14:45:47 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/userFiles-a0bbfd9a-380b-490a-94d5-1ecda715630f/darima.zip
20/12/04 14:45:49 INFO InMemoryFileIndex: It took 66 ms to list leaf files for 1 paths.
20/12/04 14:45:51 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/12/04 14:45:51 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:45:51 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:45:51 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:45:51 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:45:52 INFO CodeGenerator: Code generated in 255.647073 ms
20/12/04 14:45:52 INFO CodeGenerator: Code generated in 31.733466 ms
20/12/04 14:45:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:45:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:45:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:34033 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:45:52 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 14:45:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:45:53 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 14:45:53 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 14:45:53 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 14:45:53 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 14:45:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 14:45:53 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 14:45:53 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:45:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 14:45:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 14:45:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:34033 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:45:53 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 14:45:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:45:53 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 14:45:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:45:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:34471 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:45:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:34471 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:45:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3119 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 14:45:56 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 14:45:56 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.284 s
20/12/04 14:45:56 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:45:56 INFO DAGScheduler: running: Set()
20/12/04 14:45:56 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 14:45:56 INFO DAGScheduler: failed: Set()
20/12/04 14:45:56 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:45:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 14:45:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 14:45:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:34033 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:45:56 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 14:45:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:45:56 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 14:45:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:45:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:34471 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:45:56 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:54754
20/12/04 14:45:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 319 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 14:45:56 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 14:45:56 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.334 s
20/12/04 14:45:56 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 14:45:56 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 14:45:56 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.715425 s
20/12/04 14:45:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:34471 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:45:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:34033 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:45:59 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:34471 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:45:59 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:34033 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:45:59 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 14:45:59 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:45:59 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:45:59 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:45:59 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:46:00 INFO CodeGenerator: Code generated in 31.743005 ms
20/12/04 14:46:00 INFO CodeGenerator: Code generated in 16.15074 ms
20/12/04 14:46:00 INFO CodeGenerator: Code generated in 19.777999 ms
20/12/04 14:46:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:46:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:46:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:34033 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:46:00 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/userFiles-a0bbfd9a-380b-490a-94d5-1ecda715630f/darima.zip/darima/dlsa.py:22
20/12/04 14:46:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:46:00 INFO SparkContext: Starting job: toPandas at /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/userFiles-a0bbfd9a-380b-490a-94d5-1ecda715630f/darima.zip/darima/dlsa.py:22
20/12/04 14:46:00 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/userFiles-a0bbfd9a-380b-490a-94d5-1ecda715630f/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 14:46:00 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/userFiles-a0bbfd9a-380b-490a-94d5-1ecda715630f/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 14:46:00 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/userFiles-a0bbfd9a-380b-490a-94d5-1ecda715630f/darima.zip/darima/dlsa.py:22)
20/12/04 14:46:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 14:46:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 14:46:00 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/userFiles-a0bbfd9a-380b-490a-94d5-1ecda715630f/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:46:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 14:46:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 14:46:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:34033 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 14:46:00 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 14:46:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/userFiles-a0bbfd9a-380b-490a-94d5-1ecda715630f/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 14:46:00 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 14:46:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:46:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:34471 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 14:46:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:34471 (size: 28.7 KiB, free: 912.2 MiB)
20/12/04 14:46:01 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1312 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 14:46:01 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 44659
20/12/04 14:46:01 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 14:46:01 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/userFiles-a0bbfd9a-380b-490a-94d5-1ecda715630f/darima.zip/darima/dlsa.py:22) finished in 1.334 s
20/12/04 14:46:01 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:46:01 INFO DAGScheduler: running: Set()
20/12/04 14:46:01 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 14:46:01 INFO DAGScheduler: failed: Set()
20/12/04 14:46:01 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/userFiles-a0bbfd9a-380b-490a-94d5-1ecda715630f/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:46:01 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 14:46:01 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 14:46:01 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:34033 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 14:46:01 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 14:46:01 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/userFiles-a0bbfd9a-380b-490a-94d5-1ecda715630f/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 14:46:01 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 14:46:01 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:46:01 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:46:01 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:34471 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 14:46:02 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:54754
20/12/04 14:46:02 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:37829 (size: 131.5 KiB, free: 912.2 MiB)
20/12/04 14:46:05 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:54750
20/12/04 14:46:07 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:46:07 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 14:46:09 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 6, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:46:09 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 14:46:10 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 7, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:46:10 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 14:46:12 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 8, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:46:12 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 14:46:12 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:46:12 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 14:46:13 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 10, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:46:13 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 14:46:14 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 11, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:46:14 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 14:46:14 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 12, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:46:14 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 10) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/04 14:46:15 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 13, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:46:15 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 11) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/04 14:46:15 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 14:46:15 INFO YarnScheduler: Cancelling stage 3
20/12/04 14:46:15 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 14:46:15 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 14:46:15 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/userFiles-a0bbfd9a-380b-490a-94d5-1ecda715630f/darima.zip/darima/dlsa.py:22) failed in 13.996 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 11, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 14:46:15 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/userFiles-a0bbfd9a-380b-490a-94d5-1ecda715630f/darima.zip/darima/dlsa.py:22, took 15.388696 s
20/12/04 14:46:15 WARN TaskSetManager: Lost task 2.2 in stage 3.0 (TID 13, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 14:46:16 WARN TaskSetManager: Lost task 3.3 in stage 3.0 (TID 12, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/04 14:46:16 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 14:46:16 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 14:46:16 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 14:46:16 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 14:46:16 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 14:46:16 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 14:46:16 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 14:46:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 14:46:16 INFO MemoryStore: MemoryStore cleared
20/12/04 14:46:16 INFO BlockManager: BlockManager stopped
20/12/04 14:46:16 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 14:46:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 14:46:16 INFO SparkContext: Successfully stopped SparkContext
20/12/04 14:46:16 INFO ShutdownHookManager: Shutdown hook called
20/12/04 14:46:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443
20/12/04 14:46:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-b56e8b0a-d563-4d04-9279-2f1288696cd1
20/12/04 14:46:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-a763d9a4-5055-49fa-a463-a9cbc105d443/pyspark-28af413a-0871-4d8a-944f-821999b69bf1
20/12/04 14:46:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 14:46:19 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:46:19 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:46:19 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:46:19 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:46:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:46:20 INFO SparkContext: Running Spark version 3.0.1
20/12/04 14:46:20 INFO ResourceUtils: ==============================================================
20/12/04 14:46:20 INFO ResourceUtils: Resources for spark.driver:

20/12/04 14:46:20 INFO ResourceUtils: ==============================================================
20/12/04 14:46:20 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 14:46:20 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:46:20 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:46:20 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:46:20 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:46:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:46:20 INFO Utils: Successfully started service 'sparkDriver' on port 40755.
20/12/04 14:46:20 INFO SparkEnv: Registering MapOutputTracker
20/12/04 14:46:20 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 14:46:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 14:46:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 14:46:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 14:46:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-61ffc176-0f39-412c-8e47-362eb3ece501
20/12/04 14:46:21 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 14:46:21 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 14:46:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 14:46:21 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 14:46:21 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 14:46:22 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 14:46:22 INFO Configuration: resource-types.xml not found
20/12/04 14:46:22 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 14:46:22 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 14:46:22 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 14:46:22 INFO Client: Setting up container launch context for our AM
20/12/04 14:46:22 INFO Client: Setting up the launch environment for our AM container
20/12/04 14:46:22 INFO Client: Preparing resources for our AM container
20/12/04 14:46:22 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 14:46:24 INFO Client: Uploading resource file:/tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/__spark_libs__10562209182310765508.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0053/__spark_libs__10562209182310765508.zip
20/12/04 14:46:25 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0053/pyspark.zip
20/12/04 14:46:25 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0053/py4j-0.10.9-src.zip
20/12/04 14:46:25 INFO Client: Uploading resource file:/tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/__spark_conf__480948645623196515.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0053/__spark_conf__.zip
20/12/04 14:46:26 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 14:46:26 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 14:46:26 INFO SecurityManager: Changing view acls groups to: 
20/12/04 14:46:26 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 14:46:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 14:46:26 INFO Client: Submitting application application_1607015337794_0053 to ResourceManager
20/12/04 14:46:26 INFO YarnClientImpl: Submitted application application_1607015337794_0053
20/12/04 14:46:27 INFO Client: Application report for application_1607015337794_0053 (state: ACCEPTED)
20/12/04 14:46:27 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607111186364
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0053/
	 user: bsuconn
20/12/04 14:46:28 INFO Client: Application report for application_1607015337794_0053 (state: ACCEPTED)
20/12/04 14:46:29 INFO Client: Application report for application_1607015337794_0053 (state: ACCEPTED)
20/12/04 14:46:30 INFO Client: Application report for application_1607015337794_0053 (state: ACCEPTED)
20/12/04 14:46:31 INFO Client: Application report for application_1607015337794_0053 (state: ACCEPTED)
20/12/04 14:46:31 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0053), /proxy/application_1607015337794_0053
20/12/04 14:46:32 INFO Client: Application report for application_1607015337794_0053 (state: RUNNING)
20/12/04 14:46:32 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607111186364
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0053/
	 user: bsuconn
20/12/04 14:46:32 INFO YarnClientSchedulerBackend: Application application_1607015337794_0053 has started running.
20/12/04 14:46:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45743.
20/12/04 14:46:32 INFO NettyBlockTransferService: Server created on 192.168.1.9:45743
20/12/04 14:46:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 14:46:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 45743, None)
20/12/04 14:46:32 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:45743 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 45743, None)
20/12/04 14:46:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 45743, None)
20/12/04 14:46:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 45743, None)
20/12/04 14:46:32 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 14:46:32 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:46:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 14:46:37 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:42882) with ID 1
20/12/04 14:46:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:44487 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 44487, None)
20/12/04 14:46:38 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:42886) with ID 2
20/12/04 14:46:38 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:33703 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 33703, None)
20/12/04 14:46:51 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
20/12/04 14:46:52 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 14:46:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 14:46:52 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 14:46:52 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:46:52 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:46:52 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:46:52 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:46:52 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 14:46:52 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:40755/files/darima.zip with timestamp 1607111212799
20/12/04 14:46:52 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/userFiles-f5da6627-9401-4c28-8404-188f56b4690a/darima.zip
20/12/04 14:46:54 INFO InMemoryFileIndex: It took 54 ms to list leaf files for 1 paths.
20/12/04 14:46:56 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/12/04 14:46:56 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:46:56 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:46:56 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:46:56 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:46:57 INFO CodeGenerator: Code generated in 282.272739 ms
20/12/04 14:46:57 INFO CodeGenerator: Code generated in 33.320891 ms
20/12/04 14:46:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:46:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:46:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:45743 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:46:57 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 14:46:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:46:57 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 14:46:57 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 14:46:57 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 14:46:57 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 14:46:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 14:46:57 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 14:46:58 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:46:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 14:46:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 14:46:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:45743 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:46:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 14:46:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:46:58 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 14:46:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:46:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:44487 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:46:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:44487 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:47:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3113 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:47:01 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 14:47:01 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.251 s
20/12/04 14:47:01 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:47:01 INFO DAGScheduler: running: Set()
20/12/04 14:47:01 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 14:47:01 INFO DAGScheduler: failed: Set()
20/12/04 14:47:01 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 14:47:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 14:47:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 14:47:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:45743 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:47:01 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 14:47:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 14:47:01 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 14:47:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:47:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:44487 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:47:01 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:42882
20/12/04 14:47:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 382 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 14:47:01 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 14:47:01 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.399 s
20/12/04 14:47:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 14:47:01 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 14:47:01 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.736164 s
20/12/04 14:47:03 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:44487 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:47:03 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:45743 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:47:03 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:45743 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 14:47:03 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:44487 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 14:47:03 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:45743 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 14:47:03 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:44487 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 14:47:04 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 14:47:04 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 14:47:04 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 14:47:04 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 14:47:04 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 14:47:05 INFO CodeGenerator: Code generated in 32.708365 ms
20/12/04 14:47:05 INFO CodeGenerator: Code generated in 14.178267 ms
20/12/04 14:47:05 INFO CodeGenerator: Code generated in 28.963928 ms
20/12/04 14:47:05 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 14:47:05 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 14:47:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:45743 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 14:47:05 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/userFiles-f5da6627-9401-4c28-8404-188f56b4690a/darima.zip/darima/dlsa.py:22
20/12/04 14:47:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 14:47:05 INFO SparkContext: Starting job: toPandas at /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/userFiles-f5da6627-9401-4c28-8404-188f56b4690a/darima.zip/darima/dlsa.py:22
20/12/04 14:47:05 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/userFiles-f5da6627-9401-4c28-8404-188f56b4690a/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 14:47:05 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/userFiles-f5da6627-9401-4c28-8404-188f56b4690a/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 14:47:05 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/userFiles-f5da6627-9401-4c28-8404-188f56b4690a/darima.zip/darima/dlsa.py:22)
20/12/04 14:47:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 14:47:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 14:47:05 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/userFiles-f5da6627-9401-4c28-8404-188f56b4690a/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:47:05 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 14:47:05 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 14:47:05 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:45743 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 14:47:05 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 14:47:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/userFiles-f5da6627-9401-4c28-8404-188f56b4690a/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 14:47:05 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 14:47:05 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 14:47:05 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:33703 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 14:47:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:33703 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 14:47:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 4154 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 14:47:09 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 14:47:09 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 43335
20/12/04 14:47:09 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/userFiles-f5da6627-9401-4c28-8404-188f56b4690a/darima.zip/darima/dlsa.py:22) finished in 4.179 s
20/12/04 14:47:09 INFO DAGScheduler: looking for newly runnable stages
20/12/04 14:47:09 INFO DAGScheduler: running: Set()
20/12/04 14:47:09 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 14:47:09 INFO DAGScheduler: failed: Set()
20/12/04 14:47:09 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/userFiles-f5da6627-9401-4c28-8404-188f56b4690a/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 14:47:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 14:47:09 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 14:47:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:45743 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 14:47:09 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 14:47:09 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/userFiles-f5da6627-9401-4c28-8404-188f56b4690a/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 14:47:09 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 14:47:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:47:09 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:47:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:44487 (size: 131.5 KiB, free: 912.2 MiB)
20/12/04 14:47:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:33703 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 14:47:10 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:42882
20/12/04 14:47:10 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:42886
20/12/04 14:47:16 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:47:16 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 14:47:16 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:47:16 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 14:47:17 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 7, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:47:17 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 14:47:18 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:47:18 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 14:47:19 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 9, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:47:19 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 14:47:19 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:47:19 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 14:47:20 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 11, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:47:20 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 9) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 14:47:21 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 14:47:21 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 10) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/04 14:47:21 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 14:47:21 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 11) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/04 14:47:22 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 14, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 14:47:22 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 12) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 9]
20/12/04 14:47:22 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 14:47:22 INFO YarnScheduler: Cancelling stage 3
20/12/04 14:47:22 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 14:47:22 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 14:47:22 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/userFiles-f5da6627-9401-4c28-8404-188f56b4690a/darima.zip/darima/dlsa.py:22) failed in 13.161 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 14:47:22 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/userFiles-f5da6627-9401-4c28-8404-188f56b4690a/darima.zip/darima/dlsa.py:22, took 17.404422 s
20/12/04 14:47:22 WARN TaskSetManager: Lost task 3.3 in stage 3.0 (TID 14, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 14:47:23 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/04 14:47:23 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 14:47:23 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 14:47:23 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 14:47:23 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 14:47:23 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 14:47:23 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 14:47:23 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 14:47:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 14:47:23 INFO MemoryStore: MemoryStore cleared
20/12/04 14:47:23 INFO BlockManager: BlockManager stopped
20/12/04 14:47:23 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 14:47:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 14:47:23 INFO SparkContext: Successfully stopped SparkContext
20/12/04 14:47:23 INFO ShutdownHookManager: Shutdown hook called
20/12/04 14:47:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc
20/12/04 14:47:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-b79d78ba-7914-41ff-91c5-ba8eaad389b0
20/12/04 14:47:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-d136b53c-89ae-4f88-8c69-63623fde58bc/pyspark-694bc3b2-2062-4c88-86b1-c0463f5bbc68
20/12/04 15:23:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 15:23:22 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:23:22 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:23:22 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:23:22 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:23:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:23:23 INFO SparkContext: Running Spark version 3.0.1
20/12/04 15:23:23 INFO ResourceUtils: ==============================================================
20/12/04 15:23:23 INFO ResourceUtils: Resources for spark.driver:

20/12/04 15:23:23 INFO ResourceUtils: ==============================================================
20/12/04 15:23:23 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 15:23:23 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:23:23 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:23:23 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:23:23 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:23:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:23:23 INFO Utils: Successfully started service 'sparkDriver' on port 44045.
20/12/04 15:23:24 INFO SparkEnv: Registering MapOutputTracker
20/12/04 15:23:24 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 15:23:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 15:23:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 15:23:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 15:23:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6997933b-d27d-46ac-b3a7-1a68dfa28617
20/12/04 15:23:24 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 15:23:24 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 15:23:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 15:23:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 15:23:25 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 15:23:25 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 15:23:25 INFO Configuration: resource-types.xml not found
20/12/04 15:23:25 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 15:23:25 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 15:23:25 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 15:23:25 INFO Client: Setting up container launch context for our AM
20/12/04 15:23:25 INFO Client: Setting up the launch environment for our AM container
20/12/04 15:23:25 INFO Client: Preparing resources for our AM container
20/12/04 15:23:25 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 15:23:27 INFO Client: Uploading resource file:/tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/__spark_libs__5822059244624148481.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0054/__spark_libs__5822059244624148481.zip
20/12/04 15:23:28 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0054/pyspark.zip
20/12/04 15:23:28 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0054/py4j-0.10.9-src.zip
20/12/04 15:23:28 INFO Client: Uploading resource file:/tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/__spark_conf__5148543685866884663.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0054/__spark_conf__.zip
20/12/04 15:23:28 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:23:28 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:23:28 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:23:28 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:23:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:23:28 INFO Client: Submitting application application_1607015337794_0054 to ResourceManager
20/12/04 15:23:29 INFO YarnClientImpl: Submitted application application_1607015337794_0054
20/12/04 15:23:30 INFO Client: Application report for application_1607015337794_0054 (state: ACCEPTED)
20/12/04 15:23:30 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607113409006
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0054/
	 user: bsuconn
20/12/04 15:23:31 INFO Client: Application report for application_1607015337794_0054 (state: ACCEPTED)
20/12/04 15:23:32 INFO Client: Application report for application_1607015337794_0054 (state: ACCEPTED)
20/12/04 15:23:33 INFO Client: Application report for application_1607015337794_0054 (state: ACCEPTED)
20/12/04 15:23:33 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0054), /proxy/application_1607015337794_0054
20/12/04 15:23:34 INFO Client: Application report for application_1607015337794_0054 (state: RUNNING)
20/12/04 15:23:34 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607113409006
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0054/
	 user: bsuconn
20/12/04 15:23:34 INFO YarnClientSchedulerBackend: Application application_1607015337794_0054 has started running.
20/12/04 15:23:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40779.
20/12/04 15:23:34 INFO NettyBlockTransferService: Server created on 192.168.1.9:40779
20/12/04 15:23:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 15:23:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 40779, None)
20/12/04 15:23:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40779 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 40779, None)
20/12/04 15:23:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 40779, None)
20/12/04 15:23:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 40779, None)
20/12/04 15:23:34 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:23:34 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 15:23:37 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 15:23:38 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:45972) with ID 1
20/12/04 15:23:38 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 15:23:38 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:37721 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 37721, None)
20/12/04 15:23:38 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 15:23:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 15:23:39 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 15:23:39 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:23:39 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:23:39 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:23:39 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:23:39 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:23:39 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:44045/files/darima.zip with timestamp 1607113419817
20/12/04 15:23:39 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/userFiles-dd7fcc93-bb90-4c5e-9761-9a313fc19980/darima.zip
20/12/04 15:23:41 INFO InMemoryFileIndex: It took 69 ms to list leaf files for 1 paths.
20/12/04 15:23:43 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
20/12/04 15:23:43 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:23:43 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:23:43 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:23:43 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:23:44 INFO CodeGenerator: Code generated in 254.339931 ms
20/12/04 15:23:44 INFO CodeGenerator: Code generated in 29.19437 ms
20/12/04 15:23:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:23:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:23:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:40779 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:23:44 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 15:23:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:23:44 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 15:23:44 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 15:23:44 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 15:23:44 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 15:23:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 15:23:44 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 15:23:44 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:23:44 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 15:23:44 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 15:23:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:40779 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:23:44 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 15:23:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:23:45 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 15:23:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:23:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:37721 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:23:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:37721 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:23:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3270 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:23:48 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 15:23:48 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.419 s
20/12/04 15:23:48 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:23:48 INFO DAGScheduler: running: Set()
20/12/04 15:23:48 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 15:23:48 INFO DAGScheduler: failed: Set()
20/12/04 15:23:48 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:23:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 15:23:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 15:23:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:40779 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:23:48 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 15:23:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:23:48 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 15:23:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:23:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:37721 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:23:48 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:45972
20/12/04 15:23:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 316 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:23:48 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 15:23:48 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.330 s
20/12/04 15:23:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 15:23:48 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 15:23:48 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.850815 s
20/12/04 15:23:50 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:40779 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:23:50 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:37721 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:23:50 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 15:23:50 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:40779 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:23:50 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:37721 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:23:51 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:23:51 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:23:51 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:23:51 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:23:51 INFO CodeGenerator: Code generated in 28.901625 ms
20/12/04 15:23:52 INFO CodeGenerator: Code generated in 21.184418 ms
20/12/04 15:23:52 INFO CodeGenerator: Code generated in 20.782602 ms
20/12/04 15:23:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:23:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:23:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:40779 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:23:52 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/userFiles-dd7fcc93-bb90-4c5e-9761-9a313fc19980/darima.zip/darima/dlsa.py:22
20/12/04 15:23:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:23:52 INFO SparkContext: Starting job: toPandas at /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/userFiles-dd7fcc93-bb90-4c5e-9761-9a313fc19980/darima.zip/darima/dlsa.py:22
20/12/04 15:23:52 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/userFiles-dd7fcc93-bb90-4c5e-9761-9a313fc19980/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 15:23:52 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/userFiles-dd7fcc93-bb90-4c5e-9761-9a313fc19980/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 15:23:52 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/userFiles-dd7fcc93-bb90-4c5e-9761-9a313fc19980/darima.zip/darima/dlsa.py:22)
20/12/04 15:23:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 15:23:52 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 15:23:52 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/userFiles-dd7fcc93-bb90-4c5e-9761-9a313fc19980/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:23:52 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 15:23:52 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 15:23:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:40779 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 15:23:52 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 15:23:52 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/userFiles-dd7fcc93-bb90-4c5e-9761-9a313fc19980/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 15:23:52 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 15:23:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:23:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:37721 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 15:23:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:37721 (size: 28.7 KiB, free: 912.2 MiB)
20/12/04 15:23:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1286 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:23:53 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 15:23:53 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 37475
20/12/04 15:23:53 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/userFiles-dd7fcc93-bb90-4c5e-9761-9a313fc19980/darima.zip/darima/dlsa.py:22) finished in 1.310 s
20/12/04 15:23:53 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:23:53 INFO DAGScheduler: running: Set()
20/12/04 15:23:53 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 15:23:53 INFO DAGScheduler: failed: Set()
20/12/04 15:23:53 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/userFiles-dd7fcc93-bb90-4c5e-9761-9a313fc19980/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:23:53 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 15:23:53 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 15:23:53 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:40779 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 15:23:53 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 15:23:53 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/userFiles-dd7fcc93-bb90-4c5e-9761-9a313fc19980/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 15:23:53 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 15:23:53 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:23:53 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:37721 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 15:23:53 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:45972
20/12/04 15:23:56 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:23:56 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 15:23:57 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:23:57 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 15:23:58 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:23:58 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 15:23:59 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:23:59 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 15:24:00 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:00 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 15:24:00 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:00 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 15:24:01 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:01 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 15:24:01 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 15:24:01 INFO YarnScheduler: Cancelling stage 3
20/12/04 15:24:01 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 15:24:01 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 15:24:01 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/userFiles-dd7fcc93-bb90-4c5e-9761-9a313fc19980/darima.zip/darima/dlsa.py:22) failed in 8.360 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 15:24:01 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/userFiles-dd7fcc93-bb90-4c5e-9761-9a313fc19980/darima.zip/darima/dlsa.py:22, took 9.729192 s
20/12/04 15:24:01 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 15:24:01 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 15:24:02 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 15:24:02 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 15:24:02 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 15:24:02 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 15:24:02 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 15:24:02 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 15:24:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 15:24:02 INFO MemoryStore: MemoryStore cleared
20/12/04 15:24:02 INFO BlockManager: BlockManager stopped
20/12/04 15:24:02 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 15:24:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 15:24:02 INFO SparkContext: Successfully stopped SparkContext
20/12/04 15:24:02 INFO ShutdownHookManager: Shutdown hook called
20/12/04 15:24:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-c399e1d9-22ae-48dc-ad59-544c7199907d
20/12/04 15:24:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d/pyspark-e301e5d3-07b5-4e02-b1ff-db4079665fd0
20/12/04 15:24:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-f4f18ea2-a0d6-4733-a3e1-fbfb578d6f3d
20/12/04 15:24:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 15:24:05 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:24:05 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:24:05 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:24:05 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:24:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:24:05 INFO SparkContext: Running Spark version 3.0.1
20/12/04 15:24:05 INFO ResourceUtils: ==============================================================
20/12/04 15:24:05 INFO ResourceUtils: Resources for spark.driver:

20/12/04 15:24:05 INFO ResourceUtils: ==============================================================
20/12/04 15:24:05 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 15:24:05 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:24:05 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:24:05 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:24:05 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:24:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:24:06 INFO Utils: Successfully started service 'sparkDriver' on port 42215.
20/12/04 15:24:06 INFO SparkEnv: Registering MapOutputTracker
20/12/04 15:24:06 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 15:24:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 15:24:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 15:24:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 15:24:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9ea8c143-69c9-4f6a-aef1-570bc94cab89
20/12/04 15:24:06 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 15:24:06 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 15:24:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 15:24:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 15:24:07 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 15:24:07 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 15:24:08 INFO Configuration: resource-types.xml not found
20/12/04 15:24:08 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 15:24:08 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 15:24:08 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 15:24:08 INFO Client: Setting up container launch context for our AM
20/12/04 15:24:08 INFO Client: Setting up the launch environment for our AM container
20/12/04 15:24:08 INFO Client: Preparing resources for our AM container
20/12/04 15:24:08 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 15:24:10 INFO Client: Uploading resource file:/tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/__spark_libs__3464404625382589842.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0055/__spark_libs__3464404625382589842.zip
20/12/04 15:24:10 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0055/pyspark.zip
20/12/04 15:24:11 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0055/py4j-0.10.9-src.zip
20/12/04 15:24:11 INFO Client: Uploading resource file:/tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/__spark_conf__16732410033909564860.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0055/__spark_conf__.zip
20/12/04 15:24:11 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:24:11 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:24:11 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:24:11 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:24:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:24:11 INFO Client: Submitting application application_1607015337794_0055 to ResourceManager
20/12/04 15:24:11 INFO YarnClientImpl: Submitted application application_1607015337794_0055
20/12/04 15:24:12 INFO Client: Application report for application_1607015337794_0055 (state: ACCEPTED)
20/12/04 15:24:12 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607113451817
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0055/
	 user: bsuconn
20/12/04 15:24:13 INFO Client: Application report for application_1607015337794_0055 (state: ACCEPTED)
20/12/04 15:24:14 INFO Client: Application report for application_1607015337794_0055 (state: ACCEPTED)
20/12/04 15:24:15 INFO Client: Application report for application_1607015337794_0055 (state: ACCEPTED)
20/12/04 15:24:16 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0055), /proxy/application_1607015337794_0055
20/12/04 15:24:16 INFO Client: Application report for application_1607015337794_0055 (state: RUNNING)
20/12/04 15:24:16 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607113451817
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0055/
	 user: bsuconn
20/12/04 15:24:16 INFO YarnClientSchedulerBackend: Application application_1607015337794_0055 has started running.
20/12/04 15:24:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37123.
20/12/04 15:24:16 INFO NettyBlockTransferService: Server created on 192.168.1.9:37123
20/12/04 15:24:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 15:24:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 37123, None)
20/12/04 15:24:16 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:37123 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 37123, None)
20/12/04 15:24:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 37123, None)
20/12/04 15:24:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 37123, None)
20/12/04 15:24:17 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:24:17 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 15:24:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 15:24:22 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:43096) with ID 1
20/12/04 15:24:22 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:36373 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 36373, None)
20/12/04 15:24:23 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:43106) with ID 2
20/12/04 15:24:23 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 15:24:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:42509 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 42509, None)
20/12/04 15:24:23 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 15:24:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 15:24:24 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 15:24:24 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:24:24 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:24:24 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:24:24 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:24:24 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:24:24 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:42215/files/darima.zip with timestamp 1607113464738
20/12/04 15:24:24 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/userFiles-9c6f5e55-c713-40f0-ab32-15fb42d2cb46/darima.zip
20/12/04 15:24:26 INFO InMemoryFileIndex: It took 73 ms to list leaf files for 1 paths.
20/12/04 15:24:28 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
20/12/04 15:24:28 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:24:28 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:24:28 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:24:28 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:24:29 INFO CodeGenerator: Code generated in 254.61356 ms
20/12/04 15:24:29 INFO CodeGenerator: Code generated in 32.031556 ms
20/12/04 15:24:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:24:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:24:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:37123 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:24:29 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 15:24:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:24:29 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 15:24:29 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 15:24:29 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 15:24:29 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 15:24:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 15:24:29 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 15:24:29 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:24:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 15:24:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 15:24:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:37123 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:24:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 15:24:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:24:30 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 15:24:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:24:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:42509 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:24:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:42509 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:24:33 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3284 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 15:24:33 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 15:24:33 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.417 s
20/12/04 15:24:33 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:24:33 INFO DAGScheduler: running: Set()
20/12/04 15:24:33 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 15:24:33 INFO DAGScheduler: failed: Set()
20/12/04 15:24:33 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:24:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 15:24:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 15:24:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:37123 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:24:33 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 15:24:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:24:33 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 15:24:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:42509 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:24:33 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:43106
20/12/04 15:24:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 376 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 15:24:33 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 15:24:33 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.396 s
20/12/04 15:24:33 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 15:24:33 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 15:24:33 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.902852 s
20/12/04 15:24:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:42509 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:24:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:37123 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:24:36 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:42509 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:24:36 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:37123 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:24:36 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 15:24:37 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:24:37 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:24:37 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:24:37 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:24:37 INFO CodeGenerator: Code generated in 25.509532 ms
20/12/04 15:24:37 INFO CodeGenerator: Code generated in 16.439307 ms
20/12/04 15:24:37 INFO CodeGenerator: Code generated in 35.586905 ms
20/12/04 15:24:37 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:24:37 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:24:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:37123 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:24:37 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/userFiles-9c6f5e55-c713-40f0-ab32-15fb42d2cb46/darima.zip/darima/dlsa.py:22
20/12/04 15:24:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:24:37 INFO SparkContext: Starting job: toPandas at /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/userFiles-9c6f5e55-c713-40f0-ab32-15fb42d2cb46/darima.zip/darima/dlsa.py:22
20/12/04 15:24:37 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/userFiles-9c6f5e55-c713-40f0-ab32-15fb42d2cb46/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 15:24:37 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/userFiles-9c6f5e55-c713-40f0-ab32-15fb42d2cb46/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 15:24:37 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/userFiles-9c6f5e55-c713-40f0-ab32-15fb42d2cb46/darima.zip/darima/dlsa.py:22)
20/12/04 15:24:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 15:24:37 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 15:24:37 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/userFiles-9c6f5e55-c713-40f0-ab32-15fb42d2cb46/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:24:37 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 15:24:37 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 15:24:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:37123 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 15:24:37 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 15:24:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/userFiles-9c6f5e55-c713-40f0-ab32-15fb42d2cb46/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 15:24:37 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 15:24:37 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:24:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:42509 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 15:24:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:42509 (size: 28.7 KiB, free: 912.2 MiB)
20/12/04 15:24:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1310 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 15:24:38 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 15:24:38 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 52537
20/12/04 15:24:38 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/userFiles-9c6f5e55-c713-40f0-ab32-15fb42d2cb46/darima.zip/darima/dlsa.py:22) finished in 1.334 s
20/12/04 15:24:38 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:24:38 INFO DAGScheduler: running: Set()
20/12/04 15:24:38 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 15:24:38 INFO DAGScheduler: failed: Set()
20/12/04 15:24:38 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/userFiles-9c6f5e55-c713-40f0-ab32-15fb42d2cb46/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:24:38 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 15:24:38 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 15:24:38 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:37123 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 15:24:38 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 15:24:38 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/userFiles-9c6f5e55-c713-40f0-ab32-15fb42d2cb46/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 15:24:38 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 15:24:38 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:38 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:38 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:42509 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 15:24:39 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:43106
20/12/04 15:24:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:36373 (size: 131.5 KiB, free: 912.2 MiB)
20/12/04 15:24:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:43096
20/12/04 15:24:44 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:44 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 15:24:46 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 6, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:46 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 15:24:47 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 7, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:47 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 15:24:49 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:49 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 15:24:49 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 9, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:49 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 15:24:51 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:51 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 15:24:51 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 11, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:51 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 9) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 15:24:52 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 12, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:52 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 10) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/04 15:24:52 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:24:52 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 11) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/04 15:24:52 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 15:24:52 INFO YarnScheduler: Cancelling stage 3
20/12/04 15:24:52 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 15:24:52 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 15:24:52 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/userFiles-9c6f5e55-c713-40f0-ab32-15fb42d2cb46/darima.zip/darima/dlsa.py:22) failed in 13.692 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 11, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 15:24:52 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/userFiles-9c6f5e55-c713-40f0-ab32-15fb42d2cb46/darima.zip/darima/dlsa.py:22, took 15.094172 s
20/12/04 15:24:53 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 15:24:53 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 15:24:53 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 15:24:53 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 15:24:53 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 15:24:53 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 15:24:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 15:24:53 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:169)
	at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:150)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:684)
	at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:834)
20/12/04 15:24:53 INFO MemoryStore: MemoryStore cleared
20/12/04 15:24:53 INFO BlockManager: BlockManager stopped
20/12/04 15:24:53 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 15:24:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 15:24:53 INFO SparkContext: Successfully stopped SparkContext
20/12/04 15:24:53 INFO ShutdownHookManager: Shutdown hook called
20/12/04 15:24:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e/pyspark-ae5e89ea-4562-4ec1-865b-6e8f45a7a2ae
20/12/04 15:24:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-863b75c8-d61b-4fb1-ac15-8db3a2ef3f56
20/12/04 15:24:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-f8db0837-3f46-4a8e-8db0-d6bc5c5cff9e
20/12/04 15:24:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 15:24:56 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:24:56 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:24:56 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:24:56 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:24:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:24:56 INFO SparkContext: Running Spark version 3.0.1
20/12/04 15:24:56 INFO ResourceUtils: ==============================================================
20/12/04 15:24:56 INFO ResourceUtils: Resources for spark.driver:

20/12/04 15:24:56 INFO ResourceUtils: ==============================================================
20/12/04 15:24:56 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 15:24:57 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:24:57 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:24:57 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:24:57 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:24:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:24:57 INFO Utils: Successfully started service 'sparkDriver' on port 44481.
20/12/04 15:24:57 INFO SparkEnv: Registering MapOutputTracker
20/12/04 15:24:57 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 15:24:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 15:24:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 15:24:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 15:24:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9e33b30d-9dfc-47c7-adcd-0545cbbd2cbb
20/12/04 15:24:57 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 15:24:57 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 15:24:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 15:24:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 15:24:58 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 15:24:58 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 15:24:59 INFO Configuration: resource-types.xml not found
20/12/04 15:24:59 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 15:24:59 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 15:24:59 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 15:24:59 INFO Client: Setting up container launch context for our AM
20/12/04 15:24:59 INFO Client: Setting up the launch environment for our AM container
20/12/04 15:24:59 INFO Client: Preparing resources for our AM container
20/12/04 15:24:59 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 15:25:02 INFO Client: Uploading resource file:/tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/__spark_libs__5476783159588894715.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0056/__spark_libs__5476783159588894715.zip
20/12/04 15:25:03 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0056/pyspark.zip
20/12/04 15:25:03 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0056/py4j-0.10.9-src.zip
20/12/04 15:25:03 INFO Client: Uploading resource file:/tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/__spark_conf__15527723890145620886.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0056/__spark_conf__.zip
20/12/04 15:25:03 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:25:03 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:25:03 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:25:03 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:25:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:25:03 INFO Client: Submitting application application_1607015337794_0056 to ResourceManager
20/12/04 15:25:03 INFO YarnClientImpl: Submitted application application_1607015337794_0056
20/12/04 15:25:04 INFO Client: Application report for application_1607015337794_0056 (state: ACCEPTED)
20/12/04 15:25:04 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607113503675
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0056/
	 user: bsuconn
20/12/04 15:25:05 INFO Client: Application report for application_1607015337794_0056 (state: ACCEPTED)
20/12/04 15:25:06 INFO Client: Application report for application_1607015337794_0056 (state: ACCEPTED)
20/12/04 15:25:07 INFO Client: Application report for application_1607015337794_0056 (state: ACCEPTED)
20/12/04 15:25:08 INFO Client: Application report for application_1607015337794_0056 (state: RUNNING)
20/12/04 15:25:08 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607113503675
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0056/
	 user: bsuconn
20/12/04 15:25:08 INFO YarnClientSchedulerBackend: Application application_1607015337794_0056 has started running.
20/12/04 15:25:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33521.
20/12/04 15:25:08 INFO NettyBlockTransferService: Server created on 192.168.1.9:33521
20/12/04 15:25:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 15:25:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 33521, None)
20/12/04 15:25:08 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:33521 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 33521, None)
20/12/04 15:25:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 33521, None)
20/12/04 15:25:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 33521, None)
20/12/04 15:25:08 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0056), /proxy/application_1607015337794_0056
20/12/04 15:25:09 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:25:09 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 15:25:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 15:25:14 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:54778) with ID 1
20/12/04 15:25:14 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:36563 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 36563, None)
20/12/04 15:25:15 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:54782) with ID 2
20/12/04 15:25:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:36403 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 36403, None)
20/12/04 15:25:28 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
20/12/04 15:25:28 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 15:25:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 15:25:28 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 15:25:28 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:25:28 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:25:28 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:25:28 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:25:28 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:25:29 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:44481/files/darima.zip with timestamp 1607113529194
20/12/04 15:25:29 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/userFiles-a5c0fbc4-cdac-41b7-a6c9-c2b7cba7c9c9/darima.zip
20/12/04 15:25:30 INFO InMemoryFileIndex: It took 54 ms to list leaf files for 1 paths.
20/12/04 15:25:32 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
20/12/04 15:25:33 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:25:33 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:25:33 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:25:33 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:25:33 INFO CodeGenerator: Code generated in 291.976787 ms
20/12/04 15:25:34 INFO CodeGenerator: Code generated in 34.264912 ms
20/12/04 15:25:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:25:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:25:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:33521 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:25:34 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 15:25:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:25:34 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 15:25:34 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 15:25:34 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 15:25:34 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 15:25:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 15:25:34 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 15:25:34 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:25:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 15:25:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 15:25:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:33521 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:25:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 15:25:34 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:25:34 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 15:25:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:25:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:36563 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:25:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:36563 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:25:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3152 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:25:37 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 15:25:37 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.282 s
20/12/04 15:25:37 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:25:37 INFO DAGScheduler: running: Set()
20/12/04 15:25:37 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 15:25:37 INFO DAGScheduler: failed: Set()
20/12/04 15:25:37 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:25:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 15:25:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 15:25:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:33521 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:25:37 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 15:25:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:25:37 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 15:25:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:25:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:36563 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:25:37 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:54778
20/12/04 15:25:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 373 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:25:38 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 15:25:38 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.388 s
20/12/04 15:25:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 15:25:38 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 15:25:38 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.756406 s
20/12/04 15:25:40 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:36563 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:25:40 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:33521 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:25:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:33521 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:25:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:36563 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:25:40 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:36563 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:25:40 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:33521 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:25:40 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 15:25:41 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:25:41 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:25:41 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:25:41 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:25:41 INFO CodeGenerator: Code generated in 32.415088 ms
20/12/04 15:25:41 INFO CodeGenerator: Code generated in 17.233456 ms
20/12/04 15:25:41 INFO CodeGenerator: Code generated in 20.969986 ms
20/12/04 15:25:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:25:41 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:25:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:33521 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:25:41 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/userFiles-a5c0fbc4-cdac-41b7-a6c9-c2b7cba7c9c9/darima.zip/darima/dlsa.py:22
20/12/04 15:25:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:25:41 INFO SparkContext: Starting job: toPandas at /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/userFiles-a5c0fbc4-cdac-41b7-a6c9-c2b7cba7c9c9/darima.zip/darima/dlsa.py:22
20/12/04 15:25:41 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/userFiles-a5c0fbc4-cdac-41b7-a6c9-c2b7cba7c9c9/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 15:25:41 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/userFiles-a5c0fbc4-cdac-41b7-a6c9-c2b7cba7c9c9/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 15:25:41 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/userFiles-a5c0fbc4-cdac-41b7-a6c9-c2b7cba7c9c9/darima.zip/darima/dlsa.py:22)
20/12/04 15:25:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 15:25:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 15:25:41 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/userFiles-a5c0fbc4-cdac-41b7-a6c9-c2b7cba7c9c9/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:25:41 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 15:25:41 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 15:25:41 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:33521 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 15:25:41 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 15:25:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/userFiles-a5c0fbc4-cdac-41b7-a6c9-c2b7cba7c9c9/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 15:25:41 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 15:25:41 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:25:41 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:36563 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 15:25:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:36563 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:25:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1169 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:25:42 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 15:25:42 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 40393
20/12/04 15:25:42 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/userFiles-a5c0fbc4-cdac-41b7-a6c9-c2b7cba7c9c9/darima.zip/darima/dlsa.py:22) finished in 1.191 s
20/12/04 15:25:42 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:25:42 INFO DAGScheduler: running: Set()
20/12/04 15:25:42 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 15:25:42 INFO DAGScheduler: failed: Set()
20/12/04 15:25:42 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/userFiles-a5c0fbc4-cdac-41b7-a6c9-c2b7cba7c9c9/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:25:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 15:25:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 15:25:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:33521 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 15:25:43 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 15:25:43 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/userFiles-a5c0fbc4-cdac-41b7-a6c9-c2b7cba7c9c9/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 15:25:43 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 15:25:43 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:25:43 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:25:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:36563 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 15:25:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:54778
20/12/04 15:25:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:36403 (size: 131.5 KiB, free: 912.2 MiB)
20/12/04 15:25:46 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:54782
20/12/04 15:25:48 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:25:48 WARN TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 15:25:50 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:25:50 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 15:25:52 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:25:52 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 15:25:53 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:25:53 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 15:25:53 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 9, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:25:53 INFO TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 15:25:55 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:25:55 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 15:25:55 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 11, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:25:55 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 9) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 15:25:56 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:25:56 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 10) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/04 15:25:56 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 13, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:25:56 INFO TaskSetManager: Lost task 2.3 in stage 3.0 (TID 11) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/04 15:25:56 ERROR TaskSetManager: Task 2 in stage 3.0 failed 4 times; aborting job
20/12/04 15:25:56 INFO YarnScheduler: Cancelling stage 3
20/12/04 15:25:56 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 15:25:57 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 15:25:57 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/userFiles-a5c0fbc4-cdac-41b7-a6c9-c2b7cba7c9c9/darima.zip/darima/dlsa.py:22) failed in 13.999 s due to Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 11, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 15:25:57 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/userFiles-a5c0fbc4-cdac-41b7-a6c9-c2b7cba7c9c9/darima.zip/darima/dlsa.py:22, took 15.246377 s
20/12/04 15:25:57 WARN TaskSetManager: Lost task 0.2 in stage 3.0 (TID 13, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/04 15:25:57 WARN TaskSetManager: Lost task 3.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 15:25:57 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 15:25:57 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 15:25:57 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 15:25:57 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 15:25:57 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 15:25:57 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 15:25:57 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 15:25:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 15:25:57 INFO MemoryStore: MemoryStore cleared
20/12/04 15:25:57 INFO BlockManager: BlockManager stopped
20/12/04 15:25:57 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 15:25:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 15:25:57 INFO SparkContext: Successfully stopped SparkContext
20/12/04 15:25:57 INFO ShutdownHookManager: Shutdown hook called
20/12/04 15:25:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-7a507db2-1d79-4b8c-9759-5df9202147cb
20/12/04 15:25:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3
20/12/04 15:25:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-45c9e53d-d7dc-46f7-afde-6d19af76fee3/pyspark-0d712de5-91d7-4a61-aa5e-f2454e55645a
20/12/04 15:30:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 15:30:22 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:30:22 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:30:22 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:30:22 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:30:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:30:23 INFO SparkContext: Running Spark version 3.0.1
20/12/04 15:30:23 INFO ResourceUtils: ==============================================================
20/12/04 15:30:23 INFO ResourceUtils: Resources for spark.driver:

20/12/04 15:30:23 INFO ResourceUtils: ==============================================================
20/12/04 15:30:23 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 15:30:23 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:30:23 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:30:23 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:30:23 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:30:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:30:24 INFO Utils: Successfully started service 'sparkDriver' on port 36429.
20/12/04 15:30:24 INFO SparkEnv: Registering MapOutputTracker
20/12/04 15:30:24 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 15:30:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 15:30:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 15:30:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 15:30:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6cab724d-b04d-40bb-b4b9-15c4b2fc6eeb
20/12/04 15:30:24 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 15:30:24 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 15:30:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 15:30:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 15:30:25 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 15:30:26 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 15:30:26 INFO Configuration: resource-types.xml not found
20/12/04 15:30:26 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 15:30:26 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 15:30:26 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 15:30:26 INFO Client: Setting up container launch context for our AM
20/12/04 15:30:26 INFO Client: Setting up the launch environment for our AM container
20/12/04 15:30:26 INFO Client: Preparing resources for our AM container
20/12/04 15:30:26 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 15:30:28 INFO Client: Uploading resource file:/tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/__spark_libs__3351241313508465688.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0057/__spark_libs__3351241313508465688.zip
20/12/04 15:30:29 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0057/pyspark.zip
20/12/04 15:30:29 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0057/py4j-0.10.9-src.zip
20/12/04 15:30:30 INFO Client: Uploading resource file:/tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/__spark_conf__1258234392511458864.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0057/__spark_conf__.zip
20/12/04 15:30:30 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:30:30 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:30:30 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:30:30 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:30:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:30:30 INFO Client: Submitting application application_1607015337794_0057 to ResourceManager
20/12/04 15:30:30 INFO YarnClientImpl: Submitted application application_1607015337794_0057
20/12/04 15:30:31 INFO Client: Application report for application_1607015337794_0057 (state: ACCEPTED)
20/12/04 15:30:31 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607113830276
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0057/
	 user: bsuconn
20/12/04 15:30:32 INFO Client: Application report for application_1607015337794_0057 (state: ACCEPTED)
20/12/04 15:30:33 INFO Client: Application report for application_1607015337794_0057 (state: ACCEPTED)
20/12/04 15:30:34 INFO Client: Application report for application_1607015337794_0057 (state: ACCEPTED)
20/12/04 15:30:35 INFO Client: Application report for application_1607015337794_0057 (state: ACCEPTED)
20/12/04 15:30:35 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0057), /proxy/application_1607015337794_0057
20/12/04 15:30:36 INFO Client: Application report for application_1607015337794_0057 (state: RUNNING)
20/12/04 15:30:36 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607113830276
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0057/
	 user: bsuconn
20/12/04 15:30:36 INFO YarnClientSchedulerBackend: Application application_1607015337794_0057 has started running.
20/12/04 15:30:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44983.
20/12/04 15:30:36 INFO NettyBlockTransferService: Server created on 192.168.1.9:44983
20/12/04 15:30:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 15:30:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 44983, None)
20/12/04 15:30:36 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:44983 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 44983, None)
20/12/04 15:30:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 44983, None)
20/12/04 15:30:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 44983, None)
20/12/04 15:30:36 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:30:36 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 15:30:39 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 15:30:40 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:51746) with ID 1
20/12/04 15:30:40 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 15:30:40 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:33163 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 33163, None)
20/12/04 15:30:40 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 15:30:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 15:30:40 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 15:30:40 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:30:40 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:30:40 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:30:40 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:30:40 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:30:41 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:36429/files/darima.zip with timestamp 1607113841400
20/12/04 15:30:41 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/userFiles-a1857737-b5c4-4570-bdef-28ba12d040fd/darima.zip
20/12/04 15:30:43 INFO InMemoryFileIndex: It took 77 ms to list leaf files for 1 paths.
20/12/04 15:30:45 INFO InMemoryFileIndex: It took 14 ms to list leaf files for 1 paths.
20/12/04 15:30:45 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:30:45 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:30:45 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:30:45 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:30:46 INFO CodeGenerator: Code generated in 312.711214 ms
20/12/04 15:30:46 INFO CodeGenerator: Code generated in 41.057848 ms
20/12/04 15:30:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:30:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:30:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:44983 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:30:47 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 15:30:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:30:47 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 15:30:47 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 15:30:47 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 15:30:47 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 15:30:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 15:30:47 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 15:30:47 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:30:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 15:30:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 15:30:47 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:44983 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:30:47 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 15:30:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:30:47 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 15:30:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:30:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:33163 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:30:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:33163 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:30:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3176 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:30:50 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 15:30:50 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.311 s
20/12/04 15:30:50 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:30:50 INFO DAGScheduler: running: Set()
20/12/04 15:30:50 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 15:30:50 INFO DAGScheduler: failed: Set()
20/12/04 15:30:50 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:30:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 15:30:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 15:30:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:44983 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:30:50 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 15:30:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:30:50 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 15:30:50 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:30:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:33163 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:30:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:51746
20/12/04 15:30:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 348 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:30:51 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 15:30:51 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.371 s
20/12/04 15:30:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 15:30:51 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 15:30:51 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.786007 s
20/12/04 15:30:53 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 15:30:53 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:33163 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:30:53 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:44983 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:30:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:33163 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:30:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:44983 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:30:54 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:30:54 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:30:54 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:30:54 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:30:54 INFO CodeGenerator: Code generated in 27.985706 ms
20/12/04 15:30:54 INFO CodeGenerator: Code generated in 16.82209 ms
20/12/04 15:30:54 INFO CodeGenerator: Code generated in 21.346597 ms
20/12/04 15:30:54 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:30:54 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:30:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:44983 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:30:54 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/userFiles-a1857737-b5c4-4570-bdef-28ba12d040fd/darima.zip/darima/dlsa.py:22
20/12/04 15:30:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:30:55 INFO SparkContext: Starting job: toPandas at /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/userFiles-a1857737-b5c4-4570-bdef-28ba12d040fd/darima.zip/darima/dlsa.py:22
20/12/04 15:30:55 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/userFiles-a1857737-b5c4-4570-bdef-28ba12d040fd/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 15:30:55 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/userFiles-a1857737-b5c4-4570-bdef-28ba12d040fd/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 15:30:55 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/userFiles-a1857737-b5c4-4570-bdef-28ba12d040fd/darima.zip/darima/dlsa.py:22)
20/12/04 15:30:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 15:30:55 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 15:30:55 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/userFiles-a1857737-b5c4-4570-bdef-28ba12d040fd/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:30:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 15:30:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 15:30:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:44983 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 15:30:55 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 15:30:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/userFiles-a1857737-b5c4-4570-bdef-28ba12d040fd/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 15:30:55 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 15:30:55 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:30:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:33163 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 15:30:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:33163 (size: 28.7 KiB, free: 912.2 MiB)
20/12/04 15:30:56 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1367 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:30:56 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 15:30:56 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 37179
20/12/04 15:30:56 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/userFiles-a1857737-b5c4-4570-bdef-28ba12d040fd/darima.zip/darima/dlsa.py:22) finished in 1.398 s
20/12/04 15:30:56 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:30:56 INFO DAGScheduler: running: Set()
20/12/04 15:30:56 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 15:30:56 INFO DAGScheduler: failed: Set()
20/12/04 15:30:56 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/userFiles-a1857737-b5c4-4570-bdef-28ba12d040fd/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:30:56 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 15:30:56 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 15:30:56 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:44983 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 15:30:56 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 15:30:56 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/userFiles-a1857737-b5c4-4570-bdef-28ba12d040fd/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 15:30:56 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 15:30:56 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:30:56 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:33163 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 15:30:56 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:51746
20/12/04 15:30:59 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:30:59 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 180, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 15:31:00 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:31:00 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 180, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 15:31:01 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:31:01 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 180, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 15:31:02 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:31:02 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 180, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 15:31:03 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:31:03 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 180, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 15:31:03 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:31:03 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 180, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 15:31:04 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:31:04 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 180, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 15:31:04 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 15:31:04 INFO YarnScheduler: Cancelling stage 3
20/12/04 15:31:04 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 15:31:04 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 15:31:04 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/userFiles-a1857737-b5c4-4570-bdef-28ba12d040fd/darima.zip/darima/dlsa.py:22) failed in 8.227 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 180, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 15:31:04 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/userFiles-a1857737-b5c4-4570-bdef-28ba12d040fd/darima.zip/darima/dlsa.py:22, took 9.676934 s
20/12/04 15:31:04 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 15:31:04 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 15:31:05 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 15:31:05 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 15:31:05 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 15:31:05 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 15:31:05 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 15:31:05 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 15:31:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 15:31:05 INFO MemoryStore: MemoryStore cleared
20/12/04 15:31:05 INFO BlockManager: BlockManager stopped
20/12/04 15:31:05 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 15:31:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 15:31:05 INFO SparkContext: Successfully stopped SparkContext
20/12/04 15:31:05 INFO ShutdownHookManager: Shutdown hook called
20/12/04 15:31:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025
20/12/04 15:31:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-be449777-573a-4808-92b6-5c59ce85c025/pyspark-604a63fc-7d7e-45ac-8eb6-d7ecf98d16d5
20/12/04 15:31:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-1563e815-1781-4add-8a8c-598327669e9e
20/12/04 15:31:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 15:31:08 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:31:08 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:31:08 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:31:08 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:31:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:31:08 INFO SparkContext: Running Spark version 3.0.1
20/12/04 15:31:08 INFO ResourceUtils: ==============================================================
20/12/04 15:31:08 INFO ResourceUtils: Resources for spark.driver:

20/12/04 15:31:08 INFO ResourceUtils: ==============================================================
20/12/04 15:31:08 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 15:31:08 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:31:08 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:31:08 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:31:08 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:31:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:31:09 INFO Utils: Successfully started service 'sparkDriver' on port 41437.
20/12/04 15:31:09 INFO SparkEnv: Registering MapOutputTracker
20/12/04 15:31:09 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 15:31:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 15:31:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 15:31:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 15:31:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-132e3a79-9276-4e64-99fd-ba8aa2353187
20/12/04 15:31:09 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 15:31:09 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 15:31:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 15:31:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 15:31:10 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 15:31:10 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 15:31:11 INFO Configuration: resource-types.xml not found
20/12/04 15:31:11 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 15:31:11 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 15:31:11 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 15:31:11 INFO Client: Setting up container launch context for our AM
20/12/04 15:31:11 INFO Client: Setting up the launch environment for our AM container
20/12/04 15:31:11 INFO Client: Preparing resources for our AM container
20/12/04 15:31:11 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 15:31:13 INFO Client: Uploading resource file:/tmp/spark-64568432-95b0-400d-b1f6-6fb16cdcf624/__spark_libs__11871791391881914879.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0058/__spark_libs__11871791391881914879.zip
20/12/04 15:31:14 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0058/pyspark.zip
20/12/04 15:31:14 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0058/py4j-0.10.9-src.zip
20/12/04 15:31:14 INFO Client: Uploading resource file:/tmp/spark-64568432-95b0-400d-b1f6-6fb16cdcf624/__spark_conf__14694637204282609398.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0058/__spark_conf__.zip
20/12/04 15:31:14 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:31:14 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:31:14 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:31:14 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:31:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:31:14 INFO Client: Submitting application application_1607015337794_0058 to ResourceManager
20/12/04 15:31:14 INFO YarnClientImpl: Submitted application application_1607015337794_0058
20/12/04 15:31:15 INFO Client: Application report for application_1607015337794_0058 (state: ACCEPTED)
20/12/04 15:31:15 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607113874875
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0058/
	 user: bsuconn
20/12/04 15:31:16 INFO Client: Application report for application_1607015337794_0058 (state: ACCEPTED)
20/12/04 15:31:17 INFO Client: Application report for application_1607015337794_0058 (state: ACCEPTED)
20/12/04 15:31:18 INFO Client: Application report for application_1607015337794_0058 (state: ACCEPTED)
20/12/04 15:31:19 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0058), /proxy/application_1607015337794_0058
20/12/04 15:31:19 INFO Client: Application report for application_1607015337794_0058 (state: RUNNING)
20/12/04 15:31:19 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607113874875
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0058/
	 user: bsuconn
20/12/04 15:31:19 INFO YarnClientSchedulerBackend: Application application_1607015337794_0058 has started running.
20/12/04 15:31:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33225.
20/12/04 15:31:19 INFO NettyBlockTransferService: Server created on 192.168.1.9:33225
20/12/04 15:31:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 15:31:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 33225, None)
20/12/04 15:31:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:33225 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 33225, None)
20/12/04 15:31:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 33225, None)
20/12/04 15:31:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 33225, None)
20/12/04 15:31:20 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:31:20 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 15:31:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 15:31:25 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:51098) with ID 1
20/12/04 15:31:25 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:39219 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 39219, None)
20/12/04 15:31:26 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:51102) with ID 2
20/12/04 15:31:26 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 15:31:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:43963 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 43963, None)
20/12/04 15:31:26 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 15:31:26 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 15:31:26 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 15:31:26 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:31:26 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:31:26 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:31:26 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:31:27 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:31:27 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:41437/files/darima.zip with timestamp 1607113887705
20/12/04 15:31:27 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-64568432-95b0-400d-b1f6-6fb16cdcf624/userFiles-7392326e-e952-47fb-aa65-ce796bbe10d5/darima.zip
20/12/04 15:31:30 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
20/12/04 15:31:30 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 15:31:30 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 15:31:30 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 15:31:30 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 15:31:30 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 15:31:30 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 15:31:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 15:31:31 INFO MemoryStore: MemoryStore cleared
20/12/04 15:31:31 INFO BlockManager: BlockManager stopped
20/12/04 15:31:31 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 15:31:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 15:31:31 INFO SparkContext: Successfully stopped SparkContext
20/12/04 15:31:31 INFO ShutdownHookManager: Shutdown hook called
20/12/04 15:31:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-64568432-95b0-400d-b1f6-6fb16cdcf624
20/12/04 15:31:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-95408d86-c6c3-447e-8eac-30c0220f7d92
20/12/04 15:31:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-64568432-95b0-400d-b1f6-6fb16cdcf624/pyspark-fd56212b-2eed-465d-ab34-432bdd8e817d
20/12/04 15:33:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 15:33:50 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:33:50 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:33:50 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:33:50 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:33:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:33:50 INFO SparkContext: Running Spark version 3.0.1
20/12/04 15:33:51 INFO ResourceUtils: ==============================================================
20/12/04 15:33:51 INFO ResourceUtils: Resources for spark.driver:

20/12/04 15:33:51 INFO ResourceUtils: ==============================================================
20/12/04 15:33:51 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 15:33:51 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:33:51 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:33:51 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:33:51 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:33:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:33:51 INFO Utils: Successfully started service 'sparkDriver' on port 36613.
20/12/04 15:33:51 INFO SparkEnv: Registering MapOutputTracker
20/12/04 15:33:51 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 15:33:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 15:33:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 15:33:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 15:33:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e574c5ff-7952-4753-955e-860cbf45a15d
20/12/04 15:33:51 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 15:33:51 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 15:33:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 15:33:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 15:33:52 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 15:33:53 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 15:33:53 INFO Configuration: resource-types.xml not found
20/12/04 15:33:53 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 15:33:53 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 15:33:53 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 15:33:53 INFO Client: Setting up container launch context for our AM
20/12/04 15:33:53 INFO Client: Setting up the launch environment for our AM container
20/12/04 15:33:53 INFO Client: Preparing resources for our AM container
20/12/04 15:33:53 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 15:33:55 INFO Client: Uploading resource file:/tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/__spark_libs__35578428582817576.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0059/__spark_libs__35578428582817576.zip
20/12/04 15:33:56 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0059/pyspark.zip
20/12/04 15:33:56 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0059/py4j-0.10.9-src.zip
20/12/04 15:33:57 INFO Client: Uploading resource file:/tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/__spark_conf__18017517869659425354.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0059/__spark_conf__.zip
20/12/04 15:33:57 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:33:57 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:33:57 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:33:57 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:33:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:33:57 INFO Client: Submitting application application_1607015337794_0059 to ResourceManager
20/12/04 15:33:57 INFO YarnClientImpl: Submitted application application_1607015337794_0059
20/12/04 15:33:58 INFO Client: Application report for application_1607015337794_0059 (state: ACCEPTED)
20/12/04 15:33:58 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114037692
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0059/
	 user: bsuconn
20/12/04 15:33:59 INFO Client: Application report for application_1607015337794_0059 (state: ACCEPTED)
20/12/04 15:34:00 INFO Client: Application report for application_1607015337794_0059 (state: ACCEPTED)
20/12/04 15:34:01 INFO Client: Application report for application_1607015337794_0059 (state: ACCEPTED)
20/12/04 15:34:02 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0059), /proxy/application_1607015337794_0059
20/12/04 15:34:02 INFO Client: Application report for application_1607015337794_0059 (state: RUNNING)
20/12/04 15:34:02 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114037692
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0059/
	 user: bsuconn
20/12/04 15:34:02 INFO YarnClientSchedulerBackend: Application application_1607015337794_0059 has started running.
20/12/04 15:34:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45077.
20/12/04 15:34:02 INFO NettyBlockTransferService: Server created on 192.168.1.9:45077
20/12/04 15:34:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 15:34:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 45077, None)
20/12/04 15:34:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:45077 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 45077, None)
20/12/04 15:34:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 45077, None)
20/12/04 15:34:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 45077, None)
20/12/04 15:34:03 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:34:03 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 15:34:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 15:34:07 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:47760) with ID 1
20/12/04 15:34:07 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 15:34:07 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:32943 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 32943, None)
20/12/04 15:34:07 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 15:34:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 15:34:07 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 15:34:07 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:34:07 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:34:07 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:34:07 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:34:07 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:34:08 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:36613/files/darima.zip with timestamp 1607114048484
20/12/04 15:34:08 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/userFiles-45ad82a0-e7fa-4ceb-860b-7f8c657da866/darima.zip
20/12/04 15:34:10 INFO InMemoryFileIndex: It took 67 ms to list leaf files for 1 paths.
20/12/04 15:34:12 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/12/04 15:34:13 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:34:13 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:34:13 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:34:13 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:34:13 INFO CodeGenerator: Code generated in 326.369752 ms
20/12/04 15:34:14 INFO CodeGenerator: Code generated in 31.942604 ms
20/12/04 15:34:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:34:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:34:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:45077 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:34:14 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 15:34:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:34:14 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 15:34:14 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 15:34:14 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 15:34:14 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 15:34:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 15:34:14 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 15:34:14 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:34:14 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 15:34:14 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 15:34:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:45077 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:34:14 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 15:34:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:34:14 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 15:34:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:34:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:32943 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:34:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:32943 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:34:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3335 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:34:17 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 15:34:17 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.483 s
20/12/04 15:34:17 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:34:17 INFO DAGScheduler: running: Set()
20/12/04 15:34:17 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 15:34:17 INFO DAGScheduler: failed: Set()
20/12/04 15:34:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:34:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 15:34:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 15:34:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:45077 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:34:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 15:34:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:34:18 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 15:34:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:34:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:32943 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:34:18 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:47760
20/12/04 15:34:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 372 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:34:18 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.388 s
20/12/04 15:34:18 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 15:34:18 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 15:34:18 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 15:34:18 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.964429 s
20/12/04 15:34:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:45077 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:34:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:32943 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:34:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:45077 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:34:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:32943 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:34:20 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:45077 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:34:20 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:32943 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:34:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 15:34:21 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:34:21 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:34:21 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:34:21 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:34:22 INFO CodeGenerator: Code generated in 26.637255 ms
20/12/04 15:34:22 INFO CodeGenerator: Code generated in 18.411277 ms
20/12/04 15:34:22 INFO CodeGenerator: Code generated in 30.866522 ms
20/12/04 15:34:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:34:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:34:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:45077 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:34:22 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/userFiles-45ad82a0-e7fa-4ceb-860b-7f8c657da866/darima.zip/darima/dlsa.py:22
20/12/04 15:34:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:34:22 INFO SparkContext: Starting job: toPandas at /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/userFiles-45ad82a0-e7fa-4ceb-860b-7f8c657da866/darima.zip/darima/dlsa.py:22
20/12/04 15:34:22 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/userFiles-45ad82a0-e7fa-4ceb-860b-7f8c657da866/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 15:34:22 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/userFiles-45ad82a0-e7fa-4ceb-860b-7f8c657da866/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 15:34:22 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/userFiles-45ad82a0-e7fa-4ceb-860b-7f8c657da866/darima.zip/darima/dlsa.py:22)
20/12/04 15:34:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 15:34:22 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 15:34:22 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/userFiles-45ad82a0-e7fa-4ceb-860b-7f8c657da866/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:34:22 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 15:34:22 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 15:34:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:45077 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 15:34:22 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 15:34:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/userFiles-45ad82a0-e7fa-4ceb-860b-7f8c657da866/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 15:34:22 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 15:34:22 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:34:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:32943 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 15:34:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:32943 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:34:23 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1309 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:34:23 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 15:34:23 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 42311
20/12/04 15:34:23 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/userFiles-45ad82a0-e7fa-4ceb-860b-7f8c657da866/darima.zip/darima/dlsa.py:22) finished in 1.339 s
20/12/04 15:34:23 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:34:23 INFO DAGScheduler: running: Set()
20/12/04 15:34:23 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 15:34:23 INFO DAGScheduler: failed: Set()
20/12/04 15:34:23 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/userFiles-45ad82a0-e7fa-4ceb-860b-7f8c657da866/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:34:23 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 15:34:23 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 15:34:23 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:45077 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 15:34:23 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 15:34:23 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/userFiles-45ad82a0-e7fa-4ceb-860b-7f8c657da866/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 15:34:23 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 15:34:23 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:34:23 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:32943 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 15:34:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:47760
20/12/04 15:34:27 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:34:27 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 15:34:28 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:34:28 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 15:34:29 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:34:29 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 15:34:30 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:34:30 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 15:34:30 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:34:30 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 15:34:31 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:34:31 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 15:34:32 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:34:32 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 15:34:32 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 15:34:32 INFO YarnScheduler: Cancelling stage 3
20/12/04 15:34:32 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 15:34:32 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 15:34:32 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/userFiles-45ad82a0-e7fa-4ceb-860b-7f8c657da866/darima.zip/darima/dlsa.py:22) failed in 8.983 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 15:34:32 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/userFiles-45ad82a0-e7fa-4ceb-860b-7f8c657da866/darima.zip/darima/dlsa.py:22, took 10.385599 s
20/12/04 15:34:32 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 15:34:32 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 15:34:33 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 15:34:33 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 15:34:33 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 15:34:33 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 15:34:33 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 15:34:33 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 15:34:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 15:34:33 INFO MemoryStore: MemoryStore cleared
20/12/04 15:34:33 INFO BlockManager: BlockManager stopped
20/12/04 15:34:33 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 15:34:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 15:34:33 INFO SparkContext: Successfully stopped SparkContext
20/12/04 15:34:33 INFO ShutdownHookManager: Shutdown hook called
20/12/04 15:34:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7/pyspark-53915137-cee0-4ef2-b718-4b71efee7da3
20/12/04 15:34:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-b2010632-edeb-49e3-99dc-bac65fc589a7
20/12/04 15:34:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-7e36c967-e7b6-4661-ac15-4c6bb561c3d4
20/12/04 15:34:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 15:34:36 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:34:36 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:34:36 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:34:36 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:34:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:34:36 INFO SparkContext: Running Spark version 3.0.1
20/12/04 15:34:36 INFO ResourceUtils: ==============================================================
20/12/04 15:34:36 INFO ResourceUtils: Resources for spark.driver:

20/12/04 15:34:36 INFO ResourceUtils: ==============================================================
20/12/04 15:34:36 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 15:34:36 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:34:36 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:34:36 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:34:36 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:34:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:34:37 INFO Utils: Successfully started service 'sparkDriver' on port 40745.
20/12/04 15:34:37 INFO SparkEnv: Registering MapOutputTracker
20/12/04 15:34:37 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 15:34:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 15:34:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 15:34:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 15:34:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d65eb5b2-58e1-433c-a0a1-c673d19ffa55
20/12/04 15:34:37 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 15:34:37 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 15:34:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 15:34:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 15:34:38 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 15:34:38 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 15:34:39 INFO Configuration: resource-types.xml not found
20/12/04 15:34:39 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 15:34:39 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 15:34:39 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 15:34:39 INFO Client: Setting up container launch context for our AM
20/12/04 15:34:39 INFO Client: Setting up the launch environment for our AM container
20/12/04 15:34:39 INFO Client: Preparing resources for our AM container
20/12/04 15:34:39 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 15:34:41 INFO Client: Uploading resource file:/tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/__spark_libs__15167482893878747103.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0060/__spark_libs__15167482893878747103.zip
20/12/04 15:34:42 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0060/pyspark.zip
20/12/04 15:34:42 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0060/py4j-0.10.9-src.zip
20/12/04 15:34:43 INFO Client: Uploading resource file:/tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/__spark_conf__9034912559737809665.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0060/__spark_conf__.zip
20/12/04 15:34:43 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:34:43 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:34:43 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:34:43 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:34:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:34:43 INFO Client: Submitting application application_1607015337794_0060 to ResourceManager
20/12/04 15:34:43 INFO YarnClientImpl: Submitted application application_1607015337794_0060
20/12/04 15:34:44 INFO Client: Application report for application_1607015337794_0060 (state: ACCEPTED)
20/12/04 15:34:44 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114083366
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0060/
	 user: bsuconn
20/12/04 15:34:45 INFO Client: Application report for application_1607015337794_0060 (state: ACCEPTED)
20/12/04 15:34:46 INFO Client: Application report for application_1607015337794_0060 (state: ACCEPTED)
20/12/04 15:34:47 INFO Client: Application report for application_1607015337794_0060 (state: ACCEPTED)
20/12/04 15:34:48 INFO Client: Application report for application_1607015337794_0060 (state: RUNNING)
20/12/04 15:34:48 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114083366
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0060/
	 user: bsuconn
20/12/04 15:34:48 INFO YarnClientSchedulerBackend: Application application_1607015337794_0060 has started running.
20/12/04 15:34:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45395.
20/12/04 15:34:48 INFO NettyBlockTransferService: Server created on 192.168.1.9:45395
20/12/04 15:34:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 15:34:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 45395, None)
20/12/04 15:34:48 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:45395 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 45395, None)
20/12/04 15:34:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 45395, None)
20/12/04 15:34:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 45395, None)
20/12/04 15:34:48 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0060), /proxy/application_1607015337794_0060
20/12/04 15:34:48 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:34:49 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 15:34:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 15:34:54 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:56380) with ID 1
20/12/04 15:34:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:35429 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 35429, None)
20/12/04 15:34:55 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:56384) with ID 2
20/12/04 15:34:55 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 15:34:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:42181 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 42181, None)
20/12/04 15:34:55 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 15:34:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 15:34:55 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 15:34:55 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:34:55 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:34:55 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:34:55 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:34:55 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:34:56 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:40745/files/darima.zip with timestamp 1607114096538
20/12/04 15:34:56 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/userFiles-ed327dd9-34a7-4da0-9cc1-e409ba644228/darima.zip
20/12/04 15:34:58 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
20/12/04 15:35:00 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/12/04 15:35:00 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:35:00 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:35:00 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:35:00 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:35:01 INFO CodeGenerator: Code generated in 302.606364 ms
20/12/04 15:35:01 INFO CodeGenerator: Code generated in 31.397685 ms
20/12/04 15:35:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:35:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:35:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:45395 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:35:02 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 15:35:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:35:02 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 15:35:02 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 15:35:02 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 15:35:02 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 15:35:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 15:35:02 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 15:35:02 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:35:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 15:35:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 15:35:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:45395 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:35:02 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 15:35:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:35:02 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 15:35:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:35:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:42181 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:35:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:42181 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:35:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3388 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 15:35:05 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 15:35:05 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.535 s
20/12/04 15:35:05 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:35:05 INFO DAGScheduler: running: Set()
20/12/04 15:35:05 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 15:35:05 INFO DAGScheduler: failed: Set()
20/12/04 15:35:05 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:35:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 15:35:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 15:35:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:45395 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:35:05 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 15:35:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:35:05 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 15:35:05 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:35:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:42181 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:35:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:56384
20/12/04 15:35:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 389 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 15:35:06 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 15:35:06 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.406 s
20/12/04 15:35:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 15:35:06 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 15:35:06 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 4.027752 s
20/12/04 15:35:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:45395 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:35:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:42181 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:35:07 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:42181 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:35:07 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:45395 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:35:08 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 15:35:09 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:35:09 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:35:09 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:35:09 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:35:09 INFO CodeGenerator: Code generated in 28.393053 ms
20/12/04 15:35:09 INFO CodeGenerator: Code generated in 16.675574 ms
20/12/04 15:35:09 INFO CodeGenerator: Code generated in 19.296791 ms
20/12/04 15:35:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:35:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:35:09 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:45395 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:35:09 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/userFiles-ed327dd9-34a7-4da0-9cc1-e409ba644228/darima.zip/darima/dlsa.py:22
20/12/04 15:35:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:35:09 INFO SparkContext: Starting job: toPandas at /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/userFiles-ed327dd9-34a7-4da0-9cc1-e409ba644228/darima.zip/darima/dlsa.py:22
20/12/04 15:35:09 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/userFiles-ed327dd9-34a7-4da0-9cc1-e409ba644228/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 15:35:09 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/userFiles-ed327dd9-34a7-4da0-9cc1-e409ba644228/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 15:35:09 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/userFiles-ed327dd9-34a7-4da0-9cc1-e409ba644228/darima.zip/darima/dlsa.py:22)
20/12/04 15:35:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 15:35:09 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 15:35:09 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/userFiles-ed327dd9-34a7-4da0-9cc1-e409ba644228/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:35:09 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 15:35:09 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 15:35:09 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:45395 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 15:35:09 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 15:35:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/userFiles-ed327dd9-34a7-4da0-9cc1-e409ba644228/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 15:35:09 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 15:35:09 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:35:09 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:42181 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 15:35:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:42181 (size: 28.7 KiB, free: 912.2 MiB)
20/12/04 15:35:10 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1194 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 15:35:10 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 15:35:10 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 44939
20/12/04 15:35:10 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/userFiles-ed327dd9-34a7-4da0-9cc1-e409ba644228/darima.zip/darima/dlsa.py:22) finished in 1.213 s
20/12/04 15:35:10 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:35:10 INFO DAGScheduler: running: Set()
20/12/04 15:35:10 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 15:35:10 INFO DAGScheduler: failed: Set()
20/12/04 15:35:10 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/userFiles-ed327dd9-34a7-4da0-9cc1-e409ba644228/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:35:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 15:35:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 15:35:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:45395 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 15:35:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 15:35:11 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/userFiles-ed327dd9-34a7-4da0-9cc1-e409ba644228/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 15:35:11 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 15:35:11 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:35:11 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:35:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:42181 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 15:35:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:56384
20/12/04 15:35:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:35429 (size: 131.5 KiB, free: 912.2 MiB)
20/12/04 15:35:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:56380
20/12/04 15:35:16 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:35:16 WARN TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 15:35:18 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:35:18 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 15:35:20 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 7, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:35:20 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 15:35:21 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:35:21 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 15:35:21 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:35:21 INFO TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 15:35:23 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 10, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:35:23 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 15:35:23 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 11, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:35:23 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 15:35:24 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:35:24 INFO TaskSetManager: Lost task 2.3 in stage 3.0 (TID 11) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/04 15:35:24 ERROR TaskSetManager: Task 2 in stage 3.0 failed 4 times; aborting job
20/12/04 15:35:24 INFO YarnScheduler: Cancelling stage 3
20/12/04 15:35:24 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 15:35:24 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 15:35:24 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/userFiles-ed327dd9-34a7-4da0-9cc1-e409ba644228/darima.zip/darima/dlsa.py:22) failed in 13.959 s due to Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 11, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 15:35:24 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/userFiles-ed327dd9-34a7-4da0-9cc1-e409ba644228/darima.zip/darima/dlsa.py:22, took 15.228201 s
20/12/04 15:35:24 WARN TaskSetManager: Lost task 3.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 15:35:25 WARN TaskSetManager: Lost task 0.1 in stage 3.0 (TID 10, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/04 15:35:25 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 15:35:25 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 15:35:25 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 15:35:25 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 15:35:25 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 15:35:25 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 15:35:25 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 15:35:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 15:35:25 INFO MemoryStore: MemoryStore cleared
20/12/04 15:35:25 INFO BlockManager: BlockManager stopped
20/12/04 15:35:25 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 15:35:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 15:35:25 INFO SparkContext: Successfully stopped SparkContext
20/12/04 15:35:25 INFO ShutdownHookManager: Shutdown hook called
20/12/04 15:35:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-369c046c-6be0-4e36-9959-fc3deeb8c707
20/12/04 15:35:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f/pyspark-e579e1bb-c845-45ed-ad34-85e5819f581c
20/12/04 15:35:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-8bdeef23-831f-4b72-b280-ef264f24198f
20/12/04 15:35:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 15:35:28 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:35:28 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:35:28 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:35:28 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:35:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:35:29 INFO SparkContext: Running Spark version 3.0.1
20/12/04 15:35:29 INFO ResourceUtils: ==============================================================
20/12/04 15:35:29 INFO ResourceUtils: Resources for spark.driver:

20/12/04 15:35:29 INFO ResourceUtils: ==============================================================
20/12/04 15:35:29 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 15:35:29 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:35:29 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:35:29 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:35:29 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:35:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:35:29 INFO Utils: Successfully started service 'sparkDriver' on port 42691.
20/12/04 15:35:29 INFO SparkEnv: Registering MapOutputTracker
20/12/04 15:35:29 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 15:35:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 15:35:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 15:35:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 15:35:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dd4797c6-3f26-4f84-9c09-9123cf85b119
20/12/04 15:35:29 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 15:35:29 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 15:35:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 15:35:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 15:35:30 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 15:35:31 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 15:35:31 INFO Configuration: resource-types.xml not found
20/12/04 15:35:31 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 15:35:31 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 15:35:31 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 15:35:31 INFO Client: Setting up container launch context for our AM
20/12/04 15:35:31 INFO Client: Setting up the launch environment for our AM container
20/12/04 15:35:31 INFO Client: Preparing resources for our AM container
20/12/04 15:35:31 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 15:35:33 INFO Client: Uploading resource file:/tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/__spark_libs__5768863709576952404.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0061/__spark_libs__5768863709576952404.zip
20/12/04 15:35:34 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0061/pyspark.zip
20/12/04 15:35:34 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0061/py4j-0.10.9-src.zip
20/12/04 15:35:35 INFO Client: Uploading resource file:/tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/__spark_conf__7410392472032085163.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0061/__spark_conf__.zip
20/12/04 15:35:35 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:35:35 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:35:35 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:35:35 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:35:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:35:35 INFO Client: Submitting application application_1607015337794_0061 to ResourceManager
20/12/04 15:35:35 INFO YarnClientImpl: Submitted application application_1607015337794_0061
20/12/04 15:35:36 INFO Client: Application report for application_1607015337794_0061 (state: ACCEPTED)
20/12/04 15:35:36 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114135238
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0061/
	 user: bsuconn
20/12/04 15:35:37 INFO Client: Application report for application_1607015337794_0061 (state: ACCEPTED)
20/12/04 15:35:38 INFO Client: Application report for application_1607015337794_0061 (state: ACCEPTED)
20/12/04 15:35:39 INFO Client: Application report for application_1607015337794_0061 (state: ACCEPTED)
20/12/04 15:35:39 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0061), /proxy/application_1607015337794_0061
20/12/04 15:35:40 INFO Client: Application report for application_1607015337794_0061 (state: RUNNING)
20/12/04 15:35:40 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114135238
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0061/
	 user: bsuconn
20/12/04 15:35:40 INFO YarnClientSchedulerBackend: Application application_1607015337794_0061 has started running.
20/12/04 15:35:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44477.
20/12/04 15:35:40 INFO NettyBlockTransferService: Server created on 192.168.1.9:44477
20/12/04 15:35:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 15:35:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 44477, None)
20/12/04 15:35:40 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:44477 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 44477, None)
20/12/04 15:35:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 44477, None)
20/12/04 15:35:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 44477, None)
20/12/04 15:35:40 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:35:40 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 15:35:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 15:35:45 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:55024) with ID 1
20/12/04 15:35:46 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:43253 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 43253, None)
20/12/04 15:35:47 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:55028) with ID 2
20/12/04 15:35:47 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:32845 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 32845, None)
20/12/04 15:36:00 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
20/12/04 15:36:00 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 15:36:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 15:36:00 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 15:36:00 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:36:00 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:36:00 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:36:00 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:36:00 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:36:01 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:42691/files/darima.zip with timestamp 1607114161374
20/12/04 15:36:01 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/userFiles-7b988eef-a84a-4cc1-90d8-cc04eda52f7f/darima.zip
20/12/04 15:36:03 INFO InMemoryFileIndex: It took 73 ms to list leaf files for 1 paths.
20/12/04 15:36:05 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/12/04 15:36:05 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:36:05 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:36:05 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:36:05 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:36:06 INFO CodeGenerator: Code generated in 247.267341 ms
20/12/04 15:36:06 INFO CodeGenerator: Code generated in 32.654752 ms
20/12/04 15:36:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:36:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:36:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:44477 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:36:06 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 15:36:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:36:07 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 15:36:07 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 15:36:07 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 15:36:07 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 15:36:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 15:36:07 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 15:36:07 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:36:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 15:36:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 15:36:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:44477 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:36:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 15:36:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:36:07 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 15:36:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:36:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:32845 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:36:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:32845 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:36:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3316 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 15:36:10 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 15:36:10 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.463 s
20/12/04 15:36:10 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:36:10 INFO DAGScheduler: running: Set()
20/12/04 15:36:10 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 15:36:10 INFO DAGScheduler: failed: Set()
20/12/04 15:36:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:36:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 15:36:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 15:36:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:44477 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:36:10 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 15:36:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:36:10 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 15:36:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:36:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:43253 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:36:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:55024
20/12/04 15:36:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1705 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:36:12 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 15:36:12 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 1.722 s
20/12/04 15:36:12 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 15:36:12 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 15:36:12 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 5.275804 s
20/12/04 15:36:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:44477 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:36:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:32845 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:36:14 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:32845 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:36:14 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:44477 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:36:14 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:44477 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:36:14 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:43253 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:36:14 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 15:36:15 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:36:15 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:36:15 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:36:15 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:36:15 INFO CodeGenerator: Code generated in 26.519758 ms
20/12/04 15:36:15 INFO CodeGenerator: Code generated in 17.035001 ms
20/12/04 15:36:16 INFO CodeGenerator: Code generated in 22.807649 ms
20/12/04 15:36:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:36:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:36:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:44477 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:36:16 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/userFiles-7b988eef-a84a-4cc1-90d8-cc04eda52f7f/darima.zip/darima/dlsa.py:22
20/12/04 15:36:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:36:16 INFO SparkContext: Starting job: toPandas at /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/userFiles-7b988eef-a84a-4cc1-90d8-cc04eda52f7f/darima.zip/darima/dlsa.py:22
20/12/04 15:36:16 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/userFiles-7b988eef-a84a-4cc1-90d8-cc04eda52f7f/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 15:36:16 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/userFiles-7b988eef-a84a-4cc1-90d8-cc04eda52f7f/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 15:36:16 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/userFiles-7b988eef-a84a-4cc1-90d8-cc04eda52f7f/darima.zip/darima/dlsa.py:22)
20/12/04 15:36:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 15:36:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 15:36:16 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/userFiles-7b988eef-a84a-4cc1-90d8-cc04eda52f7f/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:36:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 15:36:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 15:36:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:44477 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 15:36:16 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 15:36:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/userFiles-7b988eef-a84a-4cc1-90d8-cc04eda52f7f/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 15:36:16 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 15:36:16 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:36:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:43253 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 15:36:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:43253 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:36:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2776 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:36:18 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 15:36:18 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50655
20/12/04 15:36:18 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/userFiles-7b988eef-a84a-4cc1-90d8-cc04eda52f7f/darima.zip/darima/dlsa.py:22) finished in 2.799 s
20/12/04 15:36:18 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:36:18 INFO DAGScheduler: running: Set()
20/12/04 15:36:18 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 15:36:18 INFO DAGScheduler: failed: Set()
20/12/04 15:36:18 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/userFiles-7b988eef-a84a-4cc1-90d8-cc04eda52f7f/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:36:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 15:36:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 15:36:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:44477 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 15:36:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 15:36:19 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/userFiles-7b988eef-a84a-4cc1-90d8-cc04eda52f7f/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 15:36:19 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 15:36:19 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:36:19 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:36:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:32845 (size: 131.5 KiB, free: 912.2 MiB)
20/12/04 15:36:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:43253 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 15:36:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:55024
20/12/04 15:36:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:55028
20/12/04 15:36:25 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:36:25 WARN TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 15:36:26 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:36:26 INFO TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 15:36:27 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:36:27 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 15:36:28 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 8, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:36:28 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 15:36:28 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:36:28 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 15:36:29 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 10, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:36:29 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 8) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 15:36:30 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 11, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:36:30 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 15:36:30 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 12, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:36:30 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 10) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/04 15:36:31 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 13, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:36:31 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 11) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/04 15:36:32 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 14, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:36:32 INFO TaskSetManager: Lost task 2.3 in stage 3.0 (TID 12) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 9]
20/12/04 15:36:32 ERROR TaskSetManager: Task 2 in stage 3.0 failed 4 times; aborting job
20/12/04 15:36:32 INFO YarnScheduler: Cancelling stage 3
20/12/04 15:36:32 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 15:36:32 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 15:36:32 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/userFiles-7b988eef-a84a-4cc1-90d8-cc04eda52f7f/darima.zip/darima/dlsa.py:22) failed in 13.377 s due to Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 12, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 15:36:32 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/userFiles-7b988eef-a84a-4cc1-90d8-cc04eda52f7f/darima.zip/darima/dlsa.py:22, took 16.246790 s
20/12/04 15:36:32 WARN TaskSetManager: Lost task 3.3 in stage 3.0 (TID 14, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/04 15:36:32 WARN TaskSetManager: Lost task 0.3 in stage 3.0 (TID 13, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 15:36:32 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 15:36:33 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 15:36:33 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 15:36:33 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 15:36:33 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 15:36:33 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 15:36:33 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 15:36:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 15:36:33 INFO MemoryStore: MemoryStore cleared
20/12/04 15:36:33 INFO BlockManager: BlockManager stopped
20/12/04 15:36:33 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 15:36:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 15:36:33 WARN Dispatcher: Message RemoteProcessDisconnected(192.168.1.9:55024) dropped. Could not find OutputCommitCoordinator.
20/12/04 15:36:33 INFO SparkContext: Successfully stopped SparkContext
20/12/04 15:36:33 INFO ShutdownHookManager: Shutdown hook called
20/12/04 15:36:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-bda1dd11-6f0b-4fdf-88f1-af9549cc3712
20/12/04 15:36:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70
20/12/04 15:36:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-e5c703ce-a3ce-4f33-bbea-407806950c70/pyspark-13a5d2d3-f451-47b5-981c-4db45a6b6801
20/12/04 15:38:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 15:38:57 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:38:57 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:38:57 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:38:57 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:38:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:38:58 INFO SparkContext: Running Spark version 3.0.1
20/12/04 15:38:58 INFO ResourceUtils: ==============================================================
20/12/04 15:38:58 INFO ResourceUtils: Resources for spark.driver:

20/12/04 15:38:58 INFO ResourceUtils: ==============================================================
20/12/04 15:38:58 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 15:38:58 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:38:58 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:38:58 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:38:58 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:38:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:38:58 INFO Utils: Successfully started service 'sparkDriver' on port 40447.
20/12/04 15:38:58 INFO SparkEnv: Registering MapOutputTracker
20/12/04 15:38:58 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 15:38:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 15:38:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 15:38:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 15:38:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d677fe78-2e1b-4d7e-b60e-63411722ec6a
20/12/04 15:38:58 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 15:38:58 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 15:38:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 15:38:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 15:38:59 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 15:39:00 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 15:39:00 INFO Configuration: resource-types.xml not found
20/12/04 15:39:00 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 15:39:00 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 15:39:00 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 15:39:00 INFO Client: Setting up container launch context for our AM
20/12/04 15:39:00 INFO Client: Setting up the launch environment for our AM container
20/12/04 15:39:00 INFO Client: Preparing resources for our AM container
20/12/04 15:39:00 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 15:39:03 INFO Client: Uploading resource file:/tmp/spark-2a18d02b-b339-423f-8524-99a8c1f94bd2/__spark_libs__8949137168967364507.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0062/__spark_libs__8949137168967364507.zip
20/12/04 15:39:04 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0062/pyspark.zip
20/12/04 15:39:05 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0062/py4j-0.10.9-src.zip
20/12/04 15:39:05 INFO Client: Uploading resource file:/tmp/spark-2a18d02b-b339-423f-8524-99a8c1f94bd2/__spark_conf__9809788347163491199.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0062/__spark_conf__.zip
20/12/04 15:39:05 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:39:05 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:39:05 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:39:05 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:39:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:39:05 INFO Client: Submitting application application_1607015337794_0062 to ResourceManager
20/12/04 15:39:05 INFO YarnClientImpl: Submitted application application_1607015337794_0062
20/12/04 15:39:06 INFO Client: Application report for application_1607015337794_0062 (state: ACCEPTED)
20/12/04 15:39:06 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114345959
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0062/
	 user: bsuconn
20/12/04 15:39:07 INFO Client: Application report for application_1607015337794_0062 (state: ACCEPTED)
20/12/04 15:39:08 INFO Client: Application report for application_1607015337794_0062 (state: ACCEPTED)
20/12/04 15:39:09 INFO Client: Application report for application_1607015337794_0062 (state: ACCEPTED)
20/12/04 15:39:10 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0062), /proxy/application_1607015337794_0062
20/12/04 15:39:10 INFO Client: Application report for application_1607015337794_0062 (state: RUNNING)
20/12/04 15:39:10 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114345959
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0062/
	 user: bsuconn
20/12/04 15:39:11 INFO YarnClientSchedulerBackend: Application application_1607015337794_0062 has started running.
20/12/04 15:39:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39625.
20/12/04 15:39:11 INFO NettyBlockTransferService: Server created on 192.168.1.9:39625
20/12/04 15:39:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 15:39:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 39625, None)
20/12/04 15:39:11 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:39625 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 39625, None)
20/12/04 15:39:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 39625, None)
20/12/04 15:39:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 39625, None)
20/12/04 15:39:11 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:39:11 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 15:39:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 15:39:15 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:60416) with ID 1
20/12/04 15:39:15 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 15:39:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40751 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 40751, None)
20/12/04 15:39:15 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 15:39:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 15:39:16 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 15:39:16 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:39:16 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:39:16 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:39:16 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:39:16 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:39:16 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:40447/files/darima.zip with timestamp 1607114356820
20/12/04 15:39:16 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-2a18d02b-b339-423f-8524-99a8c1f94bd2/userFiles-bcb9daac-e27a-460a-b754-42322121087a/darima.zip
20/12/04 15:39:20 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 15:39:20 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 15:39:20 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 15:39:20 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 15:39:20 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 15:39:20 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 15:39:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 15:39:20 INFO MemoryStore: MemoryStore cleared
20/12/04 15:39:20 INFO BlockManager: BlockManager stopped
20/12/04 15:39:20 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 15:39:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 15:39:20 INFO SparkContext: Successfully stopped SparkContext
20/12/04 15:39:20 INFO ShutdownHookManager: Shutdown hook called
20/12/04 15:39:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-19c0cf51-aa95-41dd-bfff-726268ccd541
20/12/04 15:39:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-2a18d02b-b339-423f-8524-99a8c1f94bd2
20/12/04 15:39:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-2a18d02b-b339-423f-8524-99a8c1f94bd2/pyspark-68828ec2-a191-4963-8234-290ecfe38d36
20/12/04 15:39:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 15:39:22 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:39:22 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:39:22 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:39:22 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:39:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:39:23 INFO SparkContext: Running Spark version 3.0.1
20/12/04 15:39:23 INFO ResourceUtils: ==============================================================
20/12/04 15:39:23 INFO ResourceUtils: Resources for spark.driver:

20/12/04 15:39:23 INFO ResourceUtils: ==============================================================
20/12/04 15:39:23 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 15:39:23 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:39:23 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:39:23 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:39:23 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:39:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:39:23 INFO Utils: Successfully started service 'sparkDriver' on port 44573.
20/12/04 15:39:24 INFO SparkEnv: Registering MapOutputTracker
20/12/04 15:39:24 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 15:39:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 15:39:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 15:39:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 15:39:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5b612b79-21c0-4c36-8c7f-7500c22c8558
20/12/04 15:39:24 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 15:39:24 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 15:39:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 15:39:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 15:39:25 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 15:39:25 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 15:39:25 INFO Configuration: resource-types.xml not found
20/12/04 15:39:25 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 15:39:25 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 15:39:26 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 15:39:26 INFO Client: Setting up container launch context for our AM
20/12/04 15:39:26 INFO Client: Setting up the launch environment for our AM container
20/12/04 15:39:26 INFO Client: Preparing resources for our AM container
20/12/04 15:39:26 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 15:39:28 INFO Client: Uploading resource file:/tmp/spark-ec5f8883-1432-4b5d-aeec-4e46a520ab4c/__spark_libs__11824182659139230100.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0063/__spark_libs__11824182659139230100.zip
20/12/04 15:39:29 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0063/pyspark.zip
20/12/04 15:39:30 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0063/py4j-0.10.9-src.zip
20/12/04 15:39:30 INFO Client: Uploading resource file:/tmp/spark-ec5f8883-1432-4b5d-aeec-4e46a520ab4c/__spark_conf__14810563876254333459.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0063/__spark_conf__.zip
20/12/04 15:39:30 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:39:30 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:39:30 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:39:30 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:39:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:39:30 INFO Client: Submitting application application_1607015337794_0063 to ResourceManager
20/12/04 15:39:30 INFO YarnClientImpl: Submitted application application_1607015337794_0063
20/12/04 15:39:31 INFO Client: Application report for application_1607015337794_0063 (state: ACCEPTED)
20/12/04 15:39:31 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114370462
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0063/
	 user: bsuconn
20/12/04 15:39:32 INFO Client: Application report for application_1607015337794_0063 (state: ACCEPTED)
20/12/04 15:39:33 INFO Client: Application report for application_1607015337794_0063 (state: ACCEPTED)
20/12/04 15:39:34 INFO Client: Application report for application_1607015337794_0063 (state: ACCEPTED)
20/12/04 15:39:35 INFO Client: Application report for application_1607015337794_0063 (state: ACCEPTED)
20/12/04 15:39:35 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0063), /proxy/application_1607015337794_0063
20/12/04 15:39:36 INFO Client: Application report for application_1607015337794_0063 (state: RUNNING)
20/12/04 15:39:36 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114370462
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0063/
	 user: bsuconn
20/12/04 15:39:36 INFO YarnClientSchedulerBackend: Application application_1607015337794_0063 has started running.
20/12/04 15:39:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37823.
20/12/04 15:39:36 INFO NettyBlockTransferService: Server created on 192.168.1.9:37823
20/12/04 15:39:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 15:39:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 37823, None)
20/12/04 15:39:36 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:37823 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 37823, None)
20/12/04 15:39:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 37823, None)
20/12/04 15:39:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 37823, None)
20/12/04 15:39:36 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:39:37 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 15:39:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 15:39:42 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:36648) with ID 1
20/12/04 15:39:42 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:36263 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 36263, None)
20/12/04 15:39:43 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:36652) with ID 2
20/12/04 15:39:43 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 15:39:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:42137 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 42137, None)
20/12/04 15:39:43 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 15:39:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 15:39:43 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 15:39:43 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:39:43 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:39:43 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:39:43 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:39:43 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:39:44 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:44573/files/darima.zip with timestamp 1607114384618
20/12/04 15:39:44 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-ec5f8883-1432-4b5d-aeec-4e46a520ab4c/userFiles-bf464111-2e66-4ab0-956a-9537e2f15c07/darima.zip
20/12/04 15:39:47 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 15:39:47 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 15:39:47 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 15:39:47 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 15:39:47 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 15:39:48 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 15:39:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 15:39:48 INFO MemoryStore: MemoryStore cleared
20/12/04 15:39:48 INFO BlockManager: BlockManager stopped
20/12/04 15:39:48 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 15:39:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 15:39:48 INFO SparkContext: Successfully stopped SparkContext
20/12/04 15:39:48 INFO ShutdownHookManager: Shutdown hook called
20/12/04 15:39:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-ec5f8883-1432-4b5d-aeec-4e46a520ab4c
20/12/04 15:39:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-ec5f8883-1432-4b5d-aeec-4e46a520ab4c/pyspark-3be7f823-1f7c-4021-9f46-ac2c3673411a
20/12/04 15:39:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-8d032878-1fef-4c88-92c6-a3b2fc69822c
20/12/04 15:39:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 15:39:50 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:39:50 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:39:50 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:39:50 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:39:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:39:51 INFO SparkContext: Running Spark version 3.0.1
20/12/04 15:39:51 INFO ResourceUtils: ==============================================================
20/12/04 15:39:51 INFO ResourceUtils: Resources for spark.driver:

20/12/04 15:39:51 INFO ResourceUtils: ==============================================================
20/12/04 15:39:51 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 15:39:51 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:39:51 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:39:51 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:39:51 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:39:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:39:52 INFO Utils: Successfully started service 'sparkDriver' on port 40827.
20/12/04 15:39:52 INFO SparkEnv: Registering MapOutputTracker
20/12/04 15:39:52 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 15:39:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 15:39:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 15:39:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 15:39:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f909755a-900e-41e6-8a9f-9d8cbba4cddf
20/12/04 15:39:52 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 15:39:52 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 15:39:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 15:39:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 15:39:53 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 15:39:53 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 15:39:53 INFO Configuration: resource-types.xml not found
20/12/04 15:39:53 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 15:39:53 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 15:39:53 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 15:39:53 INFO Client: Setting up container launch context for our AM
20/12/04 15:39:53 INFO Client: Setting up the launch environment for our AM container
20/12/04 15:39:53 INFO Client: Preparing resources for our AM container
20/12/04 15:39:54 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 15:39:55 INFO Client: Uploading resource file:/tmp/spark-2db1a191-39eb-437e-b10b-d2cc9e59cf9a/__spark_libs__8991616023724789706.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0064/__spark_libs__8991616023724789706.zip
20/12/04 15:39:56 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0064/pyspark.zip
20/12/04 15:39:57 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0064/py4j-0.10.9-src.zip
20/12/04 15:39:57 INFO Client: Uploading resource file:/tmp/spark-2db1a191-39eb-437e-b10b-d2cc9e59cf9a/__spark_conf__3459357525937514551.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0064/__spark_conf__.zip
20/12/04 15:39:57 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:39:57 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:39:57 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:39:57 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:39:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:39:57 INFO Client: Submitting application application_1607015337794_0064 to ResourceManager
20/12/04 15:39:57 INFO YarnClientImpl: Submitted application application_1607015337794_0064
20/12/04 15:39:58 INFO Client: Application report for application_1607015337794_0064 (state: ACCEPTED)
20/12/04 15:39:58 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114397433
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0064/
	 user: bsuconn
20/12/04 15:39:59 INFO Client: Application report for application_1607015337794_0064 (state: ACCEPTED)
20/12/04 15:40:00 INFO Client: Application report for application_1607015337794_0064 (state: ACCEPTED)
20/12/04 15:40:01 INFO Client: Application report for application_1607015337794_0064 (state: ACCEPTED)
20/12/04 15:40:02 INFO Client: Application report for application_1607015337794_0064 (state: RUNNING)
20/12/04 15:40:02 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114397433
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0064/
	 user: bsuconn
20/12/04 15:40:02 INFO YarnClientSchedulerBackend: Application application_1607015337794_0064 has started running.
20/12/04 15:40:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39815.
20/12/04 15:40:02 INFO NettyBlockTransferService: Server created on 192.168.1.9:39815
20/12/04 15:40:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 15:40:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 39815, None)
20/12/04 15:40:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:39815 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 39815, None)
20/12/04 15:40:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 39815, None)
20/12/04 15:40:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 39815, None)
20/12/04 15:40:02 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0064), /proxy/application_1607015337794_0064
20/12/04 15:40:02 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:40:03 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 15:40:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 15:40:08 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:50554) with ID 1
20/12/04 15:40:08 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:42547 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 42547, None)
20/12/04 15:40:09 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:50558) with ID 2
20/12/04 15:40:09 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:37905 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 37905, None)
20/12/04 15:40:22 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
20/12/04 15:40:23 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 15:40:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 15:40:23 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 15:40:23 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:40:23 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:40:23 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:40:23 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:40:23 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:40:23 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:40827/files/darima.zip with timestamp 1607114423937
20/12/04 15:40:23 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-2db1a191-39eb-437e-b10b-d2cc9e59cf9a/userFiles-108ac2fa-321a-4c35-8fc2-164f637e77ab/darima.zip
20/12/04 15:40:27 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 15:40:27 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 15:40:27 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 15:40:27 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 15:40:27 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 15:40:27 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 15:40:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 15:40:27 INFO MemoryStore: MemoryStore cleared
20/12/04 15:40:27 INFO BlockManager: BlockManager stopped
20/12/04 15:40:27 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 15:40:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 15:40:27 INFO SparkContext: Successfully stopped SparkContext
20/12/04 15:40:27 INFO ShutdownHookManager: Shutdown hook called
20/12/04 15:40:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d42e021-404c-4a71-99cf-27700bcd48a6
20/12/04 15:40:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-2db1a191-39eb-437e-b10b-d2cc9e59cf9a
20/12/04 15:40:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-2db1a191-39eb-437e-b10b-d2cc9e59cf9a/pyspark-e9b82c47-346f-4f7c-ad12-0f5ce3c35030
20/12/04 15:45:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 15:45:36 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:45:36 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:45:36 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:45:36 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:45:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:45:37 INFO SparkContext: Running Spark version 3.0.1
20/12/04 15:45:37 INFO ResourceUtils: ==============================================================
20/12/04 15:45:37 INFO ResourceUtils: Resources for spark.driver:

20/12/04 15:45:37 INFO ResourceUtils: ==============================================================
20/12/04 15:45:37 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 15:45:37 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:45:37 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:45:37 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:45:37 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:45:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:45:37 INFO Utils: Successfully started service 'sparkDriver' on port 40347.
20/12/04 15:45:38 INFO SparkEnv: Registering MapOutputTracker
20/12/04 15:45:38 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 15:45:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 15:45:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 15:45:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 15:45:38 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-03a8b538-3112-49a9-a256-4763dad689f0
20/12/04 15:45:38 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 15:45:38 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 15:45:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 15:45:38 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 15:45:39 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 15:45:39 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 15:45:39 INFO Configuration: resource-types.xml not found
20/12/04 15:45:39 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 15:45:39 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 15:45:39 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 15:45:39 INFO Client: Setting up container launch context for our AM
20/12/04 15:45:39 INFO Client: Setting up the launch environment for our AM container
20/12/04 15:45:39 INFO Client: Preparing resources for our AM container
20/12/04 15:45:39 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 15:45:43 INFO Client: Uploading resource file:/tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/__spark_libs__14069821132626344228.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0065/__spark_libs__14069821132626344228.zip
20/12/04 15:45:44 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0065/pyspark.zip
20/12/04 15:45:44 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0065/py4j-0.10.9-src.zip
20/12/04 15:45:44 INFO Client: Uploading resource file:/tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/__spark_conf__14626413171346630775.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0065/__spark_conf__.zip
20/12/04 15:45:45 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:45:45 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:45:45 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:45:45 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:45:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:45:45 INFO Client: Submitting application application_1607015337794_0065 to ResourceManager
20/12/04 15:45:45 INFO YarnClientImpl: Submitted application application_1607015337794_0065
20/12/04 15:45:46 INFO Client: Application report for application_1607015337794_0065 (state: ACCEPTED)
20/12/04 15:45:46 INFO Client: 
	 client token: N/A
	 diagnostics: [Fri Dec 04 15:45:46 -0500 2020] Scheduler has assigned a container for AM, waiting for AM container to be launched
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114745494
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0065/
	 user: bsuconn
20/12/04 15:45:47 INFO Client: Application report for application_1607015337794_0065 (state: ACCEPTED)
20/12/04 15:45:48 INFO Client: Application report for application_1607015337794_0065 (state: ACCEPTED)
20/12/04 15:45:49 INFO Client: Application report for application_1607015337794_0065 (state: ACCEPTED)
20/12/04 15:45:50 INFO Client: Application report for application_1607015337794_0065 (state: ACCEPTED)
20/12/04 15:45:51 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0065), /proxy/application_1607015337794_0065
20/12/04 15:45:51 INFO Client: Application report for application_1607015337794_0065 (state: RUNNING)
20/12/04 15:45:51 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114745494
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0065/
	 user: bsuconn
20/12/04 15:45:51 INFO YarnClientSchedulerBackend: Application application_1607015337794_0065 has started running.
20/12/04 15:45:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42363.
20/12/04 15:45:51 INFO NettyBlockTransferService: Server created on 192.168.1.9:42363
20/12/04 15:45:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 15:45:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 42363, None)
20/12/04 15:45:51 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:42363 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 42363, None)
20/12/04 15:45:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 42363, None)
20/12/04 15:45:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 42363, None)
20/12/04 15:45:51 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:45:52 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 15:45:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 15:45:58 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:48916) with ID 1
20/12/04 15:45:58 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 15:45:58 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:46071 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 46071, None)
20/12/04 15:45:58 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 15:45:58 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 15:45:58 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 15:45:58 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:45:58 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:45:58 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:45:58 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:45:58 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:45:59 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:40347/files/darima.zip with timestamp 1607114759667
20/12/04 15:45:59 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/userFiles-f4d14f6f-e85f-47c0-a71c-09a0430cf72c/darima.zip
20/12/04 15:46:02 INFO InMemoryFileIndex: It took 55 ms to list leaf files for 1 paths.
20/12/04 15:46:04 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/12/04 15:46:04 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:46:04 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:46:04 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:46:04 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:46:05 INFO CodeGenerator: Code generated in 286.884131 ms
20/12/04 15:46:05 INFO CodeGenerator: Code generated in 33.402333 ms
20/12/04 15:46:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:46:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:46:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:42363 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:46:05 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 15:46:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:46:06 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 15:46:06 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 15:46:06 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 15:46:06 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 15:46:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 15:46:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 15:46:06 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:46:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 15:46:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 15:46:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:42363 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:46:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 15:46:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:46:06 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 15:46:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:46:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:46071 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:46:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:46071 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:46:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3630 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:46:09 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 15:46:09 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.784 s
20/12/04 15:46:09 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:46:09 INFO DAGScheduler: running: Set()
20/12/04 15:46:09 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 15:46:09 INFO DAGScheduler: failed: Set()
20/12/04 15:46:09 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:46:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 15:46:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 15:46:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:42363 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:46:09 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 15:46:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:46:09 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 15:46:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:46:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:46071 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:46:10 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:48916
20/12/04 15:46:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 421 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:46:10 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 15:46:10 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.441 s
20/12/04 15:46:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 15:46:10 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 15:46:10 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 4.330770 s
20/12/04 15:46:13 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:42363 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:46:13 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:46071 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:46:13 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:42363 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:46:13 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:46071 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:46:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 15:46:14 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:46:14 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:46:14 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:46:14 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:46:14 INFO CodeGenerator: Code generated in 30.658976 ms
20/12/04 15:46:14 INFO CodeGenerator: Code generated in 18.505468 ms
20/12/04 15:46:14 INFO CodeGenerator: Code generated in 27.112583 ms
20/12/04 15:46:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:46:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:46:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:42363 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:46:14 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/userFiles-f4d14f6f-e85f-47c0-a71c-09a0430cf72c/darima.zip/darima/dlsa.py:22
20/12/04 15:46:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:46:14 INFO SparkContext: Starting job: toPandas at /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/userFiles-f4d14f6f-e85f-47c0-a71c-09a0430cf72c/darima.zip/darima/dlsa.py:22
20/12/04 15:46:14 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/userFiles-f4d14f6f-e85f-47c0-a71c-09a0430cf72c/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 15:46:14 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/userFiles-f4d14f6f-e85f-47c0-a71c-09a0430cf72c/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 15:46:14 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/userFiles-f4d14f6f-e85f-47c0-a71c-09a0430cf72c/darima.zip/darima/dlsa.py:22)
20/12/04 15:46:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 15:46:14 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 15:46:14 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/userFiles-f4d14f6f-e85f-47c0-a71c-09a0430cf72c/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:46:14 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.2 KiB, free 5.8 GiB)
20/12/04 15:46:14 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 15:46:14 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:42363 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 15:46:14 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 15:46:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/userFiles-f4d14f6f-e85f-47c0-a71c-09a0430cf72c/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 15:46:14 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 15:46:14 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:46:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:46071 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 15:46:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:46071 (size: 28.7 KiB, free: 912.2 MiB)
20/12/04 15:46:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1481 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:46:16 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 15:46:16 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 36551
20/12/04 15:46:16 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/userFiles-f4d14f6f-e85f-47c0-a71c-09a0430cf72c/darima.zip/darima/dlsa.py:22) finished in 1.509 s
20/12/04 15:46:16 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:46:16 INFO DAGScheduler: running: Set()
20/12/04 15:46:16 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 15:46:16 INFO DAGScheduler: failed: Set()
20/12/04 15:46:16 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/userFiles-f4d14f6f-e85f-47c0-a71c-09a0430cf72c/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:46:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.0 KiB, free 5.8 GiB)
20/12/04 15:46:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.4 KiB, free 5.8 GiB)
20/12/04 15:46:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:42363 (size: 131.4 KiB, free: 5.8 GiB)
20/12/04 15:46:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 15:46:16 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/userFiles-f4d14f6f-e85f-47c0-a71c-09a0430cf72c/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 15:46:16 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 15:46:16 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:46:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:46071 (size: 131.4 KiB, free: 912.1 MiB)
20/12/04 15:46:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:48916
20/12/04 15:46:20 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:46:20 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 15:46:21 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:46:21 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 15:46:21 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:46:21 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 15:46:22 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 15:46:22 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:46:23 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:46:23 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 15:46:24 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 15:46:24 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:46:25 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:46:25 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 15:46:25 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 15:46:25 INFO YarnScheduler: Cancelling stage 3
20/12/04 15:46:25 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 15:46:25 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 15:46:25 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/userFiles-f4d14f6f-e85f-47c0-a71c-09a0430cf72c/darima.zip/darima/dlsa.py:22) failed in 8.749 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 15:46:25 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/userFiles-f4d14f6f-e85f-47c0-a71c-09a0430cf72c/darima.zip/darima/dlsa.py:22, took 10.331720 s
20/12/04 15:46:25 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 15:46:25 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 15:46:25 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 15:46:25 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 15:46:25 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 15:46:25 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 15:46:25 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 15:46:25 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 15:46:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 15:46:25 INFO MemoryStore: MemoryStore cleared
20/12/04 15:46:25 INFO BlockManager: BlockManager stopped
20/12/04 15:46:25 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 15:46:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 15:46:26 INFO SparkContext: Successfully stopped SparkContext
20/12/04 15:46:26 INFO ShutdownHookManager: Shutdown hook called
20/12/04 15:46:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-640f0e8b-e941-4954-b8c9-e3208a4233f9
20/12/04 15:46:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a/pyspark-ee4b9e03-f5e0-4ef6-9a42-24c74f6d7727
20/12/04 15:46:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-52931ff0-3992-4359-942f-3e636fc3d58a
20/12/04 15:46:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 15:46:28 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:46:28 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:46:28 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:46:28 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:46:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:46:29 INFO SparkContext: Running Spark version 3.0.1
20/12/04 15:46:29 INFO ResourceUtils: ==============================================================
20/12/04 15:46:29 INFO ResourceUtils: Resources for spark.driver:

20/12/04 15:46:29 INFO ResourceUtils: ==============================================================
20/12/04 15:46:29 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 15:46:29 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:46:29 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:46:29 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:46:29 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:46:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:46:30 INFO Utils: Successfully started service 'sparkDriver' on port 37575.
20/12/04 15:46:30 INFO SparkEnv: Registering MapOutputTracker
20/12/04 15:46:30 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 15:46:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 15:46:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 15:46:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 15:46:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2716f685-e5d5-4801-829f-3576b89e4aee
20/12/04 15:46:30 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 15:46:30 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 15:46:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 15:46:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 15:46:31 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 15:46:31 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 15:46:31 INFO Configuration: resource-types.xml not found
20/12/04 15:46:31 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 15:46:31 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 15:46:31 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 15:46:31 INFO Client: Setting up container launch context for our AM
20/12/04 15:46:31 INFO Client: Setting up the launch environment for our AM container
20/12/04 15:46:31 INFO Client: Preparing resources for our AM container
20/12/04 15:46:32 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 15:46:33 INFO DiskBlockManager: Shutdown hook called
20/12/04 15:46:33 INFO ShutdownHookManager: Shutdown hook called
20/12/04 15:46:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-9a4cf73e-0265-46d2-9fb8-9c5f07a924eb/userFiles-65d54b2e-7ba4-4f42-8deb-00c44a4d349e
20/12/04 15:46:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-f62ca1cb-f83f-46f9-bd4d-59832fb1c278
20/12/04 15:46:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-9a4cf73e-0265-46d2-9fb8-9c5f07a924eb
20/12/04 15:47:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 15:47:09 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:47:09 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:47:09 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:47:09 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:47:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:47:09 INFO SparkContext: Running Spark version 3.0.1
20/12/04 15:47:09 INFO ResourceUtils: ==============================================================
20/12/04 15:47:09 INFO ResourceUtils: Resources for spark.driver:

20/12/04 15:47:09 INFO ResourceUtils: ==============================================================
20/12/04 15:47:09 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 15:47:09 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:47:09 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:47:09 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:47:09 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:47:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:47:10 INFO Utils: Successfully started service 'sparkDriver' on port 46101.
20/12/04 15:47:10 INFO SparkEnv: Registering MapOutputTracker
20/12/04 15:47:10 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 15:47:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 15:47:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 15:47:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 15:47:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1efbda23-90bc-443c-86bd-05a472ce6b5b
20/12/04 15:47:10 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 15:47:10 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 15:47:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 15:47:10 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 15:47:11 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 15:47:11 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 15:47:11 INFO Configuration: resource-types.xml not found
20/12/04 15:47:11 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 15:47:12 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 15:47:12 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 15:47:12 INFO Client: Setting up container launch context for our AM
20/12/04 15:47:12 INFO Client: Setting up the launch environment for our AM container
20/12/04 15:47:12 INFO Client: Preparing resources for our AM container
20/12/04 15:47:12 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 15:47:13 INFO Client: Uploading resource file:/tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/__spark_libs__15047664632570325574.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0067/__spark_libs__15047664632570325574.zip
20/12/04 15:47:14 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0067/pyspark.zip
20/12/04 15:47:15 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0067/py4j-0.10.9-src.zip
20/12/04 15:47:15 INFO Client: Uploading resource file:/tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/__spark_conf__6449282692372987936.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0067/__spark_conf__.zip
20/12/04 15:47:15 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 15:47:15 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 15:47:15 INFO SecurityManager: Changing view acls groups to: 
20/12/04 15:47:15 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 15:47:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 15:47:15 INFO Client: Submitting application application_1607015337794_0067 to ResourceManager
20/12/04 15:47:15 INFO YarnClientImpl: Submitted application application_1607015337794_0067
20/12/04 15:47:16 INFO Client: Application report for application_1607015337794_0067 (state: ACCEPTED)
20/12/04 15:47:16 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114835791
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0067/
	 user: bsuconn
20/12/04 15:47:17 INFO Client: Application report for application_1607015337794_0067 (state: ACCEPTED)
20/12/04 15:47:18 INFO Client: Application report for application_1607015337794_0067 (state: ACCEPTED)
20/12/04 15:47:19 INFO Client: Application report for application_1607015337794_0067 (state: ACCEPTED)
20/12/04 15:47:20 INFO Client: Application report for application_1607015337794_0067 (state: ACCEPTED)
20/12/04 15:47:21 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0067), /proxy/application_1607015337794_0067
20/12/04 15:47:21 INFO Client: Application report for application_1607015337794_0067 (state: RUNNING)
20/12/04 15:47:21 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607114835791
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0067/
	 user: bsuconn
20/12/04 15:47:21 INFO YarnClientSchedulerBackend: Application application_1607015337794_0067 has started running.
20/12/04 15:47:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36831.
20/12/04 15:47:21 INFO NettyBlockTransferService: Server created on 192.168.1.9:36831
20/12/04 15:47:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 15:47:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 36831, None)
20/12/04 15:47:21 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:36831 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 36831, None)
20/12/04 15:47:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 36831, None)
20/12/04 15:47:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 36831, None)
20/12/04 15:47:22 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:47:22 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 15:47:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 15:47:27 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:39818) with ID 1
20/12/04 15:47:27 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:43767 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 43767, None)
20/12/04 15:47:28 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:39822) with ID 2
20/12/04 15:47:28 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:46267 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 46267, None)
20/12/04 15:47:41 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
20/12/04 15:47:41 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 15:47:41 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 15:47:41 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 15:47:41 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:47:41 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:47:41 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:47:41 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:47:41 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 15:47:42 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:46101/files/darima.zip with timestamp 1607114862278
20/12/04 15:47:42 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/userFiles-b90fbf9e-e706-4c5b-a575-bb1f38bfaba6/darima.zip
20/12/04 15:47:44 INFO InMemoryFileIndex: It took 79 ms to list leaf files for 1 paths.
20/12/04 15:47:46 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
20/12/04 15:47:47 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:47:47 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:47:47 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:47:47 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:47:48 INFO CodeGenerator: Code generated in 476.115494 ms
20/12/04 15:47:48 INFO CodeGenerator: Code generated in 36.254278 ms
20/12/04 15:47:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:47:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:47:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:36831 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:47:48 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 15:47:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:47:49 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 15:47:49 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 15:47:49 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 15:47:49 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 15:47:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 15:47:49 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 15:47:49 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:47:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 15:47:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 15:47:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:36831 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:47:49 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 15:47:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:47:49 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 15:47:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:47:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:43767 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:47:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:43767 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:47:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4572 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:47:54 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 15:47:54 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 4.805 s
20/12/04 15:47:54 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:47:54 INFO DAGScheduler: running: Set()
20/12/04 15:47:54 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 15:47:54 INFO DAGScheduler: failed: Set()
20/12/04 15:47:54 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 15:47:54 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 15:47:54 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 15:47:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:36831 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:47:54 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 15:47:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 15:47:54 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 15:47:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:47:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:43767 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:47:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:39818
20/12/04 15:47:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 714 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:47:54 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 15:47:54 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.736 s
20/12/04 15:47:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 15:47:54 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 15:47:54 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 5.674444 s
20/12/04 15:47:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:36831 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:47:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:43767 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:47:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:36831 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 15:47:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:43767 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 15:47:58 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:43767 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 15:47:58 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:36831 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 15:47:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 15:47:59 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 15:47:59 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 15:47:59 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 15:47:59 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 15:47:59 INFO CodeGenerator: Code generated in 38.084938 ms
20/12/04 15:47:59 INFO CodeGenerator: Code generated in 18.638244 ms
20/12/04 15:47:59 INFO CodeGenerator: Code generated in 22.98058 ms
20/12/04 15:47:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 15:47:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 15:47:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:36831 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 15:47:59 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/userFiles-b90fbf9e-e706-4c5b-a575-bb1f38bfaba6/darima.zip/darima/dlsa.py:22
20/12/04 15:47:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 15:47:59 INFO SparkContext: Starting job: toPandas at /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/userFiles-b90fbf9e-e706-4c5b-a575-bb1f38bfaba6/darima.zip/darima/dlsa.py:22
20/12/04 15:47:59 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/userFiles-b90fbf9e-e706-4c5b-a575-bb1f38bfaba6/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 15:47:59 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/userFiles-b90fbf9e-e706-4c5b-a575-bb1f38bfaba6/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 15:47:59 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/userFiles-b90fbf9e-e706-4c5b-a575-bb1f38bfaba6/darima.zip/darima/dlsa.py:22)
20/12/04 15:47:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 15:47:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 15:47:59 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/userFiles-b90fbf9e-e706-4c5b-a575-bb1f38bfaba6/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:47:59 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 15:47:59 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 15:47:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:36831 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 15:47:59 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 15:47:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/userFiles-b90fbf9e-e706-4c5b-a575-bb1f38bfaba6/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 15:47:59 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 15:47:59 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 15:47:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:43767 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 15:48:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:43767 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 15:48:01 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1455 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 15:48:01 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 15:48:01 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 41109
20/12/04 15:48:01 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/userFiles-b90fbf9e-e706-4c5b-a575-bb1f38bfaba6/darima.zip/darima/dlsa.py:22) finished in 1.483 s
20/12/04 15:48:01 INFO DAGScheduler: looking for newly runnable stages
20/12/04 15:48:01 INFO DAGScheduler: running: Set()
20/12/04 15:48:01 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 15:48:01 INFO DAGScheduler: failed: Set()
20/12/04 15:48:01 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/userFiles-b90fbf9e-e706-4c5b-a575-bb1f38bfaba6/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 15:48:01 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 15:48:01 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 15:48:01 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:36831 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 15:48:01 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 15:48:01 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/userFiles-b90fbf9e-e706-4c5b-a575-bb1f38bfaba6/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 15:48:01 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 15:48:01 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:48:01 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:48:01 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:43767 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 15:48:01 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:39818
20/12/04 15:48:02 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:46267 (size: 131.5 KiB, free: 912.2 MiB)
20/12/04 15:48:04 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:39822
20/12/04 15:48:07 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:48:07 WARN TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 15:48:07 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:36831 in memory (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 15:48:07 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:43767 in memory (size: 11.6 KiB, free: 912.1 MiB)
20/12/04 15:48:09 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:48:09 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/04 15:48:10 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:48:10 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/04 15:48:12 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:48:12 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/04 15:48:13 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 9, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:48:13 INFO TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/04 15:48:13 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:48:13 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/04 15:48:15 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 11, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 15:48:15 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 9) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/04 15:48:15 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 15:48:15 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 10) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/04 15:48:16 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 13, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 15:48:16 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 14, 192.168.1.9, executor 2, partition 4, NODE_LOCAL, 7336 bytes)
20/12/04 15:48:16 INFO TaskSetManager: Lost task 2.3 in stage 3.0 (TID 11) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/04 15:48:16 ERROR TaskSetManager: Task 2 in stage 3.0 failed 4 times; aborting job
20/12/04 15:48:16 INFO TaskSetManager: Lost task 3.3 in stage 3.0 (TID 12) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 9]
20/12/04 15:48:16 INFO YarnScheduler: Cancelling stage 3
20/12/04 15:48:16 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 15:48:16 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 15:48:16 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/userFiles-b90fbf9e-e706-4c5b-a575-bb1f38bfaba6/darima.zip/darima/dlsa.py:22) failed in 15.384 s due to Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 11, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 15:48:16 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/userFiles-b90fbf9e-e706-4c5b-a575-bb1f38bfaba6/darima.zip/darima/dlsa.py:22, took 16.953522 s
20/12/04 15:48:16 WARN TaskSetManager: Lost task 4.0 in stage 3.0 (TID 14, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/04 15:48:17 WARN TaskSetManager: Lost task 0.2 in stage 3.0 (TID 13, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 15:48:17 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 15:48:17 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 15:48:17 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 15:48:17 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 15:48:17 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 15:48:17 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 15:48:17 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 15:48:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 15:48:17 INFO MemoryStore: MemoryStore cleared
20/12/04 15:48:17 INFO BlockManager: BlockManager stopped
20/12/04 15:48:17 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 15:48:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 15:48:17 INFO SparkContext: Successfully stopped SparkContext
20/12/04 15:48:17 INFO ShutdownHookManager: Shutdown hook called
20/12/04 15:48:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748/pyspark-780bcecd-6fc7-444b-99d9-75d972cae0e4
20/12/04 15:48:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-7cf0cc30-1836-4031-b013-7630052935b2
20/12/04 15:48:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-18f8460d-2211-44be-8bbc-4b3bdb4a9748
20/12/04 16:01:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 16:01:43 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:01:43 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:01:43 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:01:43 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:01:44 INFO SparkContext: Running Spark version 3.0.1
20/12/04 16:01:44 INFO ResourceUtils: ==============================================================
20/12/04 16:01:44 INFO ResourceUtils: Resources for spark.driver:

20/12/04 16:01:44 INFO ResourceUtils: ==============================================================
20/12/04 16:01:44 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 16:01:44 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:01:44 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:01:44 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:01:44 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:01:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:01:45 INFO Utils: Successfully started service 'sparkDriver' on port 41081.
20/12/04 16:01:45 INFO SparkEnv: Registering MapOutputTracker
20/12/04 16:01:45 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 16:01:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 16:01:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 16:01:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 16:01:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-426172f3-f7eb-42d2-92f1-e08b798e1410
20/12/04 16:01:45 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 16:01:45 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 16:01:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 16:01:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 16:01:46 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 16:01:46 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 16:01:47 INFO Configuration: resource-types.xml not found
20/12/04 16:01:47 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 16:01:47 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 16:01:47 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 16:01:47 INFO Client: Setting up container launch context for our AM
20/12/04 16:01:47 INFO Client: Setting up the launch environment for our AM container
20/12/04 16:01:47 INFO Client: Preparing resources for our AM container
20/12/04 16:01:47 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 16:01:49 INFO Client: Uploading resource file:/tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/__spark_libs__17443259213496273329.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0068/__spark_libs__17443259213496273329.zip
20/12/04 16:01:50 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0068/pyspark.zip
20/12/04 16:01:50 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0068/py4j-0.10.9-src.zip
20/12/04 16:01:50 INFO Client: Uploading resource file:/tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/__spark_conf__3697850399672108050.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0068/__spark_conf__.zip
20/12/04 16:01:50 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:01:50 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:01:50 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:01:50 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:01:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:01:50 INFO Client: Submitting application application_1607015337794_0068 to ResourceManager
20/12/04 16:01:50 INFO YarnClientImpl: Submitted application application_1607015337794_0068
20/12/04 16:01:51 INFO Client: Application report for application_1607015337794_0068 (state: ACCEPTED)
20/12/04 16:01:51 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607115710564
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0068/
	 user: bsuconn
20/12/04 16:01:52 INFO Client: Application report for application_1607015337794_0068 (state: ACCEPTED)
20/12/04 16:01:53 INFO Client: Application report for application_1607015337794_0068 (state: ACCEPTED)
20/12/04 16:01:54 INFO Client: Application report for application_1607015337794_0068 (state: ACCEPTED)
20/12/04 16:01:55 INFO Client: Application report for application_1607015337794_0068 (state: ACCEPTED)
20/12/04 16:01:56 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0068), /proxy/application_1607015337794_0068
20/12/04 16:01:56 INFO Client: Application report for application_1607015337794_0068 (state: RUNNING)
20/12/04 16:01:56 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607115710564
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0068/
	 user: bsuconn
20/12/04 16:01:56 INFO YarnClientSchedulerBackend: Application application_1607015337794_0068 has started running.
20/12/04 16:01:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44883.
20/12/04 16:01:56 INFO NettyBlockTransferService: Server created on 192.168.1.9:44883
20/12/04 16:01:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 16:01:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 44883, None)
20/12/04 16:01:56 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:44883 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 44883, None)
20/12/04 16:01:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 44883, None)
20/12/04 16:01:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 44883, None)
20/12/04 16:01:57 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:01:57 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 16:02:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 16:02:02 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:43922) with ID 1
20/12/04 16:02:02 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 16:02:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:36099 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 36099, None)
20/12/04 16:02:02 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 16:02:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 16:02:02 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 16:02:02 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:02:02 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:02:02 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:02:02 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:02:02 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:02:03 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:41081/files/darima.zip with timestamp 1607115723492
20/12/04 16:02:03 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/userFiles-b809e03a-2890-4f8e-ac3f-c54a986b9947/darima.zip
20/12/04 16:02:05 INFO InMemoryFileIndex: It took 68 ms to list leaf files for 1 paths.
20/12/04 16:02:08 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 1 paths.
20/12/04 16:02:09 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:02:09 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:02:09 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:02:09 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:02:10 INFO CodeGenerator: Code generated in 377.491958 ms
20/12/04 16:02:10 INFO CodeGenerator: Code generated in 46.582404 ms
20/12/04 16:02:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:02:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:02:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:44883 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:02:10 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 16:02:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:02:10 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 16:02:10 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 16:02:10 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 16:02:10 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 16:02:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 16:02:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 16:02:10 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:02:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 16:02:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 16:02:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:44883 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:02:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 16:02:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:02:11 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 16:02:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:02:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:36099 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:02:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:36099 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:02:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4635 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:02:15 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 16:02:15 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 4.883 s
20/12/04 16:02:15 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:02:15 INFO DAGScheduler: running: Set()
20/12/04 16:02:15 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 16:02:15 INFO DAGScheduler: failed: Set()
20/12/04 16:02:15 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:02:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 16:02:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 16:02:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:44883 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:02:15 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 16:02:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:02:15 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 16:02:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:02:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:36099 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:02:16 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:43922
20/12/04 16:02:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 516 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:02:16 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 16:02:16 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.577 s
20/12/04 16:02:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:02:16 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 16:02:16 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 5.633162 s
20/12/04 16:02:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:36099 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:02:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:44883 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:02:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:44883 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:02:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:36099 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:02:20 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:44883 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:02:20 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:36099 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:02:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 16:02:22 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:02:22 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:02:22 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:02:22 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:02:22 INFO CodeGenerator: Code generated in 30.563345 ms
20/12/04 16:02:22 INFO CodeGenerator: Code generated in 24.64017 ms
20/12/04 16:02:22 INFO CodeGenerator: Code generated in 42.969968 ms
20/12/04 16:02:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:02:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:02:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:44883 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:02:22 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/userFiles-b809e03a-2890-4f8e-ac3f-c54a986b9947/darima.zip/darima/dlsa.py:22
20/12/04 16:02:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:02:22 INFO SparkContext: Starting job: toPandas at /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/userFiles-b809e03a-2890-4f8e-ac3f-c54a986b9947/darima.zip/darima/dlsa.py:22
20/12/04 16:02:22 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/userFiles-b809e03a-2890-4f8e-ac3f-c54a986b9947/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 16:02:22 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/userFiles-b809e03a-2890-4f8e-ac3f-c54a986b9947/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 16:02:22 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/userFiles-b809e03a-2890-4f8e-ac3f-c54a986b9947/darima.zip/darima/dlsa.py:22)
20/12/04 16:02:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 16:02:22 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 16:02:22 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/userFiles-b809e03a-2890-4f8e-ac3f-c54a986b9947/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:02:22 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 16:02:22 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 16:02:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:44883 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 16:02:22 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 16:02:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/userFiles-b809e03a-2890-4f8e-ac3f-c54a986b9947/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 16:02:22 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 16:02:22 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:02:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:36099 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 16:02:23 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:36099 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:02:24 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1820 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:02:24 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 16:02:24 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 58725
20/12/04 16:02:24 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/userFiles-b809e03a-2890-4f8e-ac3f-c54a986b9947/darima.zip/darima/dlsa.py:22) finished in 1.862 s
20/12/04 16:02:24 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:02:24 INFO DAGScheduler: running: Set()
20/12/04 16:02:24 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 16:02:24 INFO DAGScheduler: failed: Set()
20/12/04 16:02:24 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/userFiles-b809e03a-2890-4f8e-ac3f-c54a986b9947/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:02:25 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 16:02:25 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 16:02:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:44883 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 16:02:25 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 16:02:25 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/userFiles-b809e03a-2890-4f8e-ac3f-c54a986b9947/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 16:02:25 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 16:02:25 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:02:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:36099 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 16:02:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:43922
20/12/04 16:02:30 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:02:30 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 16:02:31 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:02:31 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 1]
20/12/04 16:02:32 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:02:32 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 2]
20/12/04 16:02:33 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:02:33 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 3]
20/12/04 16:02:35 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:02:35 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 4]
20/12/04 16:02:36 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:02:36 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 5]
20/12/04 16:02:37 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:02:37 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 6]
20/12/04 16:02:37 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 16:02:37 INFO YarnScheduler: Cancelling stage 3
20/12/04 16:02:37 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 16:02:37 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 16:02:37 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/userFiles-b809e03a-2890-4f8e-ac3f-c54a986b9947/darima.zip/darima/dlsa.py:22) failed in 12.338 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 16:02:37 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/userFiles-b809e03a-2890-4f8e-ac3f-c54a986b9947/darima.zip/darima/dlsa.py:22, took 14.316732 s
20/12/04 16:02:37 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 16:02:37 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 16:02:38 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 16:02:38 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 16:02:38 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 16:02:38 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 16:02:38 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 16:02:38 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 16:02:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 16:02:38 INFO MemoryStore: MemoryStore cleared
20/12/04 16:02:38 INFO BlockManager: BlockManager stopped
20/12/04 16:02:38 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 16:02:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 16:02:38 INFO SparkContext: Successfully stopped SparkContext
20/12/04 16:02:38 INFO ShutdownHookManager: Shutdown hook called
20/12/04 16:02:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-441c9b45-72b7-4e4f-96f2-74cb092ddc74
20/12/04 16:02:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de/pyspark-3f772f6c-ccd4-4a68-93fd-e25d66dd0ab2
20/12/04 16:02:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-07ccd5b7-e555-4718-9757-4192dff706de
20/12/04 16:02:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 16:02:43 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:02:43 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:02:43 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:02:43 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:02:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:02:44 INFO SparkContext: Running Spark version 3.0.1
20/12/04 16:02:44 INFO ResourceUtils: ==============================================================
20/12/04 16:02:44 INFO ResourceUtils: Resources for spark.driver:

20/12/04 16:02:44 INFO ResourceUtils: ==============================================================
20/12/04 16:02:44 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 16:02:44 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:02:44 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:02:44 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:02:44 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:02:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:02:45 INFO Utils: Successfully started service 'sparkDriver' on port 39023.
20/12/04 16:02:45 INFO SparkEnv: Registering MapOutputTracker
20/12/04 16:02:45 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 16:02:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 16:02:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 16:02:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 16:02:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fe3ed84b-8090-46bb-90b6-fb1ba661383b
20/12/04 16:02:45 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 16:02:45 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 16:02:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 16:02:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 16:02:46 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 16:02:46 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 16:02:47 INFO Configuration: resource-types.xml not found
20/12/04 16:02:47 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 16:02:47 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 16:02:47 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 16:02:47 INFO Client: Setting up container launch context for our AM
20/12/04 16:02:47 INFO Client: Setting up the launch environment for our AM container
20/12/04 16:02:47 INFO Client: Preparing resources for our AM container
20/12/04 16:02:47 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 16:02:49 INFO Client: Uploading resource file:/tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/__spark_libs__6950812736954813948.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0069/__spark_libs__6950812736954813948.zip
20/12/04 16:02:50 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0069/pyspark.zip
20/12/04 16:02:51 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0069/py4j-0.10.9-src.zip
20/12/04 16:02:51 INFO Client: Uploading resource file:/tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/__spark_conf__13394264697887136639.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0069/__spark_conf__.zip
20/12/04 16:02:51 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:02:51 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:02:51 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:02:51 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:02:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:02:51 INFO Client: Submitting application application_1607015337794_0069 to ResourceManager
20/12/04 16:02:51 INFO YarnClientImpl: Submitted application application_1607015337794_0069
20/12/04 16:02:52 INFO Client: Application report for application_1607015337794_0069 (state: ACCEPTED)
20/12/04 16:02:52 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607115771399
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0069/
	 user: bsuconn
20/12/04 16:02:53 INFO Client: Application report for application_1607015337794_0069 (state: ACCEPTED)
20/12/04 16:02:54 INFO Client: Application report for application_1607015337794_0069 (state: ACCEPTED)
20/12/04 16:02:55 INFO Client: Application report for application_1607015337794_0069 (state: ACCEPTED)
20/12/04 16:02:56 INFO Client: Application report for application_1607015337794_0069 (state: ACCEPTED)
20/12/04 16:02:56 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0069), /proxy/application_1607015337794_0069
20/12/04 16:02:57 INFO Client: Application report for application_1607015337794_0069 (state: RUNNING)
20/12/04 16:02:57 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607115771399
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0069/
	 user: bsuconn
20/12/04 16:02:57 INFO YarnClientSchedulerBackend: Application application_1607015337794_0069 has started running.
20/12/04 16:02:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39015.
20/12/04 16:02:57 INFO NettyBlockTransferService: Server created on 192.168.1.9:39015
20/12/04 16:02:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 16:02:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 39015, None)
20/12/04 16:02:57 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:39015 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 39015, None)
20/12/04 16:02:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 39015, None)
20/12/04 16:02:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 39015, None)
20/12/04 16:02:57 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:02:57 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 16:03:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 16:03:02 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:48864) with ID 1
20/12/04 16:03:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:34059 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 34059, None)
20/12/04 16:03:03 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:48870) with ID 2
20/12/04 16:03:03 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 16:03:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:38007 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 38007, None)
20/12/04 16:03:04 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 16:03:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 16:03:04 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 16:03:04 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:03:04 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:03:04 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:03:04 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:03:04 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:03:04 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:39023/files/darima.zip with timestamp 1607115784951
20/12/04 16:03:04 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/userFiles-005f80d6-7d40-496a-abd3-1c763fba06bd/darima.zip
20/12/04 16:03:07 INFO InMemoryFileIndex: It took 76 ms to list leaf files for 1 paths.
20/12/04 16:03:08 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
20/12/04 16:03:09 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:03:09 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:03:09 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:03:09 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:03:10 INFO CodeGenerator: Code generated in 277.764607 ms
20/12/04 16:03:10 INFO CodeGenerator: Code generated in 31.435034 ms
20/12/04 16:03:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:03:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:03:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:39015 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:03:10 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 16:03:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:03:10 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 16:03:10 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 16:03:10 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 16:03:10 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 16:03:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 16:03:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 16:03:10 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:03:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 16:03:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 16:03:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:39015 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:03:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 16:03:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:03:10 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 16:03:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:03:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:38007 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:03:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:38007 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:03:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3325 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 16:03:14 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 16:03:14 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.475 s
20/12/04 16:03:14 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:03:14 INFO DAGScheduler: running: Set()
20/12/04 16:03:14 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 16:03:14 INFO DAGScheduler: failed: Set()
20/12/04 16:03:14 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:03:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 16:03:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 16:03:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:39015 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:03:14 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 16:03:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:03:14 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 16:03:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:03:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:38007 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:03:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:48870
20/12/04 16:03:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 372 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 16:03:14 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 16:03:14 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.389 s
20/12/04 16:03:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:03:14 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 16:03:14 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.955139 s
20/12/04 16:03:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:38007 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:03:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:39015 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:03:16 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:39015 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:03:16 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:38007 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:03:17 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 16:03:18 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:03:18 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:03:18 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:03:18 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:03:18 INFO CodeGenerator: Code generated in 25.9702 ms
20/12/04 16:03:18 INFO CodeGenerator: Code generated in 15.334658 ms
20/12/04 16:03:18 INFO CodeGenerator: Code generated in 23.847654 ms
20/12/04 16:03:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:03:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:03:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:39015 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:03:18 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/userFiles-005f80d6-7d40-496a-abd3-1c763fba06bd/darima.zip/darima/dlsa.py:22
20/12/04 16:03:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:03:18 INFO SparkContext: Starting job: toPandas at /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/userFiles-005f80d6-7d40-496a-abd3-1c763fba06bd/darima.zip/darima/dlsa.py:22
20/12/04 16:03:18 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/userFiles-005f80d6-7d40-496a-abd3-1c763fba06bd/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 16:03:18 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/userFiles-005f80d6-7d40-496a-abd3-1c763fba06bd/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 16:03:18 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/userFiles-005f80d6-7d40-496a-abd3-1c763fba06bd/darima.zip/darima/dlsa.py:22)
20/12/04 16:03:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 16:03:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 16:03:18 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/userFiles-005f80d6-7d40-496a-abd3-1c763fba06bd/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:03:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 16:03:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 16:03:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:39015 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 16:03:18 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 16:03:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/userFiles-005f80d6-7d40-496a-abd3-1c763fba06bd/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 16:03:18 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 16:03:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:03:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:34059 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 16:03:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:34059 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:03:22 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 4074 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:03:22 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 16:03:22 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 48569
20/12/04 16:03:22 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/userFiles-005f80d6-7d40-496a-abd3-1c763fba06bd/darima.zip/darima/dlsa.py:22) finished in 4.099 s
20/12/04 16:03:22 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:03:22 INFO DAGScheduler: running: Set()
20/12/04 16:03:22 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 16:03:22 INFO DAGScheduler: failed: Set()
20/12/04 16:03:22 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/userFiles-005f80d6-7d40-496a-abd3-1c763fba06bd/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:03:22 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 16:03:22 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 16:03:22 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:39015 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 16:03:22 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 16:03:22 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/userFiles-005f80d6-7d40-496a-abd3-1c763fba06bd/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 16:03:22 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 16:03:22 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:03:22 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:03:22 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:38007 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 16:03:22 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:34059 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 16:03:23 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:48870
20/12/04 16:03:23 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:48864
20/12/04 16:03:29 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 16:03:29 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/04 16:03:29 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 6, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:03:29 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 1]
20/12/04 16:03:31 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:03:31 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 2]
20/12/04 16:03:31 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 8, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 16:03:31 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 3]
20/12/04 16:03:33 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 9, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:03:33 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 8) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 4]
20/12/04 16:03:33 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 5]
20/12/04 16:03:33 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:03:34 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 11, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 16:03:34 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 9) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 6]
20/12/04 16:03:34 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:03:34 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 10) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 7]
20/12/04 16:03:35 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:03:35 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 11) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 8]
20/12/04 16:03:35 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 14, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 16:03:35 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 12) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’

) [duplicate 9]
20/12/04 16:03:35 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/04 16:03:35 INFO YarnScheduler: Cancelling stage 3
20/12/04 16:03:35 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/04 16:03:35 INFO YarnScheduler: Stage 3 was cancelled
20/12/04 16:03:35 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/userFiles-005f80d6-7d40-496a-abd3-1c763fba06bd/darima.zip/darima/dlsa.py:22) failed in 12.999 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 178, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
  there is no package called ‘Rcpp’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/04 16:03:35 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/userFiles-005f80d6-7d40-496a-abd3-1c763fba06bd/darima.zip/darima/dlsa.py:22, took 17.167643 s
20/12/04 16:03:35 WARN TaskSetManager: Lost task 3.3 in stage 3.0 (TID 14, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/04 16:03:36 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/04 16:03:36 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 16:03:36 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 16:03:36 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 16:03:36 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 16:03:36 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 16:03:36 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 16:03:36 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 16:03:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 16:03:36 INFO MemoryStore: MemoryStore cleared
20/12/04 16:03:36 INFO BlockManager: BlockManager stopped
20/12/04 16:03:36 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 16:03:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 16:03:36 INFO SparkContext: Successfully stopped SparkContext
20/12/04 16:03:36 INFO ShutdownHookManager: Shutdown hook called
20/12/04 16:03:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91
20/12/04 16:03:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-175ac46e-5a3d-4ea2-8019-c88080731d91/pyspark-e0a90f10-bd81-4ac4-bc5f-eed6f223e08c
20/12/04 16:03:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-b479dcd3-be87-4440-9b84-72672d59897d
20/12/04 16:03:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 16:03:39 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:03:39 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:03:39 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:03:39 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:03:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:03:40 INFO SparkContext: Running Spark version 3.0.1
20/12/04 16:03:40 INFO ResourceUtils: ==============================================================
20/12/04 16:03:40 INFO ResourceUtils: Resources for spark.driver:

20/12/04 16:03:40 INFO ResourceUtils: ==============================================================
20/12/04 16:03:40 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 16:03:40 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:03:40 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:03:40 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:03:40 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:03:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:03:40 INFO Utils: Successfully started service 'sparkDriver' on port 45079.
20/12/04 16:03:40 INFO SparkEnv: Registering MapOutputTracker
20/12/04 16:03:40 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 16:03:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 16:03:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 16:03:40 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 16:03:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4de80d4a-d8e7-4b56-ab4e-24c90cbcd5f0
20/12/04 16:03:40 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 16:03:40 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 16:03:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 16:03:41 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 16:03:41 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 16:03:42 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 16:03:42 INFO Configuration: resource-types.xml not found
20/12/04 16:03:42 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 16:03:42 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 16:03:42 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 16:03:42 INFO Client: Setting up container launch context for our AM
20/12/04 16:03:42 INFO Client: Setting up the launch environment for our AM container
20/12/04 16:03:42 INFO Client: Preparing resources for our AM container
20/12/04 16:03:42 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 16:03:45 INFO Client: Uploading resource file:/tmp/spark-0d3a4833-d3e2-4a91-bfc2-5b5dd8711f8e/__spark_libs__14025791203202306029.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0070/__spark_libs__14025791203202306029.zip
20/12/04 16:03:46 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0070/pyspark.zip
20/12/04 16:03:46 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0070/py4j-0.10.9-src.zip
20/12/04 16:03:46 INFO Client: Uploading resource file:/tmp/spark-0d3a4833-d3e2-4a91-bfc2-5b5dd8711f8e/__spark_conf__16235796202596890983.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0070/__spark_conf__.zip
20/12/04 16:03:46 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:03:46 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:03:46 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:03:46 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:03:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:03:46 INFO Client: Submitting application application_1607015337794_0070 to ResourceManager
20/12/04 16:03:46 INFO YarnClientImpl: Submitted application application_1607015337794_0070
20/12/04 16:03:47 INFO Client: Application report for application_1607015337794_0070 (state: ACCEPTED)
20/12/04 16:03:47 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607115826766
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0070/
	 user: bsuconn
20/12/04 16:03:48 INFO Client: Application report for application_1607015337794_0070 (state: ACCEPTED)
20/12/04 16:03:49 INFO Client: Application report for application_1607015337794_0070 (state: ACCEPTED)
20/12/04 16:03:50 INFO Client: Application report for application_1607015337794_0070 (state: ACCEPTED)
20/12/04 16:03:51 INFO Client: Application report for application_1607015337794_0070 (state: RUNNING)
20/12/04 16:03:51 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607115826766
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0070/
	 user: bsuconn
20/12/04 16:03:51 INFO YarnClientSchedulerBackend: Application application_1607015337794_0070 has started running.
20/12/04 16:03:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37617.
20/12/04 16:03:51 INFO NettyBlockTransferService: Server created on 192.168.1.9:37617
20/12/04 16:03:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 16:03:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 37617, None)
20/12/04 16:03:51 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:37617 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 37617, None)
20/12/04 16:03:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 37617, None)
20/12/04 16:03:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 37617, None)
20/12/04 16:03:52 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0070), /proxy/application_1607015337794_0070
20/12/04 16:03:53 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 16:03:54 INFO DiskBlockManager: Shutdown hook called
20/12/04 16:03:54 INFO ShutdownHookManager: Shutdown hook called
20/12/04 16:03:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-0d3a4833-d3e2-4a91-bfc2-5b5dd8711f8e/userFiles-0488afad-eb94-4f05-8484-9b9e7a36ead8
20/12/04 16:03:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-0d3a4833-d3e2-4a91-bfc2-5b5dd8711f8e
20/12/04 16:03:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-873ca267-2ab7-4b24-91e1-843d75f74690
20/12/04 16:05:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 16:05:08 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:05:08 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:05:08 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:05:08 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:05:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:05:09 INFO SparkContext: Running Spark version 3.0.1
20/12/04 16:05:09 INFO ResourceUtils: ==============================================================
20/12/04 16:05:09 INFO ResourceUtils: Resources for spark.driver:

20/12/04 16:05:09 INFO ResourceUtils: ==============================================================
20/12/04 16:05:09 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 16:05:09 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:05:09 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:05:09 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:05:09 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:05:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:05:09 INFO Utils: Successfully started service 'sparkDriver' on port 46405.
20/12/04 16:05:09 INFO SparkEnv: Registering MapOutputTracker
20/12/04 16:05:09 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 16:05:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 16:05:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 16:05:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 16:05:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-91cc3694-2e35-44ab-b61d-15b5f8357559
20/12/04 16:05:10 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 16:05:10 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 16:05:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 16:05:10 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 16:05:10 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 16:05:11 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 16:05:11 INFO Configuration: resource-types.xml not found
20/12/04 16:05:11 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 16:05:11 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 16:05:11 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 16:05:11 INFO Client: Setting up container launch context for our AM
20/12/04 16:05:11 INFO Client: Setting up the launch environment for our AM container
20/12/04 16:05:11 INFO Client: Preparing resources for our AM container
20/12/04 16:05:11 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 16:05:13 INFO Client: Uploading resource file:/tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/__spark_libs__6231294139706209989.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0071/__spark_libs__6231294139706209989.zip
20/12/04 16:05:15 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0071/pyspark.zip
20/12/04 16:05:15 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0071/py4j-0.10.9-src.zip
20/12/04 16:05:15 INFO Client: Uploading resource file:/tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/__spark_conf__5729475830398292314.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0071/__spark_conf__.zip
20/12/04 16:05:15 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:05:15 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:05:15 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:05:15 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:05:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:05:15 INFO Client: Submitting application application_1607015337794_0071 to ResourceManager
20/12/04 16:05:15 INFO YarnClientImpl: Submitted application application_1607015337794_0071
20/12/04 16:05:16 INFO Client: Application report for application_1607015337794_0071 (state: ACCEPTED)
20/12/04 16:05:16 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607115915489
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0071/
	 user: bsuconn
20/12/04 16:05:17 INFO Client: Application report for application_1607015337794_0071 (state: ACCEPTED)
20/12/04 16:05:18 INFO Client: Application report for application_1607015337794_0071 (state: ACCEPTED)
20/12/04 16:05:19 INFO Client: Application report for application_1607015337794_0071 (state: ACCEPTED)
20/12/04 16:05:20 INFO Client: Application report for application_1607015337794_0071 (state: ACCEPTED)
20/12/04 16:05:20 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0071), /proxy/application_1607015337794_0071
20/12/04 16:05:21 INFO Client: Application report for application_1607015337794_0071 (state: RUNNING)
20/12/04 16:05:21 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607115915489
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0071/
	 user: bsuconn
20/12/04 16:05:21 INFO YarnClientSchedulerBackend: Application application_1607015337794_0071 has started running.
20/12/04 16:05:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40051.
20/12/04 16:05:21 INFO NettyBlockTransferService: Server created on 192.168.1.9:40051
20/12/04 16:05:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 16:05:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 40051, None)
20/12/04 16:05:21 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40051 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 40051, None)
20/12/04 16:05:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 40051, None)
20/12/04 16:05:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 40051, None)
20/12/04 16:05:21 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:05:21 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 16:05:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 16:05:25 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:55512) with ID 1
20/12/04 16:05:25 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 16:05:25 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40889 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 40889, None)
20/12/04 16:05:25 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 16:05:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 16:05:25 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 16:05:26 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:05:26 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:05:26 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:05:26 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:05:26 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:05:26 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:46405/files/darima.zip with timestamp 1607115926636
20/12/04 16:05:26 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/userFiles-f2dc4b20-eee5-44c5-9b1e-7b4235fec5a4/darima.zip
20/12/04 16:05:28 INFO InMemoryFileIndex: It took 78 ms to list leaf files for 1 paths.
20/12/04 16:05:30 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/12/04 16:05:30 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:05:30 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:05:30 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:05:30 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:05:31 INFO CodeGenerator: Code generated in 311.235239 ms
20/12/04 16:05:31 INFO CodeGenerator: Code generated in 31.24774 ms
20/12/04 16:05:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:05:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:05:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:40051 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:05:31 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 16:05:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:05:32 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 16:05:32 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 16:05:32 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 16:05:32 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 16:05:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 16:05:32 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 16:05:32 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:05:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 16:05:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 16:05:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:40051 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:05:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 16:05:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:05:32 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 16:05:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:05:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:40889 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:05:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:40889 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:05:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3300 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:05:35 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 16:05:35 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.436 s
20/12/04 16:05:35 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:05:35 INFO DAGScheduler: running: Set()
20/12/04 16:05:35 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 16:05:35 INFO DAGScheduler: failed: Set()
20/12/04 16:05:35 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:05:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 16:05:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 16:05:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:40051 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:05:35 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 16:05:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:05:35 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 16:05:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:05:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:40889 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:05:35 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:55512
20/12/04 16:05:36 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 383 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:05:36 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 16:05:36 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.401 s
20/12/04 16:05:36 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:05:36 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 16:05:36 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.927521 s
20/12/04 16:05:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:40889 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:05:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:40051 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:05:38 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:40889 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:05:38 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:40051 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:05:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:40889 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:05:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:40051 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:05:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 16:05:39 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:05:39 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:05:39 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:05:39 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:05:39 INFO CodeGenerator: Code generated in 23.870433 ms
20/12/04 16:05:39 INFO CodeGenerator: Code generated in 16.729432 ms
20/12/04 16:05:39 INFO CodeGenerator: Code generated in 19.769114 ms
20/12/04 16:05:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:05:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:05:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:40051 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:05:39 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/userFiles-f2dc4b20-eee5-44c5-9b1e-7b4235fec5a4/darima.zip/darima/dlsa.py:22
20/12/04 16:05:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:05:39 INFO SparkContext: Starting job: toPandas at /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/userFiles-f2dc4b20-eee5-44c5-9b1e-7b4235fec5a4/darima.zip/darima/dlsa.py:22
20/12/04 16:05:39 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/userFiles-f2dc4b20-eee5-44c5-9b1e-7b4235fec5a4/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 16:05:39 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/userFiles-f2dc4b20-eee5-44c5-9b1e-7b4235fec5a4/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 16:05:39 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/userFiles-f2dc4b20-eee5-44c5-9b1e-7b4235fec5a4/darima.zip/darima/dlsa.py:22)
20/12/04 16:05:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 16:05:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 16:05:39 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/userFiles-f2dc4b20-eee5-44c5-9b1e-7b4235fec5a4/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:05:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 16:05:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 16:05:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:40051 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 16:05:39 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 16:05:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/userFiles-f2dc4b20-eee5-44c5-9b1e-7b4235fec5a4/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 16:05:39 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 16:05:39 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:05:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:40889 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 16:05:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:40889 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:05:40 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1292 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:05:40 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 16:05:40 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 35115
20/12/04 16:05:40 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/userFiles-f2dc4b20-eee5-44c5-9b1e-7b4235fec5a4/darima.zip/darima/dlsa.py:22) finished in 1.318 s
20/12/04 16:05:40 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:05:40 INFO DAGScheduler: running: Set()
20/12/04 16:05:40 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 16:05:40 INFO DAGScheduler: failed: Set()
20/12/04 16:05:40 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/userFiles-f2dc4b20-eee5-44c5-9b1e-7b4235fec5a4/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:05:40 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 16:05:40 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 16:05:40 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:40051 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 16:05:40 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 16:05:40 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/userFiles-f2dc4b20-eee5-44c5-9b1e-7b4235fec5a4/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 16:05:40 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 16:05:40 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:05:40 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:40889 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 16:05:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:55512
20/12/04 16:05:45 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:05:45 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 4932 ms on 192.168.1.9 (executor 1) (1/200)
20/12/04 16:05:47 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 16:05:47 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 4) in 1174 ms on 192.168.1.9 (executor 1) (2/200)
20/12/04 16:05:49 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 4, NODE_LOCAL, 7336 bytes)
20/12/04 16:05:49 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 5) in 2321 ms on 192.168.1.9 (executor 1) (3/200)
20/12/04 16:05:50 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 5, NODE_LOCAL, 7336 bytes)
20/12/04 16:05:50 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 6) in 1244 ms on 192.168.1.9 (executor 1) (4/200)
20/12/04 16:05:51 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 6, NODE_LOCAL, 7336 bytes)
20/12/04 16:05:51 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 7) in 1036 ms on 192.168.1.9 (executor 1) (5/200)
20/12/04 16:05:53 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 7, NODE_LOCAL, 7336 bytes)
20/12/04 16:05:53 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 8) in 1416 ms on 192.168.1.9 (executor 1) (6/200)
20/12/04 16:05:54 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 10, NODE_LOCAL, 7336 bytes)
20/12/04 16:05:54 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 9) in 1721 ms on 192.168.1.9 (executor 1) (7/200)
20/12/04 16:05:56 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 11, 192.168.1.9, executor 1, partition 11, NODE_LOCAL, 7336 bytes)
20/12/04 16:05:56 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 10) in 1585 ms on 192.168.1.9 (executor 1) (8/200)
20/12/04 16:05:57 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 12, 192.168.1.9, executor 1, partition 12, NODE_LOCAL, 7336 bytes)
20/12/04 16:05:57 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 11) in 1243 ms on 192.168.1.9 (executor 1) (9/200)
20/12/04 16:06:00 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 13, 192.168.1.9, executor 1, partition 13, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:00 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 12) in 3358 ms on 192.168.1.9 (executor 1) (10/200)
20/12/04 16:06:01 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 14, 192.168.1.9, executor 1, partition 14, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:01 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 13) in 737 ms on 192.168.1.9 (executor 1) (11/200)
20/12/04 16:06:02 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 15, 192.168.1.9, executor 1, partition 18, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:02 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 14) in 1134 ms on 192.168.1.9 (executor 1) (12/200)
20/12/04 16:06:03 INFO TaskSetManager: Starting task 19.0 in stage 3.0 (TID 16, 192.168.1.9, executor 1, partition 19, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:03 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 15) in 1069 ms on 192.168.1.9 (executor 1) (13/200)
20/12/04 16:06:06 INFO TaskSetManager: Starting task 21.0 in stage 3.0 (TID 17, 192.168.1.9, executor 1, partition 21, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:06 INFO TaskSetManager: Finished task 19.0 in stage 3.0 (TID 16) in 2689 ms on 192.168.1.9 (executor 1) (14/200)
20/12/04 16:06:08 INFO TaskSetManager: Starting task 23.0 in stage 3.0 (TID 18, 192.168.1.9, executor 1, partition 23, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:08 INFO TaskSetManager: Finished task 21.0 in stage 3.0 (TID 17) in 1823 ms on 192.168.1.9 (executor 1) (15/200)
20/12/04 16:06:09 INFO TaskSetManager: Starting task 24.0 in stage 3.0 (TID 19, 192.168.1.9, executor 1, partition 24, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:09 INFO TaskSetManager: Finished task 23.0 in stage 3.0 (TID 18) in 1208 ms on 192.168.1.9 (executor 1) (16/200)
20/12/04 16:06:10 INFO TaskSetManager: Starting task 27.0 in stage 3.0 (TID 20, 192.168.1.9, executor 1, partition 27, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:10 INFO TaskSetManager: Finished task 24.0 in stage 3.0 (TID 19) in 1111 ms on 192.168.1.9 (executor 1) (17/200)
20/12/04 16:06:11 INFO TaskSetManager: Starting task 30.0 in stage 3.0 (TID 21, 192.168.1.9, executor 1, partition 30, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:11 INFO TaskSetManager: Finished task 27.0 in stage 3.0 (TID 20) in 612 ms on 192.168.1.9 (executor 1) (18/200)
20/12/04 16:06:12 INFO TaskSetManager: Starting task 31.0 in stage 3.0 (TID 22, 192.168.1.9, executor 1, partition 31, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:12 INFO TaskSetManager: Finished task 30.0 in stage 3.0 (TID 21) in 1464 ms on 192.168.1.9 (executor 1) (19/200)
20/12/04 16:06:14 INFO TaskSetManager: Starting task 32.0 in stage 3.0 (TID 23, 192.168.1.9, executor 1, partition 32, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:14 INFO TaskSetManager: Finished task 31.0 in stage 3.0 (TID 22) in 1426 ms on 192.168.1.9 (executor 1) (20/200)
20/12/04 16:06:15 INFO TaskSetManager: Starting task 34.0 in stage 3.0 (TID 24, 192.168.1.9, executor 1, partition 34, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:15 INFO TaskSetManager: Finished task 32.0 in stage 3.0 (TID 23) in 1522 ms on 192.168.1.9 (executor 1) (21/200)
20/12/04 16:06:16 INFO TaskSetManager: Starting task 35.0 in stage 3.0 (TID 25, 192.168.1.9, executor 1, partition 35, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:16 INFO TaskSetManager: Finished task 34.0 in stage 3.0 (TID 24) in 799 ms on 192.168.1.9 (executor 1) (22/200)
20/12/04 16:06:18 INFO TaskSetManager: Starting task 36.0 in stage 3.0 (TID 26, 192.168.1.9, executor 1, partition 36, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:18 INFO TaskSetManager: Finished task 35.0 in stage 3.0 (TID 25) in 1482 ms on 192.168.1.9 (executor 1) (23/200)
20/12/04 16:06:18 INFO TaskSetManager: Starting task 37.0 in stage 3.0 (TID 27, 192.168.1.9, executor 1, partition 37, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:18 INFO TaskSetManager: Finished task 36.0 in stage 3.0 (TID 26) in 967 ms on 192.168.1.9 (executor 1) (24/200)
20/12/04 16:06:19 INFO TaskSetManager: Starting task 41.0 in stage 3.0 (TID 28, 192.168.1.9, executor 1, partition 41, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:19 INFO TaskSetManager: Finished task 37.0 in stage 3.0 (TID 27) in 891 ms on 192.168.1.9 (executor 1) (25/200)
20/12/04 16:06:20 INFO TaskSetManager: Starting task 43.0 in stage 3.0 (TID 29, 192.168.1.9, executor 1, partition 43, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:20 INFO TaskSetManager: Finished task 41.0 in stage 3.0 (TID 28) in 793 ms on 192.168.1.9 (executor 1) (26/200)
20/12/04 16:06:22 INFO TaskSetManager: Starting task 44.0 in stage 3.0 (TID 30, 192.168.1.9, executor 1, partition 44, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:22 INFO TaskSetManager: Finished task 43.0 in stage 3.0 (TID 29) in 1385 ms on 192.168.1.9 (executor 1) (27/200)
20/12/04 16:06:23 INFO TaskSetManager: Starting task 48.0 in stage 3.0 (TID 31, 192.168.1.9, executor 1, partition 48, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:23 INFO TaskSetManager: Finished task 44.0 in stage 3.0 (TID 30) in 965 ms on 192.168.1.9 (executor 1) (28/200)
20/12/04 16:06:24 INFO TaskSetManager: Starting task 49.0 in stage 3.0 (TID 32, 192.168.1.9, executor 1, partition 49, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:24 INFO TaskSetManager: Finished task 48.0 in stage 3.0 (TID 31) in 1034 ms on 192.168.1.9 (executor 1) (29/200)
20/12/04 16:06:26 INFO TaskSetManager: Starting task 51.0 in stage 3.0 (TID 33, 192.168.1.9, executor 1, partition 51, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:26 INFO TaskSetManager: Finished task 49.0 in stage 3.0 (TID 32) in 2224 ms on 192.168.1.9 (executor 1) (30/200)
20/12/04 16:06:28 INFO TaskSetManager: Starting task 53.0 in stage 3.0 (TID 34, 192.168.1.9, executor 1, partition 53, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:28 INFO TaskSetManager: Finished task 51.0 in stage 3.0 (TID 33) in 2423 ms on 192.168.1.9 (executor 1) (31/200)
20/12/04 16:06:31 INFO TaskSetManager: Starting task 55.0 in stage 3.0 (TID 35, 192.168.1.9, executor 1, partition 55, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:31 INFO TaskSetManager: Finished task 53.0 in stage 3.0 (TID 34) in 2310 ms on 192.168.1.9 (executor 1) (32/200)
20/12/04 16:06:31 INFO TaskSetManager: Starting task 57.0 in stage 3.0 (TID 36, 192.168.1.9, executor 1, partition 57, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:31 INFO TaskSetManager: Finished task 55.0 in stage 3.0 (TID 35) in 869 ms on 192.168.1.9 (executor 1) (33/200)
20/12/04 16:06:32 INFO TaskSetManager: Starting task 58.0 in stage 3.0 (TID 37, 192.168.1.9, executor 1, partition 58, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:32 INFO TaskSetManager: Finished task 57.0 in stage 3.0 (TID 36) in 1077 ms on 192.168.1.9 (executor 1) (34/200)
20/12/04 16:06:33 INFO TaskSetManager: Starting task 61.0 in stage 3.0 (TID 38, 192.168.1.9, executor 1, partition 61, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:33 INFO TaskSetManager: Finished task 58.0 in stage 3.0 (TID 37) in 951 ms on 192.168.1.9 (executor 1) (35/200)
20/12/04 16:06:36 INFO TaskSetManager: Starting task 63.0 in stage 3.0 (TID 39, 192.168.1.9, executor 1, partition 63, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:36 INFO TaskSetManager: Finished task 61.0 in stage 3.0 (TID 38) in 2171 ms on 192.168.1.9 (executor 1) (36/200)
20/12/04 16:06:36 INFO TaskSetManager: Starting task 65.0 in stage 3.0 (TID 40, 192.168.1.9, executor 1, partition 65, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:36 INFO TaskSetManager: Finished task 63.0 in stage 3.0 (TID 39) in 866 ms on 192.168.1.9 (executor 1) (37/200)
20/12/04 16:06:37 INFO TaskSetManager: Starting task 66.0 in stage 3.0 (TID 41, 192.168.1.9, executor 1, partition 66, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:37 INFO TaskSetManager: Finished task 65.0 in stage 3.0 (TID 40) in 1011 ms on 192.168.1.9 (executor 1) (38/200)
20/12/04 16:06:38 INFO TaskSetManager: Starting task 69.0 in stage 3.0 (TID 42, 192.168.1.9, executor 1, partition 69, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:38 INFO TaskSetManager: Finished task 66.0 in stage 3.0 (TID 41) in 749 ms on 192.168.1.9 (executor 1) (39/200)
20/12/04 16:06:39 INFO TaskSetManager: Starting task 70.0 in stage 3.0 (TID 43, 192.168.1.9, executor 1, partition 70, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:39 INFO TaskSetManager: Finished task 69.0 in stage 3.0 (TID 42) in 1111 ms on 192.168.1.9 (executor 1) (40/200)
20/12/04 16:06:41 INFO TaskSetManager: Starting task 73.0 in stage 3.0 (TID 44, 192.168.1.9, executor 1, partition 73, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:41 INFO TaskSetManager: Finished task 70.0 in stage 3.0 (TID 43) in 1292 ms on 192.168.1.9 (executor 1) (41/200)
20/12/04 16:06:42 INFO TaskSetManager: Starting task 75.0 in stage 3.0 (TID 45, 192.168.1.9, executor 1, partition 75, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:42 INFO TaskSetManager: Finished task 73.0 in stage 3.0 (TID 44) in 1311 ms on 192.168.1.9 (executor 1) (42/200)
20/12/04 16:06:44 INFO TaskSetManager: Starting task 77.0 in stage 3.0 (TID 46, 192.168.1.9, executor 1, partition 77, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:44 INFO TaskSetManager: Finished task 75.0 in stage 3.0 (TID 45) in 1672 ms on 192.168.1.9 (executor 1) (43/200)
20/12/04 16:06:46 INFO TaskSetManager: Starting task 79.0 in stage 3.0 (TID 47, 192.168.1.9, executor 1, partition 79, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:46 INFO TaskSetManager: Finished task 77.0 in stage 3.0 (TID 46) in 2709 ms on 192.168.1.9 (executor 1) (44/200)
20/12/04 16:06:48 INFO TaskSetManager: Starting task 84.0 in stage 3.0 (TID 48, 192.168.1.9, executor 1, partition 84, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:48 INFO TaskSetManager: Finished task 79.0 in stage 3.0 (TID 47) in 1924 ms on 192.168.1.9 (executor 1) (45/200)
20/12/04 16:06:49 INFO TaskSetManager: Starting task 85.0 in stage 3.0 (TID 49, 192.168.1.9, executor 1, partition 85, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:49 INFO TaskSetManager: Finished task 84.0 in stage 3.0 (TID 48) in 518 ms on 192.168.1.9 (executor 1) (46/200)
20/12/04 16:06:51 INFO TaskSetManager: Finished task 85.0 in stage 3.0 (TID 49) in 2476 ms on 192.168.1.9 (executor 1) (47/200)
20/12/04 16:06:51 INFO TaskSetManager: Starting task 86.0 in stage 3.0 (TID 50, 192.168.1.9, executor 1, partition 86, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:53 INFO TaskSetManager: Starting task 88.0 in stage 3.0 (TID 51, 192.168.1.9, executor 1, partition 88, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:53 INFO TaskSetManager: Finished task 86.0 in stage 3.0 (TID 50) in 1361 ms on 192.168.1.9 (executor 1) (48/200)
20/12/04 16:06:53 INFO TaskSetManager: Starting task 89.0 in stage 3.0 (TID 52, 192.168.1.9, executor 1, partition 89, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:53 INFO TaskSetManager: Finished task 88.0 in stage 3.0 (TID 51) in 670 ms on 192.168.1.9 (executor 1) (49/200)
20/12/04 16:06:55 INFO TaskSetManager: Starting task 91.0 in stage 3.0 (TID 53, 192.168.1.9, executor 1, partition 91, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:55 INFO TaskSetManager: Finished task 89.0 in stage 3.0 (TID 52) in 1827 ms on 192.168.1.9 (executor 1) (50/200)
20/12/04 16:06:56 INFO TaskSetManager: Starting task 95.0 in stage 3.0 (TID 54, 192.168.1.9, executor 1, partition 95, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:56 INFO TaskSetManager: Finished task 91.0 in stage 3.0 (TID 53) in 1175 ms on 192.168.1.9 (executor 1) (51/200)
20/12/04 16:06:58 INFO TaskSetManager: Starting task 101.0 in stage 3.0 (TID 55, 192.168.1.9, executor 1, partition 101, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:58 INFO TaskSetManager: Finished task 95.0 in stage 3.0 (TID 54) in 1729 ms on 192.168.1.9 (executor 1) (52/200)
20/12/04 16:06:59 INFO TaskSetManager: Starting task 102.0 in stage 3.0 (TID 56, 192.168.1.9, executor 1, partition 102, NODE_LOCAL, 7336 bytes)
20/12/04 16:06:59 INFO TaskSetManager: Finished task 101.0 in stage 3.0 (TID 55) in 848 ms on 192.168.1.9 (executor 1) (53/200)
20/12/04 16:07:02 INFO TaskSetManager: Starting task 103.0 in stage 3.0 (TID 57, 192.168.1.9, executor 1, partition 103, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:02 INFO TaskSetManager: Finished task 102.0 in stage 3.0 (TID 56) in 3470 ms on 192.168.1.9 (executor 1) (54/200)
20/12/04 16:07:04 INFO TaskSetManager: Starting task 105.0 in stage 3.0 (TID 58, 192.168.1.9, executor 1, partition 105, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:04 INFO TaskSetManager: Finished task 103.0 in stage 3.0 (TID 57) in 1903 ms on 192.168.1.9 (executor 1) (55/200)
20/12/04 16:07:06 INFO TaskSetManager: Starting task 106.0 in stage 3.0 (TID 59, 192.168.1.9, executor 1, partition 106, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:06 INFO TaskSetManager: Finished task 105.0 in stage 3.0 (TID 58) in 2162 ms on 192.168.1.9 (executor 1) (56/200)
20/12/04 16:07:07 INFO TaskSetManager: Starting task 107.0 in stage 3.0 (TID 60, 192.168.1.9, executor 1, partition 107, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:07 INFO TaskSetManager: Finished task 106.0 in stage 3.0 (TID 59) in 537 ms on 192.168.1.9 (executor 1) (57/200)
20/12/04 16:07:08 INFO TaskSetManager: Starting task 108.0 in stage 3.0 (TID 61, 192.168.1.9, executor 1, partition 108, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:08 INFO TaskSetManager: Finished task 107.0 in stage 3.0 (TID 60) in 724 ms on 192.168.1.9 (executor 1) (58/200)
20/12/04 16:07:08 INFO TaskSetManager: Starting task 109.0 in stage 3.0 (TID 62, 192.168.1.9, executor 1, partition 109, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:08 INFO TaskSetManager: Finished task 108.0 in stage 3.0 (TID 61) in 813 ms on 192.168.1.9 (executor 1) (59/200)
20/12/04 16:07:11 INFO TaskSetManager: Starting task 111.0 in stage 3.0 (TID 63, 192.168.1.9, executor 1, partition 111, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:11 INFO TaskSetManager: Finished task 109.0 in stage 3.0 (TID 62) in 2230 ms on 192.168.1.9 (executor 1) (60/200)
20/12/04 16:07:12 INFO TaskSetManager: Starting task 113.0 in stage 3.0 (TID 64, 192.168.1.9, executor 1, partition 113, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:12 INFO TaskSetManager: Finished task 111.0 in stage 3.0 (TID 63) in 1576 ms on 192.168.1.9 (executor 1) (61/200)
20/12/04 16:07:13 INFO TaskSetManager: Starting task 115.0 in stage 3.0 (TID 65, 192.168.1.9, executor 1, partition 115, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:13 INFO TaskSetManager: Finished task 113.0 in stage 3.0 (TID 64) in 896 ms on 192.168.1.9 (executor 1) (62/200)
20/12/04 16:07:15 INFO TaskSetManager: Starting task 116.0 in stage 3.0 (TID 66, 192.168.1.9, executor 1, partition 116, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:15 INFO TaskSetManager: Finished task 115.0 in stage 3.0 (TID 65) in 1477 ms on 192.168.1.9 (executor 1) (63/200)
20/12/04 16:07:15 INFO TaskSetManager: Starting task 119.0 in stage 3.0 (TID 67, 192.168.1.9, executor 1, partition 119, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:15 INFO TaskSetManager: Finished task 116.0 in stage 3.0 (TID 66) in 531 ms on 192.168.1.9 (executor 1) (64/200)
20/12/04 16:07:16 INFO TaskSetManager: Starting task 122.0 in stage 3.0 (TID 68, 192.168.1.9, executor 1, partition 122, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:16 INFO TaskSetManager: Finished task 119.0 in stage 3.0 (TID 67) in 1242 ms on 192.168.1.9 (executor 1) (65/200)
20/12/04 16:07:19 INFO TaskSetManager: Starting task 124.0 in stage 3.0 (TID 69, 192.168.1.9, executor 1, partition 124, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:19 INFO TaskSetManager: Finished task 122.0 in stage 3.0 (TID 68) in 2314 ms on 192.168.1.9 (executor 1) (66/200)
20/12/04 16:07:19 INFO TaskSetManager: Starting task 126.0 in stage 3.0 (TID 70, 192.168.1.9, executor 1, partition 126, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:19 INFO TaskSetManager: Finished task 124.0 in stage 3.0 (TID 69) in 809 ms on 192.168.1.9 (executor 1) (67/200)
20/12/04 16:07:20 INFO TaskSetManager: Starting task 128.0 in stage 3.0 (TID 71, 192.168.1.9, executor 1, partition 128, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:20 INFO TaskSetManager: Finished task 126.0 in stage 3.0 (TID 70) in 847 ms on 192.168.1.9 (executor 1) (68/200)
20/12/04 16:07:21 INFO TaskSetManager: Starting task 129.0 in stage 3.0 (TID 72, 192.168.1.9, executor 1, partition 129, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:21 INFO TaskSetManager: Finished task 128.0 in stage 3.0 (TID 71) in 692 ms on 192.168.1.9 (executor 1) (69/200)
20/12/04 16:07:22 INFO TaskSetManager: Starting task 131.0 in stage 3.0 (TID 73, 192.168.1.9, executor 1, partition 131, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:22 INFO TaskSetManager: Finished task 129.0 in stage 3.0 (TID 72) in 1079 ms on 192.168.1.9 (executor 1) (70/200)
20/12/04 16:07:23 INFO TaskSetManager: Starting task 132.0 in stage 3.0 (TID 74, 192.168.1.9, executor 1, partition 132, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:23 INFO TaskSetManager: Finished task 131.0 in stage 3.0 (TID 73) in 696 ms on 192.168.1.9 (executor 1) (71/200)
20/12/04 16:07:24 INFO TaskSetManager: Starting task 133.0 in stage 3.0 (TID 75, 192.168.1.9, executor 1, partition 133, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:24 INFO TaskSetManager: Finished task 132.0 in stage 3.0 (TID 74) in 1323 ms on 192.168.1.9 (executor 1) (72/200)
20/12/04 16:07:25 INFO TaskSetManager: Starting task 135.0 in stage 3.0 (TID 76, 192.168.1.9, executor 1, partition 135, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:25 INFO TaskSetManager: Finished task 133.0 in stage 3.0 (TID 75) in 897 ms on 192.168.1.9 (executor 1) (73/200)
20/12/04 16:07:27 INFO TaskSetManager: Starting task 136.0 in stage 3.0 (TID 77, 192.168.1.9, executor 1, partition 136, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:27 INFO TaskSetManager: Finished task 135.0 in stage 3.0 (TID 76) in 2453 ms on 192.168.1.9 (executor 1) (74/200)
20/12/04 16:07:28 INFO TaskSetManager: Starting task 137.0 in stage 3.0 (TID 78, 192.168.1.9, executor 1, partition 137, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:28 INFO TaskSetManager: Finished task 136.0 in stage 3.0 (TID 77) in 729 ms on 192.168.1.9 (executor 1) (75/200)
20/12/04 16:07:31 INFO TaskSetManager: Starting task 139.0 in stage 3.0 (TID 79, 192.168.1.9, executor 1, partition 139, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:31 INFO TaskSetManager: Finished task 137.0 in stage 3.0 (TID 78) in 2996 ms on 192.168.1.9 (executor 1) (76/200)
20/12/04 16:07:32 INFO TaskSetManager: Starting task 140.0 in stage 3.0 (TID 80, 192.168.1.9, executor 1, partition 140, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:32 INFO TaskSetManager: Finished task 139.0 in stage 3.0 (TID 79) in 723 ms on 192.168.1.9 (executor 1) (77/200)
20/12/04 16:07:32 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:40051 in memory (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 16:07:32 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:40889 in memory (size: 11.6 KiB, free: 912.1 MiB)
20/12/04 16:07:33 INFO TaskSetManager: Starting task 141.0 in stage 3.0 (TID 81, 192.168.1.9, executor 1, partition 141, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:33 INFO TaskSetManager: Finished task 140.0 in stage 3.0 (TID 80) in 1492 ms on 192.168.1.9 (executor 1) (78/200)
20/12/04 16:07:34 INFO TaskSetManager: Starting task 143.0 in stage 3.0 (TID 82, 192.168.1.9, executor 1, partition 143, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:34 INFO TaskSetManager: Finished task 141.0 in stage 3.0 (TID 81) in 573 ms on 192.168.1.9 (executor 1) (79/200)
20/12/04 16:07:35 INFO TaskSetManager: Starting task 146.0 in stage 3.0 (TID 83, 192.168.1.9, executor 1, partition 146, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:35 INFO TaskSetManager: Finished task 143.0 in stage 3.0 (TID 82) in 917 ms on 192.168.1.9 (executor 1) (80/200)
20/12/04 16:07:36 INFO TaskSetManager: Starting task 150.0 in stage 3.0 (TID 84, 192.168.1.9, executor 1, partition 150, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:36 INFO TaskSetManager: Finished task 146.0 in stage 3.0 (TID 83) in 1280 ms on 192.168.1.9 (executor 1) (81/200)
20/12/04 16:07:39 INFO TaskSetManager: Starting task 151.0 in stage 3.0 (TID 85, 192.168.1.9, executor 1, partition 151, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:39 INFO TaskSetManager: Finished task 150.0 in stage 3.0 (TID 84) in 3169 ms on 192.168.1.9 (executor 1) (82/200)
20/12/04 16:07:41 INFO TaskSetManager: Starting task 153.0 in stage 3.0 (TID 86, 192.168.1.9, executor 1, partition 153, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:41 INFO TaskSetManager: Finished task 151.0 in stage 3.0 (TID 85) in 1501 ms on 192.168.1.9 (executor 1) (83/200)
20/12/04 16:07:42 INFO TaskSetManager: Starting task 155.0 in stage 3.0 (TID 87, 192.168.1.9, executor 1, partition 155, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:42 INFO TaskSetManager: Finished task 153.0 in stage 3.0 (TID 86) in 1481 ms on 192.168.1.9 (executor 1) (84/200)
20/12/04 16:07:43 INFO TaskSetManager: Starting task 156.0 in stage 3.0 (TID 88, 192.168.1.9, executor 1, partition 156, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:43 INFO TaskSetManager: Finished task 155.0 in stage 3.0 (TID 87) in 858 ms on 192.168.1.9 (executor 1) (85/200)
20/12/04 16:07:45 INFO TaskSetManager: Starting task 161.0 in stage 3.0 (TID 89, 192.168.1.9, executor 1, partition 161, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:45 INFO TaskSetManager: Finished task 156.0 in stage 3.0 (TID 88) in 1775 ms on 192.168.1.9 (executor 1) (86/200)
20/12/04 16:07:46 INFO TaskSetManager: Starting task 162.0 in stage 3.0 (TID 90, 192.168.1.9, executor 1, partition 162, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:46 INFO TaskSetManager: Finished task 161.0 in stage 3.0 (TID 89) in 1432 ms on 192.168.1.9 (executor 1) (87/200)
20/12/04 16:07:48 INFO TaskSetManager: Starting task 163.0 in stage 3.0 (TID 91, 192.168.1.9, executor 1, partition 163, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:48 INFO TaskSetManager: Finished task 162.0 in stage 3.0 (TID 90) in 1980 ms on 192.168.1.9 (executor 1) (88/200)
20/12/04 16:07:51 INFO TaskSetManager: Starting task 164.0 in stage 3.0 (TID 92, 192.168.1.9, executor 1, partition 164, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:51 INFO TaskSetManager: Finished task 163.0 in stage 3.0 (TID 91) in 2305 ms on 192.168.1.9 (executor 1) (89/200)
20/12/04 16:07:55 INFO TaskSetManager: Starting task 165.0 in stage 3.0 (TID 93, 192.168.1.9, executor 1, partition 165, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:55 INFO TaskSetManager: Finished task 164.0 in stage 3.0 (TID 92) in 4422 ms on 192.168.1.9 (executor 1) (90/200)
20/12/04 16:07:57 INFO TaskSetManager: Starting task 167.0 in stage 3.0 (TID 94, 192.168.1.9, executor 1, partition 167, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:57 INFO TaskSetManager: Finished task 165.0 in stage 3.0 (TID 93) in 2099 ms on 192.168.1.9 (executor 1) (91/200)
20/12/04 16:07:59 INFO TaskSetManager: Starting task 168.0 in stage 3.0 (TID 95, 192.168.1.9, executor 1, partition 168, NODE_LOCAL, 7336 bytes)
20/12/04 16:07:59 INFO TaskSetManager: Finished task 167.0 in stage 3.0 (TID 94) in 2219 ms on 192.168.1.9 (executor 1) (92/200)
20/12/04 16:08:00 INFO TaskSetManager: Starting task 170.0 in stage 3.0 (TID 96, 192.168.1.9, executor 1, partition 170, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:00 INFO TaskSetManager: Finished task 168.0 in stage 3.0 (TID 95) in 671 ms on 192.168.1.9 (executor 1) (93/200)
20/12/04 16:08:01 INFO TaskSetManager: Starting task 172.0 in stage 3.0 (TID 97, 192.168.1.9, executor 1, partition 172, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:01 INFO TaskSetManager: Finished task 170.0 in stage 3.0 (TID 96) in 1115 ms on 192.168.1.9 (executor 1) (94/200)
20/12/04 16:08:02 INFO TaskSetManager: Starting task 173.0 in stage 3.0 (TID 98, 192.168.1.9, executor 1, partition 173, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:02 INFO TaskSetManager: Finished task 172.0 in stage 3.0 (TID 97) in 1064 ms on 192.168.1.9 (executor 1) (95/200)
20/12/04 16:08:05 INFO TaskSetManager: Starting task 174.0 in stage 3.0 (TID 99, 192.168.1.9, executor 1, partition 174, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:05 INFO TaskSetManager: Finished task 173.0 in stage 3.0 (TID 98) in 2401 ms on 192.168.1.9 (executor 1) (96/200)
20/12/04 16:08:07 INFO TaskSetManager: Starting task 175.0 in stage 3.0 (TID 100, 192.168.1.9, executor 1, partition 175, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:07 INFO TaskSetManager: Finished task 174.0 in stage 3.0 (TID 99) in 1955 ms on 192.168.1.9 (executor 1) (97/200)
20/12/04 16:08:08 INFO TaskSetManager: Starting task 178.0 in stage 3.0 (TID 101, 192.168.1.9, executor 1, partition 178, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:08 INFO TaskSetManager: Finished task 175.0 in stage 3.0 (TID 100) in 1804 ms on 192.168.1.9 (executor 1) (98/200)
20/12/04 16:08:09 INFO TaskSetManager: Starting task 179.0 in stage 3.0 (TID 102, 192.168.1.9, executor 1, partition 179, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:09 INFO TaskSetManager: Finished task 178.0 in stage 3.0 (TID 101) in 760 ms on 192.168.1.9 (executor 1) (99/200)
20/12/04 16:08:10 INFO TaskSetManager: Starting task 181.0 in stage 3.0 (TID 103, 192.168.1.9, executor 1, partition 181, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:10 INFO TaskSetManager: Finished task 179.0 in stage 3.0 (TID 102) in 1240 ms on 192.168.1.9 (executor 1) (100/200)
20/12/04 16:08:12 INFO TaskSetManager: Starting task 183.0 in stage 3.0 (TID 104, 192.168.1.9, executor 1, partition 183, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:12 INFO TaskSetManager: Finished task 181.0 in stage 3.0 (TID 103) in 1963 ms on 192.168.1.9 (executor 1) (101/200)
20/12/04 16:08:13 INFO TaskSetManager: Starting task 184.0 in stage 3.0 (TID 105, 192.168.1.9, executor 1, partition 184, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:13 INFO TaskSetManager: Finished task 183.0 in stage 3.0 (TID 104) in 598 ms on 192.168.1.9 (executor 1) (102/200)
20/12/04 16:08:15 INFO TaskSetManager: Starting task 190.0 in stage 3.0 (TID 106, 192.168.1.9, executor 1, partition 190, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:15 INFO TaskSetManager: Finished task 184.0 in stage 3.0 (TID 105) in 1823 ms on 192.168.1.9 (executor 1) (103/200)
20/12/04 16:08:16 INFO TaskSetManager: Starting task 192.0 in stage 3.0 (TID 107, 192.168.1.9, executor 1, partition 192, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:16 INFO TaskSetManager: Finished task 190.0 in stage 3.0 (TID 106) in 1248 ms on 192.168.1.9 (executor 1) (104/200)
20/12/04 16:08:16 INFO TaskSetManager: Starting task 193.0 in stage 3.0 (TID 108, 192.168.1.9, executor 1, partition 193, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:17 INFO TaskSetManager: Finished task 192.0 in stage 3.0 (TID 107) in 507 ms on 192.168.1.9 (executor 1) (105/200)
20/12/04 16:08:18 INFO TaskSetManager: Starting task 194.0 in stage 3.0 (TID 109, 192.168.1.9, executor 1, partition 194, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:18 INFO TaskSetManager: Finished task 193.0 in stage 3.0 (TID 108) in 1930 ms on 192.168.1.9 (executor 1) (106/200)
20/12/04 16:08:19 INFO TaskSetManager: Starting task 195.0 in stage 3.0 (TID 110, 192.168.1.9, executor 1, partition 195, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:19 INFO TaskSetManager: Finished task 194.0 in stage 3.0 (TID 109) in 952 ms on 192.168.1.9 (executor 1) (107/200)
20/12/04 16:08:20 INFO TaskSetManager: Starting task 198.0 in stage 3.0 (TID 111, 192.168.1.9, executor 1, partition 198, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:20 INFO TaskSetManager: Finished task 195.0 in stage 3.0 (TID 110) in 910 ms on 192.168.1.9 (executor 1) (108/200)
20/12/04 16:08:21 INFO TaskSetManager: Starting task 199.0 in stage 3.0 (TID 112, 192.168.1.9, executor 1, partition 199, NODE_LOCAL, 7336 bytes)
20/12/04 16:08:21 INFO TaskSetManager: Finished task 198.0 in stage 3.0 (TID 111) in 565 ms on 192.168.1.9 (executor 1) (109/200)
20/12/04 16:08:22 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 113, 192.168.1.9, executor 1, partition 1, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:22 INFO TaskSetManager: Finished task 199.0 in stage 3.0 (TID 112) in 1245 ms on 192.168.1.9 (executor 1) (110/200)
20/12/04 16:08:22 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 114, 192.168.1.9, executor 1, partition 8, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:22 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 113) in 109 ms on 192.168.1.9 (executor 1) (111/200)
20/12/04 16:08:22 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 115, 192.168.1.9, executor 1, partition 9, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:22 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 114) in 127 ms on 192.168.1.9 (executor 1) (112/200)
20/12/04 16:08:22 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 116, 192.168.1.9, executor 1, partition 15, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:22 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 115) in 106 ms on 192.168.1.9 (executor 1) (113/200)
20/12/04 16:08:23 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 117, 192.168.1.9, executor 1, partition 16, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:23 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 116) in 106 ms on 192.168.1.9 (executor 1) (114/200)
20/12/04 16:08:23 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 118, 192.168.1.9, executor 1, partition 17, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:23 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 117) in 106 ms on 192.168.1.9 (executor 1) (115/200)
20/12/04 16:08:23 INFO TaskSetManager: Starting task 20.0 in stage 3.0 (TID 119, 192.168.1.9, executor 1, partition 20, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:23 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 118) in 117 ms on 192.168.1.9 (executor 1) (116/200)
20/12/04 16:08:23 INFO TaskSetManager: Starting task 22.0 in stage 3.0 (TID 120, 192.168.1.9, executor 1, partition 22, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:23 INFO TaskSetManager: Finished task 20.0 in stage 3.0 (TID 119) in 117 ms on 192.168.1.9 (executor 1) (117/200)
20/12/04 16:08:23 INFO TaskSetManager: Starting task 25.0 in stage 3.0 (TID 121, 192.168.1.9, executor 1, partition 25, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:23 INFO TaskSetManager: Finished task 22.0 in stage 3.0 (TID 120) in 114 ms on 192.168.1.9 (executor 1) (118/200)
20/12/04 16:08:23 INFO TaskSetManager: Starting task 26.0 in stage 3.0 (TID 122, 192.168.1.9, executor 1, partition 26, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:23 INFO TaskSetManager: Finished task 25.0 in stage 3.0 (TID 121) in 119 ms on 192.168.1.9 (executor 1) (119/200)
20/12/04 16:08:23 INFO TaskSetManager: Starting task 28.0 in stage 3.0 (TID 123, 192.168.1.9, executor 1, partition 28, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:23 INFO TaskSetManager: Finished task 26.0 in stage 3.0 (TID 122) in 128 ms on 192.168.1.9 (executor 1) (120/200)
20/12/04 16:08:23 INFO TaskSetManager: Starting task 29.0 in stage 3.0 (TID 124, 192.168.1.9, executor 1, partition 29, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:23 INFO TaskSetManager: Finished task 28.0 in stage 3.0 (TID 123) in 124 ms on 192.168.1.9 (executor 1) (121/200)
20/12/04 16:08:23 INFO TaskSetManager: Starting task 33.0 in stage 3.0 (TID 125, 192.168.1.9, executor 1, partition 33, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:23 INFO TaskSetManager: Finished task 29.0 in stage 3.0 (TID 124) in 117 ms on 192.168.1.9 (executor 1) (122/200)
20/12/04 16:08:24 INFO TaskSetManager: Starting task 38.0 in stage 3.0 (TID 126, 192.168.1.9, executor 1, partition 38, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:24 INFO TaskSetManager: Finished task 33.0 in stage 3.0 (TID 125) in 118 ms on 192.168.1.9 (executor 1) (123/200)
20/12/04 16:08:24 INFO TaskSetManager: Starting task 39.0 in stage 3.0 (TID 127, 192.168.1.9, executor 1, partition 39, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:24 INFO TaskSetManager: Finished task 38.0 in stage 3.0 (TID 126) in 115 ms on 192.168.1.9 (executor 1) (124/200)
20/12/04 16:08:24 INFO TaskSetManager: Starting task 40.0 in stage 3.0 (TID 128, 192.168.1.9, executor 1, partition 40, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:24 INFO TaskSetManager: Finished task 39.0 in stage 3.0 (TID 127) in 123 ms on 192.168.1.9 (executor 1) (125/200)
20/12/04 16:08:24 INFO TaskSetManager: Starting task 42.0 in stage 3.0 (TID 129, 192.168.1.9, executor 1, partition 42, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:24 INFO TaskSetManager: Finished task 40.0 in stage 3.0 (TID 128) in 124 ms on 192.168.1.9 (executor 1) (126/200)
20/12/04 16:08:24 INFO TaskSetManager: Starting task 45.0 in stage 3.0 (TID 130, 192.168.1.9, executor 1, partition 45, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:24 INFO TaskSetManager: Finished task 42.0 in stage 3.0 (TID 129) in 122 ms on 192.168.1.9 (executor 1) (127/200)
20/12/04 16:08:24 INFO TaskSetManager: Starting task 46.0 in stage 3.0 (TID 131, 192.168.1.9, executor 1, partition 46, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:24 INFO TaskSetManager: Finished task 45.0 in stage 3.0 (TID 130) in 122 ms on 192.168.1.9 (executor 1) (128/200)
20/12/04 16:08:24 INFO TaskSetManager: Starting task 47.0 in stage 3.0 (TID 132, 192.168.1.9, executor 1, partition 47, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:24 INFO TaskSetManager: Finished task 46.0 in stage 3.0 (TID 131) in 127 ms on 192.168.1.9 (executor 1) (129/200)
20/12/04 16:08:24 INFO TaskSetManager: Starting task 50.0 in stage 3.0 (TID 133, 192.168.1.9, executor 1, partition 50, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:24 INFO TaskSetManager: Finished task 47.0 in stage 3.0 (TID 132) in 127 ms on 192.168.1.9 (executor 1) (130/200)
20/12/04 16:08:25 INFO TaskSetManager: Starting task 52.0 in stage 3.0 (TID 134, 192.168.1.9, executor 1, partition 52, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:25 INFO TaskSetManager: Finished task 50.0 in stage 3.0 (TID 133) in 121 ms on 192.168.1.9 (executor 1) (131/200)
20/12/04 16:08:25 INFO TaskSetManager: Starting task 54.0 in stage 3.0 (TID 135, 192.168.1.9, executor 1, partition 54, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:25 INFO TaskSetManager: Finished task 52.0 in stage 3.0 (TID 134) in 177 ms on 192.168.1.9 (executor 1) (132/200)
20/12/04 16:08:25 INFO TaskSetManager: Starting task 56.0 in stage 3.0 (TID 136, 192.168.1.9, executor 1, partition 56, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:25 INFO TaskSetManager: Finished task 54.0 in stage 3.0 (TID 135) in 123 ms on 192.168.1.9 (executor 1) (133/200)
20/12/04 16:08:25 INFO TaskSetManager: Starting task 59.0 in stage 3.0 (TID 137, 192.168.1.9, executor 1, partition 59, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:25 INFO TaskSetManager: Finished task 56.0 in stage 3.0 (TID 136) in 116 ms on 192.168.1.9 (executor 1) (134/200)
20/12/04 16:08:25 INFO TaskSetManager: Starting task 60.0 in stage 3.0 (TID 138, 192.168.1.9, executor 1, partition 60, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:25 INFO TaskSetManager: Finished task 59.0 in stage 3.0 (TID 137) in 121 ms on 192.168.1.9 (executor 1) (135/200)
20/12/04 16:08:25 INFO TaskSetManager: Starting task 62.0 in stage 3.0 (TID 139, 192.168.1.9, executor 1, partition 62, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:25 INFO TaskSetManager: Finished task 60.0 in stage 3.0 (TID 138) in 117 ms on 192.168.1.9 (executor 1) (136/200)
20/12/04 16:08:25 INFO TaskSetManager: Starting task 64.0 in stage 3.0 (TID 140, 192.168.1.9, executor 1, partition 64, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:25 INFO TaskSetManager: Finished task 62.0 in stage 3.0 (TID 139) in 123 ms on 192.168.1.9 (executor 1) (137/200)
20/12/04 16:08:25 INFO TaskSetManager: Starting task 67.0 in stage 3.0 (TID 141, 192.168.1.9, executor 1, partition 67, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:25 INFO TaskSetManager: Finished task 64.0 in stage 3.0 (TID 140) in 113 ms on 192.168.1.9 (executor 1) (138/200)
20/12/04 16:08:26 INFO TaskSetManager: Starting task 68.0 in stage 3.0 (TID 142, 192.168.1.9, executor 1, partition 68, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:26 INFO TaskSetManager: Finished task 67.0 in stage 3.0 (TID 141) in 113 ms on 192.168.1.9 (executor 1) (139/200)
20/12/04 16:08:26 INFO TaskSetManager: Starting task 71.0 in stage 3.0 (TID 143, 192.168.1.9, executor 1, partition 71, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:26 INFO TaskSetManager: Finished task 68.0 in stage 3.0 (TID 142) in 119 ms on 192.168.1.9 (executor 1) (140/200)
20/12/04 16:08:26 INFO TaskSetManager: Starting task 72.0 in stage 3.0 (TID 144, 192.168.1.9, executor 1, partition 72, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:26 INFO TaskSetManager: Finished task 71.0 in stage 3.0 (TID 143) in 114 ms on 192.168.1.9 (executor 1) (141/200)
20/12/04 16:08:26 INFO TaskSetManager: Starting task 74.0 in stage 3.0 (TID 145, 192.168.1.9, executor 1, partition 74, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:26 INFO TaskSetManager: Finished task 72.0 in stage 3.0 (TID 144) in 114 ms on 192.168.1.9 (executor 1) (142/200)
20/12/04 16:08:26 INFO TaskSetManager: Starting task 76.0 in stage 3.0 (TID 146, 192.168.1.9, executor 1, partition 76, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:26 INFO TaskSetManager: Finished task 74.0 in stage 3.0 (TID 145) in 119 ms on 192.168.1.9 (executor 1) (143/200)
20/12/04 16:08:26 INFO TaskSetManager: Starting task 78.0 in stage 3.0 (TID 147, 192.168.1.9, executor 1, partition 78, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:26 INFO TaskSetManager: Finished task 76.0 in stage 3.0 (TID 146) in 124 ms on 192.168.1.9 (executor 1) (144/200)
20/12/04 16:08:26 INFO TaskSetManager: Starting task 80.0 in stage 3.0 (TID 148, 192.168.1.9, executor 1, partition 80, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:26 INFO TaskSetManager: Finished task 78.0 in stage 3.0 (TID 147) in 118 ms on 192.168.1.9 (executor 1) (145/200)
20/12/04 16:08:26 INFO TaskSetManager: Starting task 81.0 in stage 3.0 (TID 149, 192.168.1.9, executor 1, partition 81, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:26 INFO TaskSetManager: Finished task 80.0 in stage 3.0 (TID 148) in 121 ms on 192.168.1.9 (executor 1) (146/200)
20/12/04 16:08:27 INFO TaskSetManager: Starting task 82.0 in stage 3.0 (TID 150, 192.168.1.9, executor 1, partition 82, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:27 INFO TaskSetManager: Finished task 81.0 in stage 3.0 (TID 149) in 114 ms on 192.168.1.9 (executor 1) (147/200)
20/12/04 16:08:27 INFO TaskSetManager: Starting task 83.0 in stage 3.0 (TID 151, 192.168.1.9, executor 1, partition 83, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:27 INFO TaskSetManager: Finished task 82.0 in stage 3.0 (TID 150) in 112 ms on 192.168.1.9 (executor 1) (148/200)
20/12/04 16:08:27 INFO TaskSetManager: Starting task 87.0 in stage 3.0 (TID 152, 192.168.1.9, executor 1, partition 87, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:27 INFO TaskSetManager: Finished task 83.0 in stage 3.0 (TID 151) in 112 ms on 192.168.1.9 (executor 1) (149/200)
20/12/04 16:08:27 INFO TaskSetManager: Starting task 90.0 in stage 3.0 (TID 153, 192.168.1.9, executor 1, partition 90, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:27 INFO TaskSetManager: Finished task 87.0 in stage 3.0 (TID 152) in 132 ms on 192.168.1.9 (executor 1) (150/200)
20/12/04 16:08:27 INFO TaskSetManager: Starting task 92.0 in stage 3.0 (TID 154, 192.168.1.9, executor 1, partition 92, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:27 INFO TaskSetManager: Finished task 90.0 in stage 3.0 (TID 153) in 123 ms on 192.168.1.9 (executor 1) (151/200)
20/12/04 16:08:27 INFO TaskSetManager: Starting task 93.0 in stage 3.0 (TID 155, 192.168.1.9, executor 1, partition 93, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:27 INFO TaskSetManager: Finished task 92.0 in stage 3.0 (TID 154) in 116 ms on 192.168.1.9 (executor 1) (152/200)
20/12/04 16:08:27 INFO TaskSetManager: Starting task 94.0 in stage 3.0 (TID 156, 192.168.1.9, executor 1, partition 94, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:27 INFO TaskSetManager: Finished task 93.0 in stage 3.0 (TID 155) in 113 ms on 192.168.1.9 (executor 1) (153/200)
20/12/04 16:08:27 INFO TaskSetManager: Starting task 96.0 in stage 3.0 (TID 157, 192.168.1.9, executor 1, partition 96, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:27 INFO TaskSetManager: Finished task 94.0 in stage 3.0 (TID 156) in 113 ms on 192.168.1.9 (executor 1) (154/200)
20/12/04 16:08:27 INFO TaskSetManager: Starting task 97.0 in stage 3.0 (TID 158, 192.168.1.9, executor 1, partition 97, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:27 INFO TaskSetManager: Finished task 96.0 in stage 3.0 (TID 157) in 113 ms on 192.168.1.9 (executor 1) (155/200)
20/12/04 16:08:28 INFO TaskSetManager: Starting task 98.0 in stage 3.0 (TID 159, 192.168.1.9, executor 1, partition 98, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:28 INFO TaskSetManager: Finished task 97.0 in stage 3.0 (TID 158) in 118 ms on 192.168.1.9 (executor 1) (156/200)
20/12/04 16:08:28 INFO TaskSetManager: Starting task 99.0 in stage 3.0 (TID 160, 192.168.1.9, executor 1, partition 99, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:28 INFO TaskSetManager: Finished task 98.0 in stage 3.0 (TID 159) in 115 ms on 192.168.1.9 (executor 1) (157/200)
20/12/04 16:08:28 INFO TaskSetManager: Starting task 100.0 in stage 3.0 (TID 161, 192.168.1.9, executor 1, partition 100, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:28 INFO TaskSetManager: Finished task 99.0 in stage 3.0 (TID 160) in 117 ms on 192.168.1.9 (executor 1) (158/200)
20/12/04 16:08:28 INFO TaskSetManager: Starting task 104.0 in stage 3.0 (TID 162, 192.168.1.9, executor 1, partition 104, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:28 INFO TaskSetManager: Finished task 100.0 in stage 3.0 (TID 161) in 120 ms on 192.168.1.9 (executor 1) (159/200)
20/12/04 16:08:28 INFO TaskSetManager: Starting task 110.0 in stage 3.0 (TID 163, 192.168.1.9, executor 1, partition 110, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:28 INFO TaskSetManager: Finished task 104.0 in stage 3.0 (TID 162) in 115 ms on 192.168.1.9 (executor 1) (160/200)
20/12/04 16:08:28 INFO TaskSetManager: Starting task 112.0 in stage 3.0 (TID 164, 192.168.1.9, executor 1, partition 112, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:28 INFO TaskSetManager: Finished task 110.0 in stage 3.0 (TID 163) in 120 ms on 192.168.1.9 (executor 1) (161/200)
20/12/04 16:08:28 INFO TaskSetManager: Starting task 114.0 in stage 3.0 (TID 165, 192.168.1.9, executor 1, partition 114, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:28 INFO TaskSetManager: Finished task 112.0 in stage 3.0 (TID 164) in 133 ms on 192.168.1.9 (executor 1) (162/200)
20/12/04 16:08:28 INFO TaskSetManager: Starting task 117.0 in stage 3.0 (TID 166, 192.168.1.9, executor 1, partition 117, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:28 INFO TaskSetManager: Finished task 114.0 in stage 3.0 (TID 165) in 114 ms on 192.168.1.9 (executor 1) (163/200)
20/12/04 16:08:28 INFO TaskSetManager: Starting task 118.0 in stage 3.0 (TID 167, 192.168.1.9, executor 1, partition 118, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:29 INFO TaskSetManager: Finished task 117.0 in stage 3.0 (TID 166) in 116 ms on 192.168.1.9 (executor 1) (164/200)
20/12/04 16:08:29 INFO TaskSetManager: Starting task 120.0 in stage 3.0 (TID 168, 192.168.1.9, executor 1, partition 120, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:29 INFO TaskSetManager: Finished task 118.0 in stage 3.0 (TID 167) in 117 ms on 192.168.1.9 (executor 1) (165/200)
20/12/04 16:08:29 INFO TaskSetManager: Starting task 121.0 in stage 3.0 (TID 169, 192.168.1.9, executor 1, partition 121, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:29 INFO TaskSetManager: Finished task 120.0 in stage 3.0 (TID 168) in 114 ms on 192.168.1.9 (executor 1) (166/200)
20/12/04 16:08:29 INFO TaskSetManager: Starting task 123.0 in stage 3.0 (TID 170, 192.168.1.9, executor 1, partition 123, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:29 INFO TaskSetManager: Finished task 121.0 in stage 3.0 (TID 169) in 121 ms on 192.168.1.9 (executor 1) (167/200)
20/12/04 16:08:29 INFO TaskSetManager: Starting task 125.0 in stage 3.0 (TID 171, 192.168.1.9, executor 1, partition 125, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:29 INFO TaskSetManager: Finished task 123.0 in stage 3.0 (TID 170) in 122 ms on 192.168.1.9 (executor 1) (168/200)
20/12/04 16:08:29 INFO TaskSetManager: Starting task 127.0 in stage 3.0 (TID 172, 192.168.1.9, executor 1, partition 127, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:29 INFO TaskSetManager: Finished task 125.0 in stage 3.0 (TID 171) in 118 ms on 192.168.1.9 (executor 1) (169/200)
20/12/04 16:08:29 INFO TaskSetManager: Starting task 130.0 in stage 3.0 (TID 173, 192.168.1.9, executor 1, partition 130, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:29 INFO TaskSetManager: Finished task 127.0 in stage 3.0 (TID 172) in 113 ms on 192.168.1.9 (executor 1) (170/200)
20/12/04 16:08:29 INFO TaskSetManager: Starting task 134.0 in stage 3.0 (TID 174, 192.168.1.9, executor 1, partition 134, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:29 INFO TaskSetManager: Finished task 130.0 in stage 3.0 (TID 173) in 114 ms on 192.168.1.9 (executor 1) (171/200)
20/12/04 16:08:29 INFO TaskSetManager: Starting task 138.0 in stage 3.0 (TID 175, 192.168.1.9, executor 1, partition 138, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:29 INFO TaskSetManager: Finished task 134.0 in stage 3.0 (TID 174) in 134 ms on 192.168.1.9 (executor 1) (172/200)
20/12/04 16:08:30 INFO TaskSetManager: Starting task 142.0 in stage 3.0 (TID 176, 192.168.1.9, executor 1, partition 142, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:30 INFO TaskSetManager: Finished task 138.0 in stage 3.0 (TID 175) in 117 ms on 192.168.1.9 (executor 1) (173/200)
20/12/04 16:08:30 INFO TaskSetManager: Starting task 144.0 in stage 3.0 (TID 177, 192.168.1.9, executor 1, partition 144, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:30 INFO TaskSetManager: Finished task 142.0 in stage 3.0 (TID 176) in 132 ms on 192.168.1.9 (executor 1) (174/200)
20/12/04 16:08:30 INFO TaskSetManager: Starting task 145.0 in stage 3.0 (TID 178, 192.168.1.9, executor 1, partition 145, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:30 INFO TaskSetManager: Finished task 144.0 in stage 3.0 (TID 177) in 112 ms on 192.168.1.9 (executor 1) (175/200)
20/12/04 16:08:30 INFO TaskSetManager: Starting task 147.0 in stage 3.0 (TID 179, 192.168.1.9, executor 1, partition 147, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:30 INFO TaskSetManager: Finished task 145.0 in stage 3.0 (TID 178) in 133 ms on 192.168.1.9 (executor 1) (176/200)
20/12/04 16:08:30 INFO TaskSetManager: Starting task 148.0 in stage 3.0 (TID 180, 192.168.1.9, executor 1, partition 148, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:30 INFO TaskSetManager: Finished task 147.0 in stage 3.0 (TID 179) in 119 ms on 192.168.1.9 (executor 1) (177/200)
20/12/04 16:08:30 INFO TaskSetManager: Starting task 149.0 in stage 3.0 (TID 181, 192.168.1.9, executor 1, partition 149, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:30 INFO TaskSetManager: Finished task 148.0 in stage 3.0 (TID 180) in 113 ms on 192.168.1.9 (executor 1) (178/200)
20/12/04 16:08:30 INFO TaskSetManager: Starting task 152.0 in stage 3.0 (TID 182, 192.168.1.9, executor 1, partition 152, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:30 INFO TaskSetManager: Finished task 149.0 in stage 3.0 (TID 181) in 114 ms on 192.168.1.9 (executor 1) (179/200)
20/12/04 16:08:30 INFO TaskSetManager: Starting task 154.0 in stage 3.0 (TID 183, 192.168.1.9, executor 1, partition 154, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:30 INFO TaskSetManager: Finished task 152.0 in stage 3.0 (TID 182) in 119 ms on 192.168.1.9 (executor 1) (180/200)
20/12/04 16:08:31 INFO TaskSetManager: Starting task 157.0 in stage 3.0 (TID 184, 192.168.1.9, executor 1, partition 157, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:31 INFO TaskSetManager: Finished task 154.0 in stage 3.0 (TID 183) in 113 ms on 192.168.1.9 (executor 1) (181/200)
20/12/04 16:08:31 INFO TaskSetManager: Starting task 158.0 in stage 3.0 (TID 185, 192.168.1.9, executor 1, partition 158, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:31 INFO TaskSetManager: Finished task 157.0 in stage 3.0 (TID 184) in 111 ms on 192.168.1.9 (executor 1) (182/200)
20/12/04 16:08:31 INFO TaskSetManager: Starting task 159.0 in stage 3.0 (TID 186, 192.168.1.9, executor 1, partition 159, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:31 INFO TaskSetManager: Finished task 158.0 in stage 3.0 (TID 185) in 113 ms on 192.168.1.9 (executor 1) (183/200)
20/12/04 16:08:31 INFO TaskSetManager: Starting task 160.0 in stage 3.0 (TID 187, 192.168.1.9, executor 1, partition 160, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:31 INFO TaskSetManager: Finished task 159.0 in stage 3.0 (TID 186) in 113 ms on 192.168.1.9 (executor 1) (184/200)
20/12/04 16:08:31 INFO TaskSetManager: Starting task 166.0 in stage 3.0 (TID 188, 192.168.1.9, executor 1, partition 166, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:31 INFO TaskSetManager: Finished task 160.0 in stage 3.0 (TID 187) in 116 ms on 192.168.1.9 (executor 1) (185/200)
20/12/04 16:08:31 INFO TaskSetManager: Starting task 169.0 in stage 3.0 (TID 189, 192.168.1.9, executor 1, partition 169, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:31 INFO TaskSetManager: Finished task 166.0 in stage 3.0 (TID 188) in 122 ms on 192.168.1.9 (executor 1) (186/200)
20/12/04 16:08:31 INFO TaskSetManager: Starting task 171.0 in stage 3.0 (TID 190, 192.168.1.9, executor 1, partition 171, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:31 INFO TaskSetManager: Finished task 169.0 in stage 3.0 (TID 189) in 118 ms on 192.168.1.9 (executor 1) (187/200)
20/12/04 16:08:31 INFO TaskSetManager: Starting task 176.0 in stage 3.0 (TID 191, 192.168.1.9, executor 1, partition 176, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:31 INFO TaskSetManager: Finished task 171.0 in stage 3.0 (TID 190) in 116 ms on 192.168.1.9 (executor 1) (188/200)
20/12/04 16:08:31 INFO TaskSetManager: Starting task 177.0 in stage 3.0 (TID 192, 192.168.1.9, executor 1, partition 177, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:31 INFO TaskSetManager: Finished task 176.0 in stage 3.0 (TID 191) in 116 ms on 192.168.1.9 (executor 1) (189/200)
20/12/04 16:08:32 INFO TaskSetManager: Starting task 180.0 in stage 3.0 (TID 193, 192.168.1.9, executor 1, partition 180, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:32 INFO TaskSetManager: Finished task 177.0 in stage 3.0 (TID 192) in 118 ms on 192.168.1.9 (executor 1) (190/200)
20/12/04 16:08:32 INFO TaskSetManager: Starting task 182.0 in stage 3.0 (TID 194, 192.168.1.9, executor 1, partition 182, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:32 INFO TaskSetManager: Finished task 180.0 in stage 3.0 (TID 193) in 114 ms on 192.168.1.9 (executor 1) (191/200)
20/12/04 16:08:32 INFO TaskSetManager: Starting task 185.0 in stage 3.0 (TID 195, 192.168.1.9, executor 1, partition 185, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:32 INFO TaskSetManager: Finished task 182.0 in stage 3.0 (TID 194) in 114 ms on 192.168.1.9 (executor 1) (192/200)
20/12/04 16:08:32 INFO TaskSetManager: Starting task 186.0 in stage 3.0 (TID 196, 192.168.1.9, executor 1, partition 186, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:32 INFO TaskSetManager: Finished task 185.0 in stage 3.0 (TID 195) in 119 ms on 192.168.1.9 (executor 1) (193/200)
20/12/04 16:08:32 INFO TaskSetManager: Starting task 187.0 in stage 3.0 (TID 197, 192.168.1.9, executor 1, partition 187, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:32 INFO TaskSetManager: Finished task 186.0 in stage 3.0 (TID 196) in 117 ms on 192.168.1.9 (executor 1) (194/200)
20/12/04 16:08:32 INFO TaskSetManager: Starting task 188.0 in stage 3.0 (TID 198, 192.168.1.9, executor 1, partition 188, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:32 INFO TaskSetManager: Finished task 187.0 in stage 3.0 (TID 197) in 115 ms on 192.168.1.9 (executor 1) (195/200)
20/12/04 16:08:32 INFO TaskSetManager: Starting task 189.0 in stage 3.0 (TID 199, 192.168.1.9, executor 1, partition 189, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:32 INFO TaskSetManager: Finished task 188.0 in stage 3.0 (TID 198) in 112 ms on 192.168.1.9 (executor 1) (196/200)
20/12/04 16:08:32 INFO TaskSetManager: Starting task 191.0 in stage 3.0 (TID 200, 192.168.1.9, executor 1, partition 191, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:32 INFO TaskSetManager: Finished task 189.0 in stage 3.0 (TID 199) in 114 ms on 192.168.1.9 (executor 1) (197/200)
20/12/04 16:08:32 INFO TaskSetManager: Starting task 196.0 in stage 3.0 (TID 201, 192.168.1.9, executor 1, partition 196, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:32 INFO TaskSetManager: Finished task 191.0 in stage 3.0 (TID 200) in 113 ms on 192.168.1.9 (executor 1) (198/200)
20/12/04 16:08:33 INFO TaskSetManager: Starting task 197.0 in stage 3.0 (TID 202, 192.168.1.9, executor 1, partition 197, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:08:33 INFO TaskSetManager: Finished task 196.0 in stage 3.0 (TID 201) in 119 ms on 192.168.1.9 (executor 1) (199/200)
20/12/04 16:08:33 INFO TaskSetManager: Finished task 197.0 in stage 3.0 (TID 202) in 115 ms on 192.168.1.9 (executor 1) (200/200)
20/12/04 16:08:33 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 16:08:33 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/userFiles-f2dc4b20-eee5-44c5-9b1e-7b4235fec5a4/darima.zip/darima/dlsa.py:22) finished in 172.284 s
20/12/04 16:08:33 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:08:33 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
20/12/04 16:08:33 INFO DAGScheduler: Job 1 finished: toPandas at /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/userFiles-f2dc4b20-eee5-44c5-9b1e-7b4235fec5a4/darima.zip/darima/dlsa.py:22, took 173.657385 s
20/12/04 16:08:33 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:08:33 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:08:33 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:08:33 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:08:33 INFO CodeGenerator: Code generated in 15.910615 ms
20/12/04 16:08:33 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:08:33 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:08:33 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:40051 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:08:33 INFO SparkContext: Created broadcast 6 from toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209
20/12/04 16:08:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:08:33 INFO SparkContext: Starting job: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209
20/12/04 16:08:33 INFO DAGScheduler: Got job 2 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209) with 1 output partitions
20/12/04 16:08:33 INFO DAGScheduler: Final stage: ResultStage 4 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209)
20/12/04 16:08:33 INFO DAGScheduler: Parents of final stage: List()
20/12/04 16:08:33 INFO DAGScheduler: Missing parents: List()
20/12/04 16:08:33 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209), which has no missing parents
20/12/04 16:08:34 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 21.7 KiB, free 5.8 GiB)
20/12/04 16:08:34 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 5.8 GiB)
20/12/04 16:08:34 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:40051 (size: 10.4 KiB, free: 5.8 GiB)
20/12/04 16:08:34 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1223
20/12/04 16:08:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209) (first 15 tasks are for partitions Vector(0))
20/12/04 16:08:34 INFO YarnScheduler: Adding task set 4.0 with 1 tasks
20/12/04 16:08:34 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 203, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7790 bytes)
20/12/04 16:08:34 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:40889 (size: 10.4 KiB, free: 912.1 MiB)
20/12/04 16:08:34 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:40889 (size: 28.7 KiB, free: 912.1 MiB)
20/12/04 16:08:34 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 203) in 380 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:08:34 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/12/04 16:08:34 INFO DAGScheduler: ResultStage 4 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209) finished in 0.390 s
20/12/04 16:08:34 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:08:34 INFO YarnScheduler: Killing all running tasks in stage 4: Stage finished
20/12/04 16:08:34 INFO DAGScheduler: Job 2 finished: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209, took 0.397140 s
20/12/04 16:08:34 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 16:08:34 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 16:08:34 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 16:08:34 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 16:08:34 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 16:08:34 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 16:08:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 16:08:34 INFO MemoryStore: MemoryStore cleared
20/12/04 16:08:34 INFO BlockManager: BlockManager stopped
20/12/04 16:08:34 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 16:08:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 16:08:34 INFO SparkContext: Successfully stopped SparkContext
20/12/04 16:08:34 INFO ShutdownHookManager: Shutdown hook called
20/12/04 16:08:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e/pyspark-6b7245d8-13d5-4087-bd1f-54abbc15c5e0
20/12/04 16:08:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-46f358ce-4ab3-464c-81db-1a88fa0aa0f9
20/12/04 16:08:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-1e72b273-e530-4fc3-bda0-57ff70133c7e
20/12/04 16:08:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 16:08:37 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:08:37 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:08:37 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:08:37 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:08:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:08:38 INFO SparkContext: Running Spark version 3.0.1
20/12/04 16:08:38 INFO ResourceUtils: ==============================================================
20/12/04 16:08:38 INFO ResourceUtils: Resources for spark.driver:

20/12/04 16:08:38 INFO ResourceUtils: ==============================================================
20/12/04 16:08:38 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 16:08:38 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:08:38 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:08:38 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:08:38 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:08:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:08:38 INFO Utils: Successfully started service 'sparkDriver' on port 37711.
20/12/04 16:08:38 INFO SparkEnv: Registering MapOutputTracker
20/12/04 16:08:38 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 16:08:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 16:08:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 16:08:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 16:08:38 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dad0c813-db54-4087-8612-3d46397f06ba
20/12/04 16:08:39 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 16:08:39 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 16:08:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 16:08:39 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 16:08:39 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 16:08:40 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 16:08:40 INFO Configuration: resource-types.xml not found
20/12/04 16:08:40 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 16:08:40 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 16:08:40 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 16:08:40 INFO Client: Setting up container launch context for our AM
20/12/04 16:08:40 INFO Client: Setting up the launch environment for our AM container
20/12/04 16:08:40 INFO Client: Preparing resources for our AM container
20/12/04 16:08:40 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 16:08:42 INFO Client: Uploading resource file:/tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/__spark_libs__16208573401808351679.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0072/__spark_libs__16208573401808351679.zip
20/12/04 16:08:43 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0072/pyspark.zip
20/12/04 16:08:43 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0072/py4j-0.10.9-src.zip
20/12/04 16:08:44 INFO Client: Uploading resource file:/tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/__spark_conf__11477320386834313519.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0072/__spark_conf__.zip
20/12/04 16:08:44 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:08:44 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:08:44 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:08:44 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:08:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:08:44 INFO Client: Submitting application application_1607015337794_0072 to ResourceManager
20/12/04 16:08:44 INFO YarnClientImpl: Submitted application application_1607015337794_0072
20/12/04 16:08:45 INFO Client: Application report for application_1607015337794_0072 (state: ACCEPTED)
20/12/04 16:08:45 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607116124124
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0072/
	 user: bsuconn
20/12/04 16:08:46 INFO Client: Application report for application_1607015337794_0072 (state: ACCEPTED)
20/12/04 16:08:47 INFO Client: Application report for application_1607015337794_0072 (state: ACCEPTED)
20/12/04 16:08:48 INFO Client: Application report for application_1607015337794_0072 (state: ACCEPTED)
20/12/04 16:08:49 INFO Client: Application report for application_1607015337794_0072 (state: RUNNING)
20/12/04 16:08:49 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607116124124
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0072/
	 user: bsuconn
20/12/04 16:08:49 INFO YarnClientSchedulerBackend: Application application_1607015337794_0072 has started running.
20/12/04 16:08:49 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0072), /proxy/application_1607015337794_0072
20/12/04 16:08:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38329.
20/12/04 16:08:49 INFO NettyBlockTransferService: Server created on 192.168.1.9:38329
20/12/04 16:08:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 16:08:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 38329, None)
20/12/04 16:08:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:38329 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 38329, None)
20/12/04 16:08:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 38329, None)
20/12/04 16:08:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 38329, None)
20/12/04 16:08:49 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:08:50 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 16:08:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 16:08:55 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:44364) with ID 1
20/12/04 16:08:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:35999 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 35999, None)
20/12/04 16:08:56 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:44368) with ID 2
20/12/04 16:08:56 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 16:08:56 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:44455 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 44455, None)
20/12/04 16:08:56 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 16:08:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 16:08:56 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 16:08:56 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:08:56 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:08:56 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:08:56 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:08:56 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:08:57 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:37711/files/darima.zip with timestamp 1607116137050
20/12/04 16:08:57 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/userFiles-b523a33b-b335-47cf-aff7-b0e4e0730499/darima.zip
20/12/04 16:08:59 INFO InMemoryFileIndex: It took 66 ms to list leaf files for 1 paths.
20/12/04 16:09:01 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/12/04 16:09:01 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:09:01 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:09:01 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:09:01 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:09:02 INFO CodeGenerator: Code generated in 373.171445 ms
20/12/04 16:09:02 INFO CodeGenerator: Code generated in 35.682225 ms
20/12/04 16:09:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:09:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:09:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:38329 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:09:02 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 16:09:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:09:02 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 16:09:02 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 16:09:02 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 16:09:02 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 16:09:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 16:09:02 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 16:09:02 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:09:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 16:09:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 16:09:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:38329 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:09:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 16:09:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:09:03 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 16:09:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:09:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:35999 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:09:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:35999 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:09:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3565 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:09:06 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 16:09:06 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.723 s
20/12/04 16:09:06 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:09:06 INFO DAGScheduler: running: Set()
20/12/04 16:09:06 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 16:09:06 INFO DAGScheduler: failed: Set()
20/12/04 16:09:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:09:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 16:09:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 16:09:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:38329 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:09:06 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 16:09:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:09:06 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 16:09:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:44455 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:09:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:44368
20/12/04 16:09:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2074 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 16:09:08 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 16:09:08 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 2.092 s
20/12/04 16:09:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:09:08 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 16:09:08 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 5.932055 s
20/12/04 16:09:11 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:35999 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:09:11 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:38329 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:09:11 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:38329 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:09:11 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:44455 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:09:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 16:09:12 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:09:12 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:09:12 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:09:12 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:09:13 INFO CodeGenerator: Code generated in 31.868682 ms
20/12/04 16:09:13 INFO CodeGenerator: Code generated in 15.958319 ms
20/12/04 16:09:13 INFO CodeGenerator: Code generated in 18.949008 ms
20/12/04 16:09:13 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:09:13 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:09:13 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:38329 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:09:13 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/userFiles-b523a33b-b335-47cf-aff7-b0e4e0730499/darima.zip/darima/dlsa.py:22
20/12/04 16:09:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:09:13 INFO SparkContext: Starting job: toPandas at /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/userFiles-b523a33b-b335-47cf-aff7-b0e4e0730499/darima.zip/darima/dlsa.py:22
20/12/04 16:09:13 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/userFiles-b523a33b-b335-47cf-aff7-b0e4e0730499/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 16:09:13 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/userFiles-b523a33b-b335-47cf-aff7-b0e4e0730499/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 16:09:13 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/userFiles-b523a33b-b335-47cf-aff7-b0e4e0730499/darima.zip/darima/dlsa.py:22)
20/12/04 16:09:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 16:09:13 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 16:09:13 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/userFiles-b523a33b-b335-47cf-aff7-b0e4e0730499/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:09:13 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 16:09:13 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 16:09:13 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:38329 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 16:09:13 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 16:09:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/userFiles-b523a33b-b335-47cf-aff7-b0e4e0730499/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 16:09:13 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 16:09:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:09:13 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:35999 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 16:09:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:35999 (size: 28.7 KiB, free: 912.2 MiB)
20/12/04 16:09:14 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1459 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:09:14 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 16:09:14 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 38999
20/12/04 16:09:14 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/userFiles-b523a33b-b335-47cf-aff7-b0e4e0730499/darima.zip/darima/dlsa.py:22) finished in 1.482 s
20/12/04 16:09:14 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:09:14 INFO DAGScheduler: running: Set()
20/12/04 16:09:14 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 16:09:14 INFO DAGScheduler: failed: Set()
20/12/04 16:09:14 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/userFiles-b523a33b-b335-47cf-aff7-b0e4e0730499/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:09:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 16:09:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 16:09:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:38329 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 16:09:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 16:09:15 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/userFiles-b523a33b-b335-47cf-aff7-b0e4e0730499/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 16:09:15 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 16:09:15 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:15 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:44455 (size: 131.5 KiB, free: 912.2 MiB)
20/12/04 16:09:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:35999 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 16:09:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:44368
20/12/04 16:09:16 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:44364
20/12/04 16:09:24 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:24 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 9846 ms on 192.168.1.9 (executor 1) (1/200)
20/12/04 16:09:25 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 6, 192.168.1.9, executor 2, partition 4, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:25 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 4) in 10962 ms on 192.168.1.9 (executor 2) (2/200)
20/12/04 16:09:27 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 7, 192.168.1.9, executor 2, partition 5, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:27 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 6) in 1570 ms on 192.168.1.9 (executor 2) (3/200)
20/12/04 16:09:28 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 6, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:28 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 5) in 3733 ms on 192.168.1.9 (executor 1) (4/200)
20/12/04 16:09:28 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 9, 192.168.1.9, executor 2, partition 7, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:28 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 7) in 1406 ms on 192.168.1.9 (executor 2) (5/200)
20/12/04 16:09:30 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 10, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:30 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 8) in 2312 ms on 192.168.1.9 (executor 1) (6/200)
20/12/04 16:09:31 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 11, 192.168.1.9, executor 2, partition 11, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:31 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 9) in 2814 ms on 192.168.1.9 (executor 2) (7/200)
20/12/04 16:09:33 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 12, 192.168.1.9, executor 2, partition 12, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:33 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 11) in 1466 ms on 192.168.1.9 (executor 2) (8/200)
20/12/04 16:09:33 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 13, 192.168.1.9, executor 1, partition 13, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:33 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 10) in 2544 ms on 192.168.1.9 (executor 1) (9/200)
20/12/04 16:09:35 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 14, 192.168.1.9, executor 1, partition 14, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:35 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 13) in 1706 ms on 192.168.1.9 (executor 1) (10/200)
20/12/04 16:09:37 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 15, 192.168.1.9, executor 1, partition 18, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:37 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 14) in 2228 ms on 192.168.1.9 (executor 1) (11/200)
20/12/04 16:09:37 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:35999 in memory (size: 11.6 KiB, free: 912.1 MiB)
20/12/04 16:09:37 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:38329 in memory (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 16:09:39 INFO TaskSetManager: Starting task 19.0 in stage 3.0 (TID 16, 192.168.1.9, executor 1, partition 19, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:39 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 15) in 2622 ms on 192.168.1.9 (executor 1) (12/200)
20/12/04 16:09:40 INFO TaskSetManager: Starting task 21.0 in stage 3.0 (TID 17, 192.168.1.9, executor 2, partition 21, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:40 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 12) in 6925 ms on 192.168.1.9 (executor 2) (13/200)
20/12/04 16:09:43 INFO TaskSetManager: Finished task 21.0 in stage 3.0 (TID 17) in 3680 ms on 192.168.1.9 (executor 2) (14/200)
20/12/04 16:09:43 INFO TaskSetManager: Starting task 23.0 in stage 3.0 (TID 18, 192.168.1.9, executor 2, partition 23, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:44 INFO TaskSetManager: Starting task 24.0 in stage 3.0 (TID 19, 192.168.1.9, executor 1, partition 24, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:44 INFO TaskSetManager: Finished task 19.0 in stage 3.0 (TID 16) in 5012 ms on 192.168.1.9 (executor 1) (15/200)
20/12/04 16:09:46 INFO TaskSetManager: Starting task 27.0 in stage 3.0 (TID 20, 192.168.1.9, executor 2, partition 27, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:46 INFO TaskSetManager: Finished task 23.0 in stage 3.0 (TID 18) in 2760 ms on 192.168.1.9 (executor 2) (16/200)
20/12/04 16:09:47 INFO TaskSetManager: Starting task 30.0 in stage 3.0 (TID 21, 192.168.1.9, executor 1, partition 30, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:47 INFO TaskSetManager: Finished task 24.0 in stage 3.0 (TID 19) in 2268 ms on 192.168.1.9 (executor 1) (17/200)
20/12/04 16:09:48 INFO TaskSetManager: Starting task 31.0 in stage 3.0 (TID 22, 192.168.1.9, executor 2, partition 31, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:48 INFO TaskSetManager: Finished task 27.0 in stage 3.0 (TID 20) in 1698 ms on 192.168.1.9 (executor 2) (18/200)
20/12/04 16:09:50 INFO TaskSetManager: Starting task 32.0 in stage 3.0 (TID 23, 192.168.1.9, executor 1, partition 32, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:50 INFO TaskSetManager: Finished task 30.0 in stage 3.0 (TID 21) in 2886 ms on 192.168.1.9 (executor 1) (19/200)
20/12/04 16:09:51 INFO TaskSetManager: Starting task 34.0 in stage 3.0 (TID 24, 192.168.1.9, executor 2, partition 34, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:51 INFO TaskSetManager: Finished task 31.0 in stage 3.0 (TID 22) in 3058 ms on 192.168.1.9 (executor 2) (20/200)
20/12/04 16:09:52 INFO TaskSetManager: Starting task 35.0 in stage 3.0 (TID 25, 192.168.1.9, executor 1, partition 35, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:52 INFO TaskSetManager: Finished task 32.0 in stage 3.0 (TID 23) in 2378 ms on 192.168.1.9 (executor 1) (21/200)
20/12/04 16:09:52 INFO TaskSetManager: Starting task 36.0 in stage 3.0 (TID 26, 192.168.1.9, executor 2, partition 36, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:52 INFO TaskSetManager: Finished task 34.0 in stage 3.0 (TID 24) in 1541 ms on 192.168.1.9 (executor 2) (22/200)
20/12/04 16:09:54 INFO TaskSetManager: Starting task 37.0 in stage 3.0 (TID 27, 192.168.1.9, executor 2, partition 37, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:54 INFO TaskSetManager: Finished task 36.0 in stage 3.0 (TID 26) in 1639 ms on 192.168.1.9 (executor 2) (23/200)
20/12/04 16:09:54 INFO TaskSetManager: Starting task 41.0 in stage 3.0 (TID 28, 192.168.1.9, executor 1, partition 41, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:54 INFO TaskSetManager: Finished task 35.0 in stage 3.0 (TID 25) in 2142 ms on 192.168.1.9 (executor 1) (24/200)
20/12/04 16:09:56 INFO TaskSetManager: Starting task 43.0 in stage 3.0 (TID 29, 192.168.1.9, executor 2, partition 43, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:56 INFO TaskSetManager: Finished task 37.0 in stage 3.0 (TID 27) in 1543 ms on 192.168.1.9 (executor 2) (25/200)
20/12/04 16:09:56 INFO TaskSetManager: Starting task 44.0 in stage 3.0 (TID 30, 192.168.1.9, executor 1, partition 44, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:56 INFO TaskSetManager: Finished task 41.0 in stage 3.0 (TID 28) in 1547 ms on 192.168.1.9 (executor 1) (26/200)
20/12/04 16:09:57 INFO TaskSetManager: Starting task 48.0 in stage 3.0 (TID 31, 192.168.1.9, executor 1, partition 48, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:57 INFO TaskSetManager: Finished task 44.0 in stage 3.0 (TID 30) in 1666 ms on 192.168.1.9 (executor 1) (27/200)
20/12/04 16:09:58 INFO TaskSetManager: Starting task 49.0 in stage 3.0 (TID 32, 192.168.1.9, executor 2, partition 49, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:58 INFO TaskSetManager: Finished task 43.0 in stage 3.0 (TID 29) in 2277 ms on 192.168.1.9 (executor 2) (28/200)
20/12/04 16:09:59 INFO TaskSetManager: Starting task 51.0 in stage 3.0 (TID 33, 192.168.1.9, executor 1, partition 51, NODE_LOCAL, 7336 bytes)
20/12/04 16:09:59 INFO TaskSetManager: Finished task 48.0 in stage 3.0 (TID 31) in 1221 ms on 192.168.1.9 (executor 1) (29/200)
20/12/04 16:10:01 INFO TaskSetManager: Starting task 53.0 in stage 3.0 (TID 34, 192.168.1.9, executor 2, partition 53, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:01 INFO TaskSetManager: Finished task 49.0 in stage 3.0 (TID 32) in 2722 ms on 192.168.1.9 (executor 2) (30/200)
20/12/04 16:10:01 INFO TaskSetManager: Starting task 55.0 in stage 3.0 (TID 35, 192.168.1.9, executor 1, partition 55, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:01 INFO TaskSetManager: Finished task 51.0 in stage 3.0 (TID 33) in 2855 ms on 192.168.1.9 (executor 1) (31/200)
20/12/04 16:10:03 INFO TaskSetManager: Starting task 57.0 in stage 3.0 (TID 36, 192.168.1.9, executor 1, partition 57, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:03 INFO TaskSetManager: Finished task 55.0 in stage 3.0 (TID 35) in 1155 ms on 192.168.1.9 (executor 1) (32/200)
20/12/04 16:10:03 INFO TaskSetManager: Starting task 58.0 in stage 3.0 (TID 37, 192.168.1.9, executor 2, partition 58, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:03 INFO TaskSetManager: Finished task 53.0 in stage 3.0 (TID 34) in 2704 ms on 192.168.1.9 (executor 2) (33/200)
20/12/04 16:10:04 INFO TaskSetManager: Starting task 61.0 in stage 3.0 (TID 38, 192.168.1.9, executor 1, partition 61, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:04 INFO TaskSetManager: Finished task 57.0 in stage 3.0 (TID 36) in 1348 ms on 192.168.1.9 (executor 1) (34/200)
20/12/04 16:10:04 INFO TaskSetManager: Starting task 63.0 in stage 3.0 (TID 39, 192.168.1.9, executor 2, partition 63, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:04 INFO TaskSetManager: Finished task 58.0 in stage 3.0 (TID 37) in 1176 ms on 192.168.1.9 (executor 2) (35/200)
20/12/04 16:10:06 INFO TaskSetManager: Starting task 65.0 in stage 3.0 (TID 40, 192.168.1.9, executor 2, partition 65, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:06 INFO TaskSetManager: Finished task 63.0 in stage 3.0 (TID 39) in 1294 ms on 192.168.1.9 (executor 2) (36/200)
20/12/04 16:10:07 INFO TaskSetManager: Starting task 66.0 in stage 3.0 (TID 41, 192.168.1.9, executor 2, partition 66, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:07 INFO TaskSetManager: Finished task 65.0 in stage 3.0 (TID 40) in 1013 ms on 192.168.1.9 (executor 2) (37/200)
20/12/04 16:10:07 INFO TaskSetManager: Starting task 69.0 in stage 3.0 (TID 42, 192.168.1.9, executor 1, partition 69, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:07 INFO TaskSetManager: Finished task 61.0 in stage 3.0 (TID 38) in 2866 ms on 192.168.1.9 (executor 1) (38/200)
20/12/04 16:10:08 INFO TaskSetManager: Starting task 70.0 in stage 3.0 (TID 43, 192.168.1.9, executor 2, partition 70, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:08 INFO TaskSetManager: Finished task 66.0 in stage 3.0 (TID 41) in 988 ms on 192.168.1.9 (executor 2) (39/200)
20/12/04 16:10:08 INFO TaskSetManager: Starting task 73.0 in stage 3.0 (TID 44, 192.168.1.9, executor 1, partition 73, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:08 INFO TaskSetManager: Finished task 69.0 in stage 3.0 (TID 42) in 1452 ms on 192.168.1.9 (executor 1) (40/200)
20/12/04 16:10:09 INFO TaskSetManager: Starting task 75.0 in stage 3.0 (TID 45, 192.168.1.9, executor 2, partition 75, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:09 INFO TaskSetManager: Finished task 70.0 in stage 3.0 (TID 43) in 1350 ms on 192.168.1.9 (executor 2) (41/200)
20/12/04 16:10:10 INFO TaskSetManager: Starting task 77.0 in stage 3.0 (TID 46, 192.168.1.9, executor 1, partition 77, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:10 INFO TaskSetManager: Finished task 73.0 in stage 3.0 (TID 44) in 1439 ms on 192.168.1.9 (executor 1) (42/200)
20/12/04 16:10:11 INFO TaskSetManager: Starting task 79.0 in stage 3.0 (TID 47, 192.168.1.9, executor 2, partition 79, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:11 INFO TaskSetManager: Finished task 75.0 in stage 3.0 (TID 45) in 1977 ms on 192.168.1.9 (executor 2) (43/200)
20/12/04 16:10:13 INFO TaskSetManager: Starting task 84.0 in stage 3.0 (TID 48, 192.168.1.9, executor 1, partition 84, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:13 INFO TaskSetManager: Finished task 77.0 in stage 3.0 (TID 46) in 2907 ms on 192.168.1.9 (executor 1) (44/200)
20/12/04 16:10:13 INFO TaskSetManager: Starting task 85.0 in stage 3.0 (TID 49, 192.168.1.9, executor 1, partition 85, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:13 INFO TaskSetManager: Finished task 84.0 in stage 3.0 (TID 48) in 584 ms on 192.168.1.9 (executor 1) (45/200)
20/12/04 16:10:13 INFO TaskSetManager: Starting task 86.0 in stage 3.0 (TID 50, 192.168.1.9, executor 2, partition 86, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:13 INFO TaskSetManager: Finished task 79.0 in stage 3.0 (TID 47) in 2246 ms on 192.168.1.9 (executor 2) (46/200)
20/12/04 16:10:15 INFO TaskSetManager: Starting task 88.0 in stage 3.0 (TID 51, 192.168.1.9, executor 2, partition 88, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:15 INFO TaskSetManager: Finished task 86.0 in stage 3.0 (TID 50) in 1416 ms on 192.168.1.9 (executor 2) (47/200)
20/12/04 16:10:15 INFO TaskSetManager: Starting task 89.0 in stage 3.0 (TID 52, 192.168.1.9, executor 2, partition 89, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:15 INFO TaskSetManager: Finished task 88.0 in stage 3.0 (TID 51) in 793 ms on 192.168.1.9 (executor 2) (48/200)
20/12/04 16:10:16 INFO TaskSetManager: Starting task 91.0 in stage 3.0 (TID 53, 192.168.1.9, executor 1, partition 91, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:16 INFO TaskSetManager: Finished task 85.0 in stage 3.0 (TID 49) in 2683 ms on 192.168.1.9 (executor 1) (49/200)
20/12/04 16:10:17 INFO TaskSetManager: Finished task 91.0 in stage 3.0 (TID 53) in 1409 ms on 192.168.1.9 (executor 1) (50/200)
20/12/04 16:10:17 INFO TaskSetManager: Starting task 95.0 in stage 3.0 (TID 54, 192.168.1.9, executor 1, partition 95, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:18 INFO TaskSetManager: Starting task 101.0 in stage 3.0 (TID 55, 192.168.1.9, executor 2, partition 101, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:18 INFO TaskSetManager: Finished task 89.0 in stage 3.0 (TID 52) in 2359 ms on 192.168.1.9 (executor 2) (51/200)
20/12/04 16:10:19 INFO TaskSetManager: Starting task 102.0 in stage 3.0 (TID 56, 192.168.1.9, executor 2, partition 102, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:19 INFO TaskSetManager: Finished task 101.0 in stage 3.0 (TID 55) in 1105 ms on 192.168.1.9 (executor 2) (52/200)
20/12/04 16:10:20 INFO TaskSetManager: Starting task 103.0 in stage 3.0 (TID 57, 192.168.1.9, executor 1, partition 103, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:20 INFO TaskSetManager: Finished task 95.0 in stage 3.0 (TID 54) in 2398 ms on 192.168.1.9 (executor 1) (53/200)
20/12/04 16:10:22 INFO TaskSetManager: Starting task 105.0 in stage 3.0 (TID 58, 192.168.1.9, executor 1, partition 105, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:22 INFO TaskSetManager: Finished task 103.0 in stage 3.0 (TID 57) in 2012 ms on 192.168.1.9 (executor 1) (54/200)
20/12/04 16:10:23 INFO TaskSetManager: Starting task 106.0 in stage 3.0 (TID 59, 192.168.1.9, executor 2, partition 106, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:23 INFO TaskSetManager: Finished task 102.0 in stage 3.0 (TID 56) in 3732 ms on 192.168.1.9 (executor 2) (55/200)
20/12/04 16:10:23 INFO TaskSetManager: Starting task 107.0 in stage 3.0 (TID 60, 192.168.1.9, executor 2, partition 107, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:23 INFO TaskSetManager: Finished task 106.0 in stage 3.0 (TID 59) in 684 ms on 192.168.1.9 (executor 2) (56/200)
20/12/04 16:10:24 INFO TaskSetManager: Starting task 108.0 in stage 3.0 (TID 61, 192.168.1.9, executor 2, partition 108, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:24 INFO TaskSetManager: Finished task 107.0 in stage 3.0 (TID 60) in 844 ms on 192.168.1.9 (executor 2) (57/200)
20/12/04 16:10:24 INFO TaskSetManager: Starting task 109.0 in stage 3.0 (TID 62, 192.168.1.9, executor 1, partition 109, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:24 INFO TaskSetManager: Finished task 105.0 in stage 3.0 (TID 58) in 2619 ms on 192.168.1.9 (executor 1) (58/200)
20/12/04 16:10:26 INFO TaskSetManager: Starting task 111.0 in stage 3.0 (TID 63, 192.168.1.9, executor 2, partition 111, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:26 INFO TaskSetManager: Finished task 108.0 in stage 3.0 (TID 61) in 1314 ms on 192.168.1.9 (executor 2) (59/200)
20/12/04 16:10:27 INFO TaskSetManager: Starting task 113.0 in stage 3.0 (TID 64, 192.168.1.9, executor 1, partition 113, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:27 INFO TaskSetManager: Finished task 109.0 in stage 3.0 (TID 62) in 2940 ms on 192.168.1.9 (executor 1) (60/200)
20/12/04 16:10:27 INFO TaskSetManager: Starting task 115.0 in stage 3.0 (TID 65, 192.168.1.9, executor 2, partition 115, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:27 INFO TaskSetManager: Finished task 111.0 in stage 3.0 (TID 63) in 1903 ms on 192.168.1.9 (executor 2) (61/200)
20/12/04 16:10:29 INFO TaskSetManager: Starting task 116.0 in stage 3.0 (TID 66, 192.168.1.9, executor 1, partition 116, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:29 INFO TaskSetManager: Finished task 113.0 in stage 3.0 (TID 64) in 1306 ms on 192.168.1.9 (executor 1) (62/200)
20/12/04 16:10:29 INFO TaskSetManager: Starting task 119.0 in stage 3.0 (TID 67, 192.168.1.9, executor 1, partition 119, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:29 INFO TaskSetManager: Finished task 116.0 in stage 3.0 (TID 66) in 721 ms on 192.168.1.9 (executor 1) (63/200)
20/12/04 16:10:29 INFO TaskSetManager: Starting task 122.0 in stage 3.0 (TID 68, 192.168.1.9, executor 2, partition 122, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:29 INFO TaskSetManager: Finished task 115.0 in stage 3.0 (TID 65) in 1935 ms on 192.168.1.9 (executor 2) (64/200)
20/12/04 16:10:31 INFO TaskSetManager: Starting task 124.0 in stage 3.0 (TID 69, 192.168.1.9, executor 1, partition 124, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:31 INFO TaskSetManager: Finished task 119.0 in stage 3.0 (TID 67) in 1441 ms on 192.168.1.9 (executor 1) (65/200)
20/12/04 16:10:31 INFO TaskSetManager: Starting task 126.0 in stage 3.0 (TID 70, 192.168.1.9, executor 1, partition 126, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:31 INFO TaskSetManager: Finished task 124.0 in stage 3.0 (TID 69) in 801 ms on 192.168.1.9 (executor 1) (66/200)
20/12/04 16:10:32 INFO TaskSetManager: Starting task 128.0 in stage 3.0 (TID 71, 192.168.1.9, executor 2, partition 128, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:32 INFO TaskSetManager: Finished task 122.0 in stage 3.0 (TID 68) in 2829 ms on 192.168.1.9 (executor 2) (67/200)
20/12/04 16:10:33 INFO TaskSetManager: Starting task 129.0 in stage 3.0 (TID 72, 192.168.1.9, executor 1, partition 129, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:33 INFO TaskSetManager: Finished task 126.0 in stage 3.0 (TID 70) in 1109 ms on 192.168.1.9 (executor 1) (68/200)
20/12/04 16:10:33 INFO TaskSetManager: Starting task 131.0 in stage 3.0 (TID 73, 192.168.1.9, executor 2, partition 131, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:33 INFO TaskSetManager: Finished task 128.0 in stage 3.0 (TID 71) in 1040 ms on 192.168.1.9 (executor 2) (69/200)
20/12/04 16:10:34 INFO TaskSetManager: Starting task 132.0 in stage 3.0 (TID 74, 192.168.1.9, executor 1, partition 132, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:34 INFO TaskSetManager: Finished task 129.0 in stage 3.0 (TID 72) in 1358 ms on 192.168.1.9 (executor 1) (70/200)
20/12/04 16:10:34 INFO TaskSetManager: Starting task 133.0 in stage 3.0 (TID 75, 192.168.1.9, executor 2, partition 133, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:34 INFO TaskSetManager: Finished task 131.0 in stage 3.0 (TID 73) in 858 ms on 192.168.1.9 (executor 2) (71/200)
20/12/04 16:10:36 INFO TaskSetManager: Starting task 135.0 in stage 3.0 (TID 76, 192.168.1.9, executor 2, partition 135, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:36 INFO TaskSetManager: Finished task 133.0 in stage 3.0 (TID 75) in 1508 ms on 192.168.1.9 (executor 2) (72/200)
20/12/04 16:10:36 INFO TaskSetManager: Starting task 136.0 in stage 3.0 (TID 77, 192.168.1.9, executor 1, partition 136, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:36 INFO TaskSetManager: Finished task 132.0 in stage 3.0 (TID 74) in 2221 ms on 192.168.1.9 (executor 1) (73/200)
20/12/04 16:10:37 INFO TaskSetManager: Starting task 137.0 in stage 3.0 (TID 78, 192.168.1.9, executor 1, partition 137, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:37 INFO TaskSetManager: Finished task 136.0 in stage 3.0 (TID 77) in 1027 ms on 192.168.1.9 (executor 1) (74/200)
20/12/04 16:10:39 INFO TaskSetManager: Starting task 139.0 in stage 3.0 (TID 79, 192.168.1.9, executor 2, partition 139, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:39 INFO TaskSetManager: Finished task 135.0 in stage 3.0 (TID 76) in 3062 ms on 192.168.1.9 (executor 2) (75/200)
20/12/04 16:10:40 INFO TaskSetManager: Starting task 140.0 in stage 3.0 (TID 80, 192.168.1.9, executor 2, partition 140, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:40 INFO TaskSetManager: Finished task 139.0 in stage 3.0 (TID 79) in 915 ms on 192.168.1.9 (executor 2) (76/200)
20/12/04 16:10:41 INFO TaskSetManager: Starting task 141.0 in stage 3.0 (TID 81, 192.168.1.9, executor 1, partition 141, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:41 INFO TaskSetManager: Finished task 137.0 in stage 3.0 (TID 78) in 3408 ms on 192.168.1.9 (executor 1) (77/200)
20/12/04 16:10:41 INFO TaskSetManager: Starting task 143.0 in stage 3.0 (TID 82, 192.168.1.9, executor 1, partition 143, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:41 INFO TaskSetManager: Finished task 141.0 in stage 3.0 (TID 81) in 717 ms on 192.168.1.9 (executor 1) (78/200)
20/12/04 16:10:41 INFO TaskSetManager: Starting task 146.0 in stage 3.0 (TID 83, 192.168.1.9, executor 2, partition 146, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:41 INFO TaskSetManager: Finished task 140.0 in stage 3.0 (TID 80) in 1778 ms on 192.168.1.9 (executor 2) (79/200)
20/12/04 16:10:42 INFO TaskSetManager: Starting task 150.0 in stage 3.0 (TID 84, 192.168.1.9, executor 1, partition 150, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:42 INFO TaskSetManager: Finished task 143.0 in stage 3.0 (TID 82) in 1172 ms on 192.168.1.9 (executor 1) (80/200)
20/12/04 16:10:43 INFO TaskSetManager: Starting task 151.0 in stage 3.0 (TID 85, 192.168.1.9, executor 2, partition 151, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:43 INFO TaskSetManager: Finished task 146.0 in stage 3.0 (TID 83) in 1676 ms on 192.168.1.9 (executor 2) (81/200)
20/12/04 16:10:45 INFO TaskSetManager: Starting task 153.0 in stage 3.0 (TID 86, 192.168.1.9, executor 2, partition 153, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:45 INFO TaskSetManager: Finished task 151.0 in stage 3.0 (TID 85) in 1849 ms on 192.168.1.9 (executor 2) (82/200)
20/12/04 16:10:46 INFO TaskSetManager: Starting task 155.0 in stage 3.0 (TID 87, 192.168.1.9, executor 1, partition 155, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:46 INFO TaskSetManager: Finished task 150.0 in stage 3.0 (TID 84) in 3969 ms on 192.168.1.9 (executor 1) (83/200)
20/12/04 16:10:47 INFO TaskSetManager: Finished task 153.0 in stage 3.0 (TID 86) in 2040 ms on 192.168.1.9 (executor 2) (84/200)
20/12/04 16:10:47 INFO TaskSetManager: Starting task 156.0 in stage 3.0 (TID 88, 192.168.1.9, executor 2, partition 156, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:48 INFO TaskSetManager: Starting task 161.0 in stage 3.0 (TID 89, 192.168.1.9, executor 1, partition 161, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:48 INFO TaskSetManager: Finished task 155.0 in stage 3.0 (TID 87) in 1127 ms on 192.168.1.9 (executor 1) (85/200)
20/12/04 16:10:49 INFO TaskSetManager: Starting task 162.0 in stage 3.0 (TID 90, 192.168.1.9, executor 2, partition 162, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:49 INFO TaskSetManager: Finished task 156.0 in stage 3.0 (TID 88) in 2215 ms on 192.168.1.9 (executor 2) (86/200)
20/12/04 16:10:49 INFO TaskSetManager: Starting task 163.0 in stage 3.0 (TID 91, 192.168.1.9, executor 1, partition 163, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:49 INFO TaskSetManager: Finished task 161.0 in stage 3.0 (TID 89) in 1714 ms on 192.168.1.9 (executor 1) (87/200)
20/12/04 16:10:51 INFO TaskSetManager: Starting task 164.0 in stage 3.0 (TID 92, 192.168.1.9, executor 2, partition 164, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:51 INFO TaskSetManager: Finished task 162.0 in stage 3.0 (TID 90) in 2350 ms on 192.168.1.9 (executor 2) (88/200)
20/12/04 16:10:52 INFO TaskSetManager: Starting task 165.0 in stage 3.0 (TID 93, 192.168.1.9, executor 1, partition 165, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:52 INFO TaskSetManager: Finished task 163.0 in stage 3.0 (TID 91) in 2804 ms on 192.168.1.9 (executor 1) (89/200)
20/12/04 16:10:54 INFO TaskSetManager: Starting task 167.0 in stage 3.0 (TID 94, 192.168.1.9, executor 1, partition 167, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:54 INFO TaskSetManager: Finished task 165.0 in stage 3.0 (TID 93) in 2330 ms on 192.168.1.9 (executor 1) (90/200)
20/12/04 16:10:56 INFO TaskSetManager: Starting task 168.0 in stage 3.0 (TID 95, 192.168.1.9, executor 2, partition 168, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:56 INFO TaskSetManager: Finished task 164.0 in stage 3.0 (TID 92) in 4894 ms on 192.168.1.9 (executor 2) (91/200)
20/12/04 16:10:57 INFO TaskSetManager: Starting task 170.0 in stage 3.0 (TID 96, 192.168.1.9, executor 1, partition 170, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:57 INFO TaskSetManager: Finished task 167.0 in stage 3.0 (TID 94) in 2696 ms on 192.168.1.9 (executor 1) (92/200)
20/12/04 16:10:57 INFO TaskSetManager: Finished task 168.0 in stage 3.0 (TID 95) in 990 ms on 192.168.1.9 (executor 2) (93/200)
20/12/04 16:10:57 INFO TaskSetManager: Starting task 172.0 in stage 3.0 (TID 97, 192.168.1.9, executor 2, partition 172, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:59 INFO TaskSetManager: Starting task 173.0 in stage 3.0 (TID 98, 192.168.1.9, executor 1, partition 173, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:59 INFO TaskSetManager: Finished task 170.0 in stage 3.0 (TID 96) in 1527 ms on 192.168.1.9 (executor 1) (94/200)
20/12/04 16:10:59 INFO TaskSetManager: Starting task 174.0 in stage 3.0 (TID 99, 192.168.1.9, executor 2, partition 174, NODE_LOCAL, 7336 bytes)
20/12/04 16:10:59 INFO TaskSetManager: Finished task 172.0 in stage 3.0 (TID 97) in 1460 ms on 192.168.1.9 (executor 2) (95/200)
20/12/04 16:11:01 INFO TaskSetManager: Starting task 175.0 in stage 3.0 (TID 100, 192.168.1.9, executor 2, partition 175, NODE_LOCAL, 7336 bytes)
20/12/04 16:11:01 INFO TaskSetManager: Finished task 174.0 in stage 3.0 (TID 99) in 2284 ms on 192.168.1.9 (executor 2) (96/200)
20/12/04 16:11:02 INFO TaskSetManager: Starting task 178.0 in stage 3.0 (TID 101, 192.168.1.9, executor 1, partition 178, NODE_LOCAL, 7336 bytes)
20/12/04 16:11:02 INFO TaskSetManager: Finished task 173.0 in stage 3.0 (TID 98) in 2902 ms on 192.168.1.9 (executor 1) (97/200)
20/12/04 16:11:02 INFO TaskSetManager: Finished task 178.0 in stage 3.0 (TID 101) in 946 ms on 192.168.1.9 (executor 1) (98/200)
20/12/04 16:11:02 INFO TaskSetManager: Starting task 179.0 in stage 3.0 (TID 102, 192.168.1.9, executor 1, partition 179, NODE_LOCAL, 7336 bytes)
20/12/04 16:11:03 INFO TaskSetManager: Starting task 181.0 in stage 3.0 (TID 103, 192.168.1.9, executor 2, partition 181, NODE_LOCAL, 7336 bytes)
20/12/04 16:11:03 INFO TaskSetManager: Finished task 175.0 in stage 3.0 (TID 100) in 2361 ms on 192.168.1.9 (executor 2) (99/200)
20/12/04 16:11:04 INFO TaskSetManager: Starting task 183.0 in stage 3.0 (TID 104, 192.168.1.9, executor 1, partition 183, NODE_LOCAL, 7336 bytes)
20/12/04 16:11:04 INFO TaskSetManager: Finished task 179.0 in stage 3.0 (TID 102) in 1552 ms on 192.168.1.9 (executor 1) (100/200)
20/12/04 16:11:05 INFO TaskSetManager: Starting task 184.0 in stage 3.0 (TID 105, 192.168.1.9, executor 1, partition 184, NODE_LOCAL, 7336 bytes)
20/12/04 16:11:05 INFO TaskSetManager: Finished task 183.0 in stage 3.0 (TID 104) in 950 ms on 192.168.1.9 (executor 1) (101/200)
20/12/04 16:11:06 INFO TaskSetManager: Starting task 190.0 in stage 3.0 (TID 106, 192.168.1.9, executor 2, partition 190, NODE_LOCAL, 7336 bytes)
20/12/04 16:11:06 INFO TaskSetManager: Finished task 181.0 in stage 3.0 (TID 103) in 3009 ms on 192.168.1.9 (executor 2) (102/200)
20/12/04 16:11:07 INFO TaskSetManager: Starting task 192.0 in stage 3.0 (TID 107, 192.168.1.9, executor 1, partition 192, NODE_LOCAL, 7336 bytes)
20/12/04 16:11:07 INFO TaskSetManager: Finished task 184.0 in stage 3.0 (TID 105) in 2421 ms on 192.168.1.9 (executor 1) (103/200)
20/12/04 16:11:08 INFO TaskSetManager: Starting task 193.0 in stage 3.0 (TID 108, 192.168.1.9, executor 2, partition 193, NODE_LOCAL, 7336 bytes)
20/12/04 16:11:08 INFO TaskSetManager: Finished task 190.0 in stage 3.0 (TID 106) in 1635 ms on 192.168.1.9 (executor 2) (104/200)
20/12/04 16:11:08 INFO TaskSetManager: Starting task 194.0 in stage 3.0 (TID 109, 192.168.1.9, executor 1, partition 194, NODE_LOCAL, 7336 bytes)
20/12/04 16:11:08 INFO TaskSetManager: Finished task 192.0 in stage 3.0 (TID 107) in 729 ms on 192.168.1.9 (executor 1) (105/200)
20/12/04 16:11:09 INFO TaskSetManager: Starting task 195.0 in stage 3.0 (TID 110, 192.168.1.9, executor 1, partition 195, NODE_LOCAL, 7336 bytes)
20/12/04 16:11:09 INFO TaskSetManager: Finished task 194.0 in stage 3.0 (TID 109) in 1119 ms on 192.168.1.9 (executor 1) (106/200)
20/12/04 16:11:10 INFO TaskSetManager: Starting task 198.0 in stage 3.0 (TID 111, 192.168.1.9, executor 1, partition 198, NODE_LOCAL, 7336 bytes)
20/12/04 16:11:10 INFO TaskSetManager: Finished task 195.0 in stage 3.0 (TID 110) in 1153 ms on 192.168.1.9 (executor 1) (107/200)
20/12/04 16:11:11 INFO TaskSetManager: Starting task 199.0 in stage 3.0 (TID 112, 192.168.1.9, executor 2, partition 199, NODE_LOCAL, 7336 bytes)
20/12/04 16:11:11 INFO TaskSetManager: Finished task 193.0 in stage 3.0 (TID 108) in 2431 ms on 192.168.1.9 (executor 2) (108/200)
20/12/04 16:11:11 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 113, 192.168.1.9, executor 1, partition 1, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:11 INFO TaskSetManager: Finished task 198.0 in stage 3.0 (TID 111) in 762 ms on 192.168.1.9 (executor 1) (109/200)
20/12/04 16:11:11 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 114, 192.168.1.9, executor 1, partition 8, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:11 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 113) in 147 ms on 192.168.1.9 (executor 1) (110/200)
20/12/04 16:11:11 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 115, 192.168.1.9, executor 1, partition 9, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:11 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 114) in 130 ms on 192.168.1.9 (executor 1) (111/200)
20/12/04 16:11:12 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 116, 192.168.1.9, executor 1, partition 15, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:12 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 115) in 166 ms on 192.168.1.9 (executor 1) (112/200)
20/12/04 16:11:12 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 117, 192.168.1.9, executor 1, partition 16, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:12 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 116) in 128 ms on 192.168.1.9 (executor 1) (113/200)
20/12/04 16:11:12 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 118, 192.168.1.9, executor 1, partition 17, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:12 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 117) in 135 ms on 192.168.1.9 (executor 1) (114/200)
20/12/04 16:11:12 INFO TaskSetManager: Starting task 20.0 in stage 3.0 (TID 119, 192.168.1.9, executor 1, partition 20, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:12 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 118) in 145 ms on 192.168.1.9 (executor 1) (115/200)
20/12/04 16:11:12 INFO TaskSetManager: Starting task 22.0 in stage 3.0 (TID 120, 192.168.1.9, executor 1, partition 22, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:12 INFO TaskSetManager: Finished task 20.0 in stage 3.0 (TID 119) in 189 ms on 192.168.1.9 (executor 1) (116/200)
20/12/04 16:11:12 INFO TaskSetManager: Starting task 25.0 in stage 3.0 (TID 121, 192.168.1.9, executor 2, partition 25, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:12 INFO TaskSetManager: Finished task 199.0 in stage 3.0 (TID 112) in 1702 ms on 192.168.1.9 (executor 2) (117/200)
20/12/04 16:11:12 INFO TaskSetManager: Starting task 26.0 in stage 3.0 (TID 122, 192.168.1.9, executor 1, partition 26, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:12 INFO TaskSetManager: Finished task 22.0 in stage 3.0 (TID 120) in 144 ms on 192.168.1.9 (executor 1) (118/200)
20/12/04 16:11:12 INFO TaskSetManager: Starting task 28.0 in stage 3.0 (TID 123, 192.168.1.9, executor 2, partition 28, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:12 INFO TaskSetManager: Finished task 25.0 in stage 3.0 (TID 121) in 143 ms on 192.168.1.9 (executor 2) (119/200)
20/12/04 16:11:12 INFO TaskSetManager: Starting task 29.0 in stage 3.0 (TID 124, 192.168.1.9, executor 1, partition 29, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:12 INFO TaskSetManager: Finished task 26.0 in stage 3.0 (TID 122) in 136 ms on 192.168.1.9 (executor 1) (120/200)
20/12/04 16:11:12 INFO TaskSetManager: Starting task 33.0 in stage 3.0 (TID 125, 192.168.1.9, executor 2, partition 33, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:12 INFO TaskSetManager: Finished task 28.0 in stage 3.0 (TID 123) in 136 ms on 192.168.1.9 (executor 2) (121/200)
20/12/04 16:11:13 INFO TaskSetManager: Starting task 38.0 in stage 3.0 (TID 126, 192.168.1.9, executor 1, partition 38, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:13 INFO TaskSetManager: Finished task 29.0 in stage 3.0 (TID 124) in 159 ms on 192.168.1.9 (executor 1) (122/200)
20/12/04 16:11:13 INFO TaskSetManager: Starting task 39.0 in stage 3.0 (TID 127, 192.168.1.9, executor 2, partition 39, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:13 INFO TaskSetManager: Finished task 33.0 in stage 3.0 (TID 125) in 143 ms on 192.168.1.9 (executor 2) (123/200)
20/12/04 16:11:13 INFO TaskSetManager: Starting task 40.0 in stage 3.0 (TID 128, 192.168.1.9, executor 1, partition 40, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:13 INFO TaskSetManager: Finished task 38.0 in stage 3.0 (TID 126) in 138 ms on 192.168.1.9 (executor 1) (124/200)
20/12/04 16:11:13 INFO TaskSetManager: Finished task 39.0 in stage 3.0 (TID 127) in 185 ms on 192.168.1.9 (executor 2) (125/200)
20/12/04 16:11:13 INFO TaskSetManager: Starting task 42.0 in stage 3.0 (TID 129, 192.168.1.9, executor 2, partition 42, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:13 INFO TaskSetManager: Starting task 45.0 in stage 3.0 (TID 130, 192.168.1.9, executor 1, partition 45, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:13 INFO TaskSetManager: Finished task 40.0 in stage 3.0 (TID 128) in 168 ms on 192.168.1.9 (executor 1) (126/200)
20/12/04 16:11:13 INFO TaskSetManager: Starting task 46.0 in stage 3.0 (TID 131, 192.168.1.9, executor 2, partition 46, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:13 INFO TaskSetManager: Finished task 42.0 in stage 3.0 (TID 129) in 175 ms on 192.168.1.9 (executor 2) (127/200)
20/12/04 16:11:13 INFO TaskSetManager: Starting task 47.0 in stage 3.0 (TID 132, 192.168.1.9, executor 1, partition 47, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:13 INFO TaskSetManager: Finished task 45.0 in stage 3.0 (TID 130) in 176 ms on 192.168.1.9 (executor 1) (128/200)
20/12/04 16:11:13 INFO TaskSetManager: Starting task 50.0 in stage 3.0 (TID 133, 192.168.1.9, executor 2, partition 50, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:13 INFO TaskSetManager: Finished task 46.0 in stage 3.0 (TID 131) in 177 ms on 192.168.1.9 (executor 2) (129/200)
20/12/04 16:11:13 INFO TaskSetManager: Starting task 52.0 in stage 3.0 (TID 134, 192.168.1.9, executor 1, partition 52, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:13 INFO TaskSetManager: Finished task 47.0 in stage 3.0 (TID 132) in 127 ms on 192.168.1.9 (executor 1) (130/200)
20/12/04 16:11:13 INFO TaskSetManager: Starting task 54.0 in stage 3.0 (TID 135, 192.168.1.9, executor 2, partition 54, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:13 INFO TaskSetManager: Finished task 50.0 in stage 3.0 (TID 133) in 148 ms on 192.168.1.9 (executor 2) (131/200)
20/12/04 16:11:13 INFO TaskSetManager: Starting task 56.0 in stage 3.0 (TID 136, 192.168.1.9, executor 1, partition 56, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:13 INFO TaskSetManager: Finished task 52.0 in stage 3.0 (TID 134) in 165 ms on 192.168.1.9 (executor 1) (132/200)
20/12/04 16:11:13 INFO TaskSetManager: Starting task 59.0 in stage 3.0 (TID 137, 192.168.1.9, executor 2, partition 59, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:13 INFO TaskSetManager: Finished task 54.0 in stage 3.0 (TID 135) in 163 ms on 192.168.1.9 (executor 2) (133/200)
20/12/04 16:11:14 INFO TaskSetManager: Starting task 60.0 in stage 3.0 (TID 138, 192.168.1.9, executor 1, partition 60, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:14 INFO TaskSetManager: Finished task 56.0 in stage 3.0 (TID 136) in 168 ms on 192.168.1.9 (executor 1) (134/200)
20/12/04 16:11:14 INFO TaskSetManager: Starting task 62.0 in stage 3.0 (TID 139, 192.168.1.9, executor 2, partition 62, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:14 INFO TaskSetManager: Finished task 59.0 in stage 3.0 (TID 137) in 195 ms on 192.168.1.9 (executor 2) (135/200)
20/12/04 16:11:14 INFO TaskSetManager: Starting task 64.0 in stage 3.0 (TID 140, 192.168.1.9, executor 1, partition 64, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:14 INFO TaskSetManager: Finished task 60.0 in stage 3.0 (TID 138) in 173 ms on 192.168.1.9 (executor 1) (136/200)
20/12/04 16:11:14 INFO TaskSetManager: Starting task 67.0 in stage 3.0 (TID 141, 192.168.1.9, executor 2, partition 67, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:14 INFO TaskSetManager: Finished task 62.0 in stage 3.0 (TID 139) in 164 ms on 192.168.1.9 (executor 2) (137/200)
20/12/04 16:11:14 INFO TaskSetManager: Starting task 68.0 in stage 3.0 (TID 142, 192.168.1.9, executor 1, partition 68, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:14 INFO TaskSetManager: Finished task 64.0 in stage 3.0 (TID 140) in 140 ms on 192.168.1.9 (executor 1) (138/200)
20/12/04 16:11:14 INFO TaskSetManager: Starting task 71.0 in stage 3.0 (TID 143, 192.168.1.9, executor 2, partition 71, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:14 INFO TaskSetManager: Finished task 67.0 in stage 3.0 (TID 141) in 154 ms on 192.168.1.9 (executor 2) (139/200)
20/12/04 16:11:14 INFO TaskSetManager: Starting task 72.0 in stage 3.0 (TID 144, 192.168.1.9, executor 1, partition 72, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:14 INFO TaskSetManager: Finished task 68.0 in stage 3.0 (TID 142) in 127 ms on 192.168.1.9 (executor 1) (140/200)
20/12/04 16:11:14 INFO TaskSetManager: Starting task 74.0 in stage 3.0 (TID 145, 192.168.1.9, executor 1, partition 74, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:14 INFO TaskSetManager: Finished task 72.0 in stage 3.0 (TID 144) in 162 ms on 192.168.1.9 (executor 1) (141/200)
20/12/04 16:11:14 INFO TaskSetManager: Starting task 76.0 in stage 3.0 (TID 146, 192.168.1.9, executor 2, partition 76, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:14 INFO TaskSetManager: Finished task 71.0 in stage 3.0 (TID 143) in 183 ms on 192.168.1.9 (executor 2) (142/200)
20/12/04 16:11:14 INFO TaskSetManager: Starting task 78.0 in stage 3.0 (TID 147, 192.168.1.9, executor 1, partition 78, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:14 INFO TaskSetManager: Finished task 74.0 in stage 3.0 (TID 145) in 128 ms on 192.168.1.9 (executor 1) (143/200)
20/12/04 16:11:14 INFO TaskSetManager: Starting task 80.0 in stage 3.0 (TID 148, 192.168.1.9, executor 2, partition 80, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:14 INFO TaskSetManager: Finished task 76.0 in stage 3.0 (TID 146) in 140 ms on 192.168.1.9 (executor 2) (144/200)
20/12/04 16:11:14 INFO TaskSetManager: Starting task 81.0 in stage 3.0 (TID 149, 192.168.1.9, executor 1, partition 81, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:14 INFO TaskSetManager: Finished task 78.0 in stage 3.0 (TID 147) in 135 ms on 192.168.1.9 (executor 1) (145/200)
20/12/04 16:11:14 INFO TaskSetManager: Starting task 82.0 in stage 3.0 (TID 150, 192.168.1.9, executor 2, partition 82, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:14 INFO TaskSetManager: Finished task 80.0 in stage 3.0 (TID 148) in 164 ms on 192.168.1.9 (executor 2) (146/200)
20/12/04 16:11:15 INFO TaskSetManager: Starting task 83.0 in stage 3.0 (TID 151, 192.168.1.9, executor 1, partition 83, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:15 INFO TaskSetManager: Finished task 81.0 in stage 3.0 (TID 149) in 206 ms on 192.168.1.9 (executor 1) (147/200)
20/12/04 16:11:15 INFO TaskSetManager: Starting task 87.0 in stage 3.0 (TID 152, 192.168.1.9, executor 2, partition 87, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:15 INFO TaskSetManager: Finished task 82.0 in stage 3.0 (TID 150) in 200 ms on 192.168.1.9 (executor 2) (148/200)
20/12/04 16:11:15 INFO TaskSetManager: Starting task 90.0 in stage 3.0 (TID 153, 192.168.1.9, executor 1, partition 90, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:15 INFO TaskSetManager: Finished task 83.0 in stage 3.0 (TID 151) in 134 ms on 192.168.1.9 (executor 1) (149/200)
20/12/04 16:11:15 INFO TaskSetManager: Starting task 92.0 in stage 3.0 (TID 154, 192.168.1.9, executor 2, partition 92, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:15 INFO TaskSetManager: Finished task 87.0 in stage 3.0 (TID 152) in 131 ms on 192.168.1.9 (executor 2) (150/200)
20/12/04 16:11:15 INFO TaskSetManager: Starting task 93.0 in stage 3.0 (TID 155, 192.168.1.9, executor 1, partition 93, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:15 INFO TaskSetManager: Finished task 90.0 in stage 3.0 (TID 153) in 130 ms on 192.168.1.9 (executor 1) (151/200)
20/12/04 16:11:15 INFO TaskSetManager: Starting task 94.0 in stage 3.0 (TID 156, 192.168.1.9, executor 2, partition 94, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:15 INFO TaskSetManager: Finished task 92.0 in stage 3.0 (TID 154) in 139 ms on 192.168.1.9 (executor 2) (152/200)
20/12/04 16:11:15 INFO TaskSetManager: Starting task 96.0 in stage 3.0 (TID 157, 192.168.1.9, executor 1, partition 96, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:15 INFO TaskSetManager: Finished task 93.0 in stage 3.0 (TID 155) in 167 ms on 192.168.1.9 (executor 1) (153/200)
20/12/04 16:11:15 INFO TaskSetManager: Starting task 97.0 in stage 3.0 (TID 158, 192.168.1.9, executor 2, partition 97, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:15 INFO TaskSetManager: Finished task 94.0 in stage 3.0 (TID 156) in 210 ms on 192.168.1.9 (executor 2) (154/200)
20/12/04 16:11:15 INFO TaskSetManager: Starting task 98.0 in stage 3.0 (TID 159, 192.168.1.9, executor 1, partition 98, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:15 INFO TaskSetManager: Finished task 96.0 in stage 3.0 (TID 157) in 147 ms on 192.168.1.9 (executor 1) (155/200)
20/12/04 16:11:15 INFO TaskSetManager: Starting task 99.0 in stage 3.0 (TID 160, 192.168.1.9, executor 2, partition 99, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:15 INFO TaskSetManager: Finished task 97.0 in stage 3.0 (TID 158) in 127 ms on 192.168.1.9 (executor 2) (156/200)
20/12/04 16:11:15 INFO TaskSetManager: Starting task 100.0 in stage 3.0 (TID 161, 192.168.1.9, executor 1, partition 100, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:15 INFO TaskSetManager: Finished task 98.0 in stage 3.0 (TID 159) in 125 ms on 192.168.1.9 (executor 1) (157/200)
20/12/04 16:11:15 INFO TaskSetManager: Starting task 104.0 in stage 3.0 (TID 162, 192.168.1.9, executor 2, partition 104, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:15 INFO TaskSetManager: Finished task 99.0 in stage 3.0 (TID 160) in 147 ms on 192.168.1.9 (executor 2) (158/200)
20/12/04 16:11:15 INFO TaskSetManager: Starting task 110.0 in stage 3.0 (TID 163, 192.168.1.9, executor 1, partition 110, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:15 INFO TaskSetManager: Finished task 100.0 in stage 3.0 (TID 161) in 136 ms on 192.168.1.9 (executor 1) (159/200)
20/12/04 16:11:16 INFO TaskSetManager: Starting task 112.0 in stage 3.0 (TID 164, 192.168.1.9, executor 2, partition 112, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:16 INFO TaskSetManager: Finished task 104.0 in stage 3.0 (TID 162) in 139 ms on 192.168.1.9 (executor 2) (160/200)
20/12/04 16:11:16 INFO TaskSetManager: Starting task 114.0 in stage 3.0 (TID 165, 192.168.1.9, executor 1, partition 114, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:16 INFO TaskSetManager: Finished task 110.0 in stage 3.0 (TID 163) in 141 ms on 192.168.1.9 (executor 1) (161/200)
20/12/04 16:11:16 INFO TaskSetManager: Starting task 117.0 in stage 3.0 (TID 166, 192.168.1.9, executor 2, partition 117, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:16 INFO TaskSetManager: Finished task 112.0 in stage 3.0 (TID 164) in 166 ms on 192.168.1.9 (executor 2) (162/200)
20/12/04 16:11:16 INFO TaskSetManager: Starting task 118.0 in stage 3.0 (TID 167, 192.168.1.9, executor 1, partition 118, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:16 INFO TaskSetManager: Finished task 114.0 in stage 3.0 (TID 165) in 165 ms on 192.168.1.9 (executor 1) (163/200)
20/12/04 16:11:16 INFO TaskSetManager: Starting task 120.0 in stage 3.0 (TID 168, 192.168.1.9, executor 2, partition 120, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:16 INFO TaskSetManager: Finished task 117.0 in stage 3.0 (TID 166) in 210 ms on 192.168.1.9 (executor 2) (164/200)
20/12/04 16:11:16 INFO TaskSetManager: Starting task 121.0 in stage 3.0 (TID 169, 192.168.1.9, executor 1, partition 121, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:16 INFO TaskSetManager: Finished task 118.0 in stage 3.0 (TID 167) in 199 ms on 192.168.1.9 (executor 1) (165/200)
20/12/04 16:11:16 INFO TaskSetManager: Starting task 123.0 in stage 3.0 (TID 170, 192.168.1.9, executor 2, partition 123, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:16 INFO TaskSetManager: Finished task 120.0 in stage 3.0 (TID 168) in 177 ms on 192.168.1.9 (executor 2) (166/200)
20/12/04 16:11:16 INFO TaskSetManager: Starting task 125.0 in stage 3.0 (TID 171, 192.168.1.9, executor 1, partition 125, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:16 INFO TaskSetManager: Finished task 121.0 in stage 3.0 (TID 169) in 155 ms on 192.168.1.9 (executor 1) (167/200)
20/12/04 16:11:16 INFO TaskSetManager: Starting task 127.0 in stage 3.0 (TID 172, 192.168.1.9, executor 2, partition 127, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:16 INFO TaskSetManager: Finished task 123.0 in stage 3.0 (TID 170) in 137 ms on 192.168.1.9 (executor 2) (168/200)
20/12/04 16:11:16 INFO TaskSetManager: Starting task 130.0 in stage 3.0 (TID 173, 192.168.1.9, executor 1, partition 130, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:16 INFO TaskSetManager: Finished task 125.0 in stage 3.0 (TID 171) in 128 ms on 192.168.1.9 (executor 1) (169/200)
20/12/04 16:11:16 INFO TaskSetManager: Starting task 134.0 in stage 3.0 (TID 174, 192.168.1.9, executor 1, partition 134, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:16 INFO TaskSetManager: Finished task 130.0 in stage 3.0 (TID 173) in 127 ms on 192.168.1.9 (executor 1) (170/200)
20/12/04 16:11:16 INFO TaskSetManager: Starting task 138.0 in stage 3.0 (TID 175, 192.168.1.9, executor 2, partition 138, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:16 INFO TaskSetManager: Finished task 127.0 in stage 3.0 (TID 172) in 139 ms on 192.168.1.9 (executor 2) (171/200)
20/12/04 16:11:17 INFO TaskSetManager: Starting task 142.0 in stage 3.0 (TID 176, 192.168.1.9, executor 1, partition 142, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:17 INFO TaskSetManager: Finished task 134.0 in stage 3.0 (TID 174) in 180 ms on 192.168.1.9 (executor 1) (172/200)
20/12/04 16:11:17 INFO TaskSetManager: Starting task 144.0 in stage 3.0 (TID 177, 192.168.1.9, executor 2, partition 144, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:17 INFO TaskSetManager: Finished task 138.0 in stage 3.0 (TID 175) in 175 ms on 192.168.1.9 (executor 2) (173/200)
20/12/04 16:11:17 INFO TaskSetManager: Starting task 145.0 in stage 3.0 (TID 178, 192.168.1.9, executor 2, partition 145, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:17 INFO TaskSetManager: Finished task 144.0 in stage 3.0 (TID 177) in 135 ms on 192.168.1.9 (executor 2) (174/200)
20/12/04 16:11:17 INFO TaskSetManager: Starting task 147.0 in stage 3.0 (TID 179, 192.168.1.9, executor 1, partition 147, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:17 INFO TaskSetManager: Finished task 142.0 in stage 3.0 (TID 176) in 145 ms on 192.168.1.9 (executor 1) (175/200)
20/12/04 16:11:17 INFO TaskSetManager: Finished task 147.0 in stage 3.0 (TID 179) in 140 ms on 192.168.1.9 (executor 1) (176/200)
20/12/04 16:11:17 INFO TaskSetManager: Starting task 148.0 in stage 3.0 (TID 180, 192.168.1.9, executor 1, partition 148, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:17 INFO TaskSetManager: Starting task 149.0 in stage 3.0 (TID 181, 192.168.1.9, executor 2, partition 149, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:17 INFO TaskSetManager: Finished task 145.0 in stage 3.0 (TID 178) in 164 ms on 192.168.1.9 (executor 2) (177/200)
20/12/04 16:11:17 INFO TaskSetManager: Starting task 152.0 in stage 3.0 (TID 182, 192.168.1.9, executor 1, partition 152, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:17 INFO TaskSetManager: Finished task 148.0 in stage 3.0 (TID 180) in 173 ms on 192.168.1.9 (executor 1) (178/200)
20/12/04 16:11:17 INFO TaskSetManager: Starting task 154.0 in stage 3.0 (TID 183, 192.168.1.9, executor 2, partition 154, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:17 INFO TaskSetManager: Finished task 149.0 in stage 3.0 (TID 181) in 169 ms on 192.168.1.9 (executor 2) (179/200)
20/12/04 16:11:17 INFO TaskSetManager: Starting task 157.0 in stage 3.0 (TID 184, 192.168.1.9, executor 1, partition 157, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:17 INFO TaskSetManager: Finished task 152.0 in stage 3.0 (TID 182) in 133 ms on 192.168.1.9 (executor 1) (180/200)
20/12/04 16:11:17 INFO TaskSetManager: Starting task 158.0 in stage 3.0 (TID 185, 192.168.1.9, executor 2, partition 158, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:17 INFO TaskSetManager: Finished task 154.0 in stage 3.0 (TID 183) in 145 ms on 192.168.1.9 (executor 2) (181/200)
20/12/04 16:11:17 INFO TaskSetManager: Starting task 159.0 in stage 3.0 (TID 186, 192.168.1.9, executor 1, partition 159, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:17 INFO TaskSetManager: Finished task 157.0 in stage 3.0 (TID 184) in 177 ms on 192.168.1.9 (executor 1) (182/200)
20/12/04 16:11:17 INFO TaskSetManager: Starting task 160.0 in stage 3.0 (TID 187, 192.168.1.9, executor 2, partition 160, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:17 INFO TaskSetManager: Finished task 158.0 in stage 3.0 (TID 185) in 209 ms on 192.168.1.9 (executor 2) (183/200)
20/12/04 16:11:17 INFO TaskSetManager: Starting task 166.0 in stage 3.0 (TID 188, 192.168.1.9, executor 1, partition 166, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:17 INFO TaskSetManager: Finished task 159.0 in stage 3.0 (TID 186) in 154 ms on 192.168.1.9 (executor 1) (184/200)
20/12/04 16:11:18 INFO TaskSetManager: Finished task 160.0 in stage 3.0 (TID 187) in 156 ms on 192.168.1.9 (executor 2) (185/200)
20/12/04 16:11:18 INFO TaskSetManager: Starting task 169.0 in stage 3.0 (TID 189, 192.168.1.9, executor 2, partition 169, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:18 INFO TaskSetManager: Starting task 171.0 in stage 3.0 (TID 190, 192.168.1.9, executor 1, partition 171, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:18 INFO TaskSetManager: Finished task 166.0 in stage 3.0 (TID 188) in 160 ms on 192.168.1.9 (executor 1) (186/200)
20/12/04 16:11:18 INFO TaskSetManager: Starting task 176.0 in stage 3.0 (TID 191, 192.168.1.9, executor 2, partition 176, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:18 INFO TaskSetManager: Finished task 169.0 in stage 3.0 (TID 189) in 170 ms on 192.168.1.9 (executor 2) (187/200)
20/12/04 16:11:18 INFO TaskSetManager: Starting task 177.0 in stage 3.0 (TID 192, 192.168.1.9, executor 1, partition 177, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:18 INFO TaskSetManager: Finished task 171.0 in stage 3.0 (TID 190) in 148 ms on 192.168.1.9 (executor 1) (188/200)
20/12/04 16:11:18 INFO TaskSetManager: Starting task 180.0 in stage 3.0 (TID 193, 192.168.1.9, executor 2, partition 180, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:18 INFO TaskSetManager: Finished task 176.0 in stage 3.0 (TID 191) in 150 ms on 192.168.1.9 (executor 2) (189/200)
20/12/04 16:11:18 INFO TaskSetManager: Starting task 182.0 in stage 3.0 (TID 194, 192.168.1.9, executor 1, partition 182, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:18 INFO TaskSetManager: Finished task 177.0 in stage 3.0 (TID 192) in 141 ms on 192.168.1.9 (executor 1) (190/200)
20/12/04 16:11:18 INFO TaskSetManager: Starting task 185.0 in stage 3.0 (TID 195, 192.168.1.9, executor 2, partition 185, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:18 INFO TaskSetManager: Finished task 180.0 in stage 3.0 (TID 193) in 177 ms on 192.168.1.9 (executor 2) (191/200)
20/12/04 16:11:18 INFO TaskSetManager: Starting task 186.0 in stage 3.0 (TID 196, 192.168.1.9, executor 1, partition 186, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:18 INFO TaskSetManager: Finished task 182.0 in stage 3.0 (TID 194) in 172 ms on 192.168.1.9 (executor 1) (192/200)
20/12/04 16:11:18 INFO TaskSetManager: Starting task 187.0 in stage 3.0 (TID 197, 192.168.1.9, executor 2, partition 187, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:18 INFO TaskSetManager: Finished task 185.0 in stage 3.0 (TID 195) in 153 ms on 192.168.1.9 (executor 2) (193/200)
20/12/04 16:11:18 INFO TaskSetManager: Starting task 188.0 in stage 3.0 (TID 198, 192.168.1.9, executor 1, partition 188, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:18 INFO TaskSetManager: Finished task 186.0 in stage 3.0 (TID 196) in 125 ms on 192.168.1.9 (executor 1) (194/200)
20/12/04 16:11:18 INFO TaskSetManager: Starting task 189.0 in stage 3.0 (TID 199, 192.168.1.9, executor 2, partition 189, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:18 INFO TaskSetManager: Finished task 187.0 in stage 3.0 (TID 197) in 135 ms on 192.168.1.9 (executor 2) (195/200)
20/12/04 16:11:18 INFO TaskSetManager: Starting task 191.0 in stage 3.0 (TID 200, 192.168.1.9, executor 1, partition 191, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:18 INFO TaskSetManager: Finished task 188.0 in stage 3.0 (TID 198) in 130 ms on 192.168.1.9 (executor 1) (196/200)
20/12/04 16:11:18 INFO TaskSetManager: Starting task 196.0 in stage 3.0 (TID 201, 192.168.1.9, executor 2, partition 196, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:18 INFO TaskSetManager: Finished task 189.0 in stage 3.0 (TID 199) in 138 ms on 192.168.1.9 (executor 2) (197/200)
20/12/04 16:11:18 INFO TaskSetManager: Starting task 197.0 in stage 3.0 (TID 202, 192.168.1.9, executor 1, partition 197, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:11:18 INFO TaskSetManager: Finished task 191.0 in stage 3.0 (TID 200) in 131 ms on 192.168.1.9 (executor 1) (198/200)
20/12/04 16:11:19 INFO TaskSetManager: Finished task 196.0 in stage 3.0 (TID 201) in 134 ms on 192.168.1.9 (executor 2) (199/200)
20/12/04 16:11:19 INFO TaskSetManager: Finished task 197.0 in stage 3.0 (TID 202) in 129 ms on 192.168.1.9 (executor 1) (200/200)
20/12/04 16:11:19 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 16:11:19 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/userFiles-b523a33b-b335-47cf-aff7-b0e4e0730499/darima.zip/darima/dlsa.py:22) finished in 124.173 s
20/12/04 16:11:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:11:19 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
20/12/04 16:11:19 INFO DAGScheduler: Job 1 finished: toPandas at /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/userFiles-b523a33b-b335-47cf-aff7-b0e4e0730499/darima.zip/darima/dlsa.py:22, took 125.710935 s
20/12/04 16:11:19 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:11:19 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:11:19 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:11:19 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:11:19 INFO CodeGenerator: Code generated in 14.252518 ms
20/12/04 16:11:19 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:11:19 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:11:19 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:38329 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:11:19 INFO SparkContext: Created broadcast 6 from toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209
20/12/04 16:11:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:11:19 INFO SparkContext: Starting job: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209
20/12/04 16:11:19 INFO DAGScheduler: Got job 2 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209) with 1 output partitions
20/12/04 16:11:19 INFO DAGScheduler: Final stage: ResultStage 4 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209)
20/12/04 16:11:19 INFO DAGScheduler: Parents of final stage: List()
20/12/04 16:11:19 INFO DAGScheduler: Missing parents: List()
20/12/04 16:11:19 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209), which has no missing parents
20/12/04 16:11:19 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 21.7 KiB, free 5.8 GiB)
20/12/04 16:11:19 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 5.8 GiB)
20/12/04 16:11:19 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:38329 (size: 10.4 KiB, free: 5.8 GiB)
20/12/04 16:11:19 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1223
20/12/04 16:11:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209) (first 15 tasks are for partitions Vector(0))
20/12/04 16:11:19 INFO YarnScheduler: Adding task set 4.0 with 1 tasks
20/12/04 16:11:19 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 203, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7790 bytes)
20/12/04 16:11:19 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:44455 (size: 10.4 KiB, free: 912.2 MiB)
20/12/04 16:11:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:44455 (size: 28.7 KiB, free: 912.1 MiB)
20/12/04 16:11:21 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 203) in 1924 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 16:11:21 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/12/04 16:11:21 INFO DAGScheduler: ResultStage 4 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209) finished in 1.951 s
20/12/04 16:11:21 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:11:21 INFO YarnScheduler: Killing all running tasks in stage 4: Stage finished
20/12/04 16:11:21 INFO DAGScheduler: Job 2 finished: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:209, took 1.959838 s
20/12/04 16:11:22 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 16:11:22 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 16:11:22 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 16:11:22 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 16:11:22 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 16:11:22 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 16:11:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 16:11:22 INFO MemoryStore: MemoryStore cleared
20/12/04 16:11:22 INFO BlockManager: BlockManager stopped
20/12/04 16:11:22 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 16:11:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 16:11:22 INFO SparkContext: Successfully stopped SparkContext
20/12/04 16:11:22 INFO ShutdownHookManager: Shutdown hook called
20/12/04 16:11:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-04de1f3c-9461-40ef-a329-01045edef571
20/12/04 16:11:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126/pyspark-e77b2510-ab38-41e1-9217-9af0620bb648
20/12/04 16:11:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-5dae98dc-76b1-45a4-a685-85b142ab2126
20/12/04 16:11:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 16:11:25 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:11:25 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:11:25 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:11:25 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:11:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:11:25 INFO SparkContext: Running Spark version 3.0.1
20/12/04 16:11:25 INFO ResourceUtils: ==============================================================
20/12/04 16:11:25 INFO ResourceUtils: Resources for spark.driver:

20/12/04 16:11:25 INFO ResourceUtils: ==============================================================
20/12/04 16:11:25 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 16:11:25 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:11:26 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:11:26 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:11:26 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:11:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:11:26 INFO Utils: Successfully started service 'sparkDriver' on port 35557.
20/12/04 16:11:26 INFO SparkEnv: Registering MapOutputTracker
20/12/04 16:11:26 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 16:11:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 16:11:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 16:11:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 16:11:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1fa39b8e-1e76-48b8-aa11-c3da5482bce8
20/12/04 16:11:26 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 16:11:26 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 16:11:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 16:11:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 16:11:27 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 16:11:28 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 16:11:28 INFO Configuration: resource-types.xml not found
20/12/04 16:11:28 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 16:11:28 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 16:11:28 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 16:11:28 INFO Client: Setting up container launch context for our AM
20/12/04 16:11:28 INFO Client: Setting up the launch environment for our AM container
20/12/04 16:11:28 INFO Client: Preparing resources for our AM container
20/12/04 16:11:28 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 16:11:31 INFO Client: Uploading resource file:/tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/__spark_libs__10574472957480859724.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0073/__spark_libs__10574472957480859724.zip
20/12/04 16:11:32 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0073/pyspark.zip
20/12/04 16:11:32 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0073/py4j-0.10.9-src.zip
20/12/04 16:11:33 INFO Client: Uploading resource file:/tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/__spark_conf__14371546885048403852.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0073/__spark_conf__.zip
20/12/04 16:11:33 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:11:33 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:11:33 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:11:33 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:11:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:11:33 INFO Client: Submitting application application_1607015337794_0073 to ResourceManager
20/12/04 16:11:33 INFO YarnClientImpl: Submitted application application_1607015337794_0073
20/12/04 16:11:34 INFO Client: Application report for application_1607015337794_0073 (state: ACCEPTED)
20/12/04 16:11:34 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607116293243
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0073/
	 user: bsuconn
20/12/04 16:11:35 INFO Client: Application report for application_1607015337794_0073 (state: ACCEPTED)
20/12/04 16:11:36 INFO Client: Application report for application_1607015337794_0073 (state: ACCEPTED)
20/12/04 16:11:37 INFO Client: Application report for application_1607015337794_0073 (state: ACCEPTED)
20/12/04 16:11:38 INFO Client: Application report for application_1607015337794_0073 (state: ACCEPTED)
20/12/04 16:11:38 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0073), /proxy/application_1607015337794_0073
20/12/04 16:11:39 INFO Client: Application report for application_1607015337794_0073 (state: RUNNING)
20/12/04 16:11:39 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607116293243
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0073/
	 user: bsuconn
20/12/04 16:11:39 INFO YarnClientSchedulerBackend: Application application_1607015337794_0073 has started running.
20/12/04 16:11:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35639.
20/12/04 16:11:39 INFO NettyBlockTransferService: Server created on 192.168.1.9:35639
20/12/04 16:11:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 16:11:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 35639, None)
20/12/04 16:11:39 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:35639 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 35639, None)
20/12/04 16:11:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 35639, None)
20/12/04 16:11:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 35639, None)
20/12/04 16:11:39 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:11:39 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 16:11:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 16:11:45 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:46456) with ID 1
20/12/04 16:11:45 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:34921 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 34921, None)
20/12/04 16:11:47 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:46462) with ID 2
20/12/04 16:11:47 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:34685 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 34685, None)
20/12/04 16:11:57 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
20/12/04 16:11:57 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 16:11:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 16:11:57 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 16:11:57 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:11:57 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:11:57 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:11:57 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:11:57 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:11:58 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:35557/files/darima.zip with timestamp 1607116318580
20/12/04 16:11:58 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/userFiles-cbb5262b-bb34-4cb8-8076-40f27666df2e/darima.zip
20/12/04 16:12:00 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
20/12/04 16:12:02 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
20/12/04 16:12:02 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:12:02 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:12:02 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:12:02 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:12:03 INFO CodeGenerator: Code generated in 294.53492 ms
20/12/04 16:12:03 INFO CodeGenerator: Code generated in 36.472834 ms
20/12/04 16:12:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:12:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:12:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:35639 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:12:03 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 16:12:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:12:04 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 16:12:04 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 16:12:04 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 16:12:04 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 16:12:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 16:12:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 16:12:04 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:12:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 16:12:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 16:12:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:35639 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:12:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 16:12:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:12:04 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 16:12:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:12:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:34685 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:12:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:34685 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:12:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3538 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 16:12:07 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 16:12:07 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.697 s
20/12/04 16:12:07 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:12:07 INFO DAGScheduler: running: Set()
20/12/04 16:12:07 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 16:12:07 INFO DAGScheduler: failed: Set()
20/12/04 16:12:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:12:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 16:12:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 16:12:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:35639 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:12:08 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 16:12:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:12:08 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 16:12:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:12:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:34921 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:12:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:46456
20/12/04 16:12:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2078 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:12:10 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 16:12:10 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 2.094 s
20/12/04 16:12:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:12:10 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 16:12:10 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 5.883611 s
20/12/04 16:12:12 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:34921 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:12:12 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:35639 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:12:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:34685 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:12:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:35639 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:12:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 16:12:14 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:12:14 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:12:14 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:12:14 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:12:14 INFO CodeGenerator: Code generated in 28.45952 ms
20/12/04 16:12:14 INFO CodeGenerator: Code generated in 17.783515 ms
20/12/04 16:12:14 INFO CodeGenerator: Code generated in 24.145381 ms
20/12/04 16:12:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:12:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:12:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:35639 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:12:14 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/userFiles-cbb5262b-bb34-4cb8-8076-40f27666df2e/darima.zip/darima/dlsa.py:22
20/12/04 16:12:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:12:14 INFO SparkContext: Starting job: toPandas at /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/userFiles-cbb5262b-bb34-4cb8-8076-40f27666df2e/darima.zip/darima/dlsa.py:22
20/12/04 16:12:14 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/userFiles-cbb5262b-bb34-4cb8-8076-40f27666df2e/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 16:12:14 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/userFiles-cbb5262b-bb34-4cb8-8076-40f27666df2e/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 16:12:14 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/userFiles-cbb5262b-bb34-4cb8-8076-40f27666df2e/darima.zip/darima/dlsa.py:22)
20/12/04 16:12:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 16:12:14 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 16:12:14 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/userFiles-cbb5262b-bb34-4cb8-8076-40f27666df2e/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:12:14 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 16:12:14 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 16:12:14 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:35639 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 16:12:14 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 16:12:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/userFiles-cbb5262b-bb34-4cb8-8076-40f27666df2e/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 16:12:14 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 16:12:14 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:12:14 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:34685 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 16:12:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:34685 (size: 28.7 KiB, free: 912.2 MiB)
20/12/04 16:12:15 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1278 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 16:12:15 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 58987
20/12/04 16:12:15 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 16:12:15 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/userFiles-cbb5262b-bb34-4cb8-8076-40f27666df2e/darima.zip/darima/dlsa.py:22) finished in 1.302 s
20/12/04 16:12:15 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:12:15 INFO DAGScheduler: running: Set()
20/12/04 16:12:15 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 16:12:15 INFO DAGScheduler: failed: Set()
20/12/04 16:12:15 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/userFiles-cbb5262b-bb34-4cb8-8076-40f27666df2e/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:12:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 16:12:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.4 KiB, free 5.8 GiB)
20/12/04 16:12:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:35639 (size: 131.4 KiB, free: 5.8 GiB)
20/12/04 16:12:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 16:12:15 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/userFiles-cbb5262b-bb34-4cb8-8076-40f27666df2e/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 16:12:15 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 16:12:15 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:12:15 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:12:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:34921 (size: 131.4 KiB, free: 912.2 MiB)
20/12/04 16:12:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:34685 (size: 131.4 KiB, free: 912.1 MiB)
20/12/04 16:12:16 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:46456
20/12/04 16:12:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:46462
20/12/04 16:12:26 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 16:12:26 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 10400 ms on 192.168.1.9 (executor 2) (1/200)
20/12/04 16:12:26 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 4) in 10639 ms on 192.168.1.9 (executor 1) (2/200)
20/12/04 16:12:26 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 4, NODE_LOCAL, 7336 bytes)
20/12/04 16:12:28 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 16:12:28 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 16:12:28 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/userFiles-cbb5262b-bb34-4cb8-8076-40f27666df2e/darima.zip/darima/dlsa.py:22) failed in 12.227 s due to Stage cancelled because SparkContext was shut down
20/12/04 16:12:28 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/userFiles-cbb5262b-bb34-4cb8-8076-40f27666df2e/darima.zip/darima/dlsa.py:22, took 13.570029 s
20/12/04 16:12:28 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 16:12:28 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 16:12:28 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 16:12:28 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 16:12:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 16:12:28 INFO MemoryStore: MemoryStore cleared
20/12/04 16:12:28 INFO BlockManager: BlockManager stopped
20/12/04 16:12:28 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 16:12:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 16:12:28 INFO SparkContext: Successfully stopped SparkContext
20/12/04 16:12:28 INFO ShutdownHookManager: Shutdown hook called
20/12/04 16:12:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c
20/12/04 16:12:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-4b86eef7-e6f8-4387-802b-08ce196c615c/pyspark-fb10a827-64b7-42f7-86d1-b2985487d0bc
20/12/04 16:12:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-ea22d9d7-8b50-456e-81b8-2189a3ebe696
20/12/04 16:12:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 16:12:43 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:12:43 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:12:43 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:12:43 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:12:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:12:43 INFO SparkContext: Running Spark version 3.0.1
20/12/04 16:12:44 INFO ResourceUtils: ==============================================================
20/12/04 16:12:44 INFO ResourceUtils: Resources for spark.driver:

20/12/04 16:12:44 INFO ResourceUtils: ==============================================================
20/12/04 16:12:44 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 16:12:44 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:12:44 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:12:44 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:12:44 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:12:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:12:44 INFO Utils: Successfully started service 'sparkDriver' on port 33545.
20/12/04 16:12:44 INFO SparkEnv: Registering MapOutputTracker
20/12/04 16:12:44 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 16:12:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 16:12:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 16:12:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 16:12:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a8de8903-3ae1-4f20-8c5b-0cc2eafdb862
20/12/04 16:12:44 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 16:12:44 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 16:12:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 16:12:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 16:12:45 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 16:12:46 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 16:12:46 INFO Configuration: resource-types.xml not found
20/12/04 16:12:46 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 16:12:46 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 16:12:46 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 16:12:46 INFO Client: Setting up container launch context for our AM
20/12/04 16:12:46 INFO Client: Setting up the launch environment for our AM container
20/12/04 16:12:46 INFO Client: Preparing resources for our AM container
20/12/04 16:12:46 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 16:12:48 INFO Client: Uploading resource file:/tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/__spark_libs__2702757492037431515.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0074/__spark_libs__2702757492037431515.zip
20/12/04 16:12:49 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0074/pyspark.zip
20/12/04 16:12:49 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0074/py4j-0.10.9-src.zip
20/12/04 16:12:50 INFO Client: Uploading resource file:/tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/__spark_conf__14125533574577144223.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0074/__spark_conf__.zip
20/12/04 16:12:50 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:12:50 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:12:50 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:12:50 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:12:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:12:50 INFO Client: Submitting application application_1607015337794_0074 to ResourceManager
20/12/04 16:12:50 INFO YarnClientImpl: Submitted application application_1607015337794_0074
20/12/04 16:12:51 INFO Client: Application report for application_1607015337794_0074 (state: ACCEPTED)
20/12/04 16:12:51 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607116370646
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0074/
	 user: bsuconn
20/12/04 16:12:52 INFO Client: Application report for application_1607015337794_0074 (state: ACCEPTED)
20/12/04 16:12:53 INFO Client: Application report for application_1607015337794_0074 (state: ACCEPTED)
20/12/04 16:12:54 INFO Client: Application report for application_1607015337794_0074 (state: ACCEPTED)
20/12/04 16:12:55 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0074), /proxy/application_1607015337794_0074
20/12/04 16:12:55 INFO Client: Application report for application_1607015337794_0074 (state: RUNNING)
20/12/04 16:12:55 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607116370646
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0074/
	 user: bsuconn
20/12/04 16:12:55 INFO YarnClientSchedulerBackend: Application application_1607015337794_0074 has started running.
20/12/04 16:12:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43023.
20/12/04 16:12:55 INFO NettyBlockTransferService: Server created on 192.168.1.9:43023
20/12/04 16:12:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 16:12:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 43023, None)
20/12/04 16:12:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:43023 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 43023, None)
20/12/04 16:12:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 43023, None)
20/12/04 16:12:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 43023, None)
20/12/04 16:12:55 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:12:56 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 16:12:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 16:13:00 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:40214) with ID 1
20/12/04 16:13:00 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 16:13:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:46773 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 46773, None)
20/12/04 16:13:00 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 16:13:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 16:13:00 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 16:13:00 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:13:00 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:13:00 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:13:00 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:13:00 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:13:01 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:33545/files/darima.zip with timestamp 1607116381589
20/12/04 16:13:01 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/userFiles-b2fe09a0-49cd-4712-868c-833bf8f70710/darima.zip
20/12/04 16:13:03 INFO InMemoryFileIndex: It took 69 ms to list leaf files for 1 paths.
20/12/04 16:13:05 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/12/04 16:13:05 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:13:05 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:13:05 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:13:05 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:13:06 INFO CodeGenerator: Code generated in 247.158857 ms
20/12/04 16:13:06 INFO CodeGenerator: Code generated in 50.365337 ms
20/12/04 16:13:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:13:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:13:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:43023 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:13:06 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 16:13:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:13:06 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 16:13:06 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 16:13:06 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 16:13:06 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 16:13:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 16:13:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 16:13:06 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:13:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 16:13:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 16:13:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:43023 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:13:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 16:13:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:13:07 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 16:13:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:13:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:46773 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:13:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:46773 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:13:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3269 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:13:10 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 16:13:10 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.462 s
20/12/04 16:13:10 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:13:10 INFO DAGScheduler: running: Set()
20/12/04 16:13:10 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 16:13:10 INFO DAGScheduler: failed: Set()
20/12/04 16:13:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:13:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 16:13:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 16:13:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:43023 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:13:10 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 16:13:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:13:10 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 16:13:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:46773 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:13:10 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:40214
20/12/04 16:13:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 342 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:13:10 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 16:13:10 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.362 s
20/12/04 16:13:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:13:10 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 16:13:10 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.928293 s
20/12/04 16:13:13 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:46773 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:13:13 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:43023 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:13:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:43023 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:13:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:46773 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:13:13 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:43023 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:13:13 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:46773 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:13:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 16:13:14 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:13:14 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:13:14 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:13:14 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:13:14 INFO CodeGenerator: Code generated in 29.835916 ms
20/12/04 16:13:14 INFO CodeGenerator: Code generated in 16.300672 ms
20/12/04 16:13:14 INFO CodeGenerator: Code generated in 21.356375 ms
20/12/04 16:13:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:13:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:13:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:43023 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:13:14 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/userFiles-b2fe09a0-49cd-4712-868c-833bf8f70710/darima.zip/darima/dlsa.py:22
20/12/04 16:13:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:13:14 INFO SparkContext: Starting job: toPandas at /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/userFiles-b2fe09a0-49cd-4712-868c-833bf8f70710/darima.zip/darima/dlsa.py:22
20/12/04 16:13:14 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/userFiles-b2fe09a0-49cd-4712-868c-833bf8f70710/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 16:13:14 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/userFiles-b2fe09a0-49cd-4712-868c-833bf8f70710/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 16:13:14 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/userFiles-b2fe09a0-49cd-4712-868c-833bf8f70710/darima.zip/darima/dlsa.py:22)
20/12/04 16:13:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 16:13:14 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 16:13:14 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/userFiles-b2fe09a0-49cd-4712-868c-833bf8f70710/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:13:14 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 16:13:14 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 16:13:14 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:43023 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 16:13:14 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 16:13:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/userFiles-b2fe09a0-49cd-4712-868c-833bf8f70710/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 16:13:14 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 16:13:14 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:13:14 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:46773 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 16:13:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:46773 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:13:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1371 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:13:16 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 16:13:16 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 43771
20/12/04 16:13:16 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/userFiles-b2fe09a0-49cd-4712-868c-833bf8f70710/darima.zip/darima/dlsa.py:22) finished in 1.398 s
20/12/04 16:13:16 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:13:16 INFO DAGScheduler: running: Set()
20/12/04 16:13:16 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 16:13:16 INFO DAGScheduler: failed: Set()
20/12/04 16:13:16 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/userFiles-b2fe09a0-49cd-4712-868c-833bf8f70710/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:13:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 16:13:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 16:13:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:43023 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 16:13:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 16:13:16 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/userFiles-b2fe09a0-49cd-4712-868c-833bf8f70710/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 16:13:16 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 16:13:16 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:46773 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 16:13:16 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:40214
20/12/04 16:13:20 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:20 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 4522 ms on 192.168.1.9 (executor 1) (1/200)
20/12/04 16:13:21 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:21 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 4) in 1144 ms on 192.168.1.9 (executor 1) (2/200)
20/12/04 16:13:24 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 4, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:24 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 5) in 2547 ms on 192.168.1.9 (executor 1) (3/200)
20/12/04 16:13:25 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 5, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:25 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 6) in 1199 ms on 192.168.1.9 (executor 1) (4/200)
20/12/04 16:13:26 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 6, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:26 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 7) in 1002 ms on 192.168.1.9 (executor 1) (5/200)
20/12/04 16:13:27 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 7, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:27 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 8) in 1413 ms on 192.168.1.9 (executor 1) (6/200)
20/12/04 16:13:29 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 10, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:29 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 9) in 1722 ms on 192.168.1.9 (executor 1) (7/200)
20/12/04 16:13:31 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 11, 192.168.1.9, executor 1, partition 11, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:31 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 10) in 1638 ms on 192.168.1.9 (executor 1) (8/200)
20/12/04 16:13:32 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 12, 192.168.1.9, executor 1, partition 12, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:32 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 11) in 985 ms on 192.168.1.9 (executor 1) (9/200)
20/12/04 16:13:35 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 13, 192.168.1.9, executor 1, partition 13, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:35 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 12) in 3549 ms on 192.168.1.9 (executor 1) (10/200)
20/12/04 16:13:36 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 14, 192.168.1.9, executor 1, partition 14, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:36 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 13) in 762 ms on 192.168.1.9 (executor 1) (11/200)
20/12/04 16:13:37 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 15, 192.168.1.9, executor 1, partition 18, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:37 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 14) in 1115 ms on 192.168.1.9 (executor 1) (12/200)
20/12/04 16:13:38 INFO TaskSetManager: Starting task 19.0 in stage 3.0 (TID 16, 192.168.1.9, executor 1, partition 19, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:38 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 15) in 1012 ms on 192.168.1.9 (executor 1) (13/200)
20/12/04 16:13:41 INFO TaskSetManager: Starting task 21.0 in stage 3.0 (TID 17, 192.168.1.9, executor 1, partition 21, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:41 INFO TaskSetManager: Finished task 19.0 in stage 3.0 (TID 16) in 2634 ms on 192.168.1.9 (executor 1) (14/200)
20/12/04 16:13:43 INFO TaskSetManager: Starting task 23.0 in stage 3.0 (TID 18, 192.168.1.9, executor 1, partition 23, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:43 INFO TaskSetManager: Finished task 21.0 in stage 3.0 (TID 17) in 1865 ms on 192.168.1.9 (executor 1) (15/200)
20/12/04 16:13:44 INFO TaskSetManager: Starting task 24.0 in stage 3.0 (TID 19, 192.168.1.9, executor 1, partition 24, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:44 INFO TaskSetManager: Finished task 23.0 in stage 3.0 (TID 18) in 1203 ms on 192.168.1.9 (executor 1) (16/200)
20/12/04 16:13:45 INFO TaskSetManager: Starting task 27.0 in stage 3.0 (TID 20, 192.168.1.9, executor 1, partition 27, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:45 INFO TaskSetManager: Finished task 24.0 in stage 3.0 (TID 19) in 1211 ms on 192.168.1.9 (executor 1) (17/200)
20/12/04 16:13:46 INFO TaskSetManager: Starting task 30.0 in stage 3.0 (TID 21, 192.168.1.9, executor 1, partition 30, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:46 INFO TaskSetManager: Finished task 27.0 in stage 3.0 (TID 20) in 582 ms on 192.168.1.9 (executor 1) (18/200)
20/12/04 16:13:47 INFO TaskSetManager: Starting task 31.0 in stage 3.0 (TID 22, 192.168.1.9, executor 1, partition 31, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:47 INFO TaskSetManager: Finished task 30.0 in stage 3.0 (TID 21) in 1465 ms on 192.168.1.9 (executor 1) (19/200)
20/12/04 16:13:49 INFO TaskSetManager: Starting task 32.0 in stage 3.0 (TID 23, 192.168.1.9, executor 1, partition 32, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:49 INFO TaskSetManager: Finished task 31.0 in stage 3.0 (TID 22) in 1498 ms on 192.168.1.9 (executor 1) (20/200)
20/12/04 16:13:50 INFO TaskSetManager: Starting task 34.0 in stage 3.0 (TID 24, 192.168.1.9, executor 1, partition 34, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:50 INFO TaskSetManager: Finished task 32.0 in stage 3.0 (TID 23) in 1285 ms on 192.168.1.9 (executor 1) (21/200)
20/12/04 16:13:51 INFO TaskSetManager: Starting task 35.0 in stage 3.0 (TID 25, 192.168.1.9, executor 1, partition 35, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:51 INFO TaskSetManager: Finished task 34.0 in stage 3.0 (TID 24) in 771 ms on 192.168.1.9 (executor 1) (22/200)
20/12/04 16:13:52 INFO TaskSetManager: Starting task 36.0 in stage 3.0 (TID 26, 192.168.1.9, executor 1, partition 36, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:52 INFO TaskSetManager: Finished task 35.0 in stage 3.0 (TID 25) in 1544 ms on 192.168.1.9 (executor 1) (23/200)
20/12/04 16:13:53 INFO TaskSetManager: Starting task 37.0 in stage 3.0 (TID 27, 192.168.1.9, executor 1, partition 37, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:53 INFO TaskSetManager: Finished task 36.0 in stage 3.0 (TID 26) in 945 ms on 192.168.1.9 (executor 1) (24/200)
20/12/04 16:13:54 INFO TaskSetManager: Starting task 41.0 in stage 3.0 (TID 28, 192.168.1.9, executor 1, partition 41, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:54 INFO TaskSetManager: Finished task 37.0 in stage 3.0 (TID 27) in 880 ms on 192.168.1.9 (executor 1) (25/200)
20/12/04 16:13:55 INFO TaskSetManager: Starting task 43.0 in stage 3.0 (TID 29, 192.168.1.9, executor 1, partition 43, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:55 INFO TaskSetManager: Finished task 41.0 in stage 3.0 (TID 28) in 792 ms on 192.168.1.9 (executor 1) (26/200)
20/12/04 16:13:56 INFO TaskSetManager: Finished task 43.0 in stage 3.0 (TID 29) in 1346 ms on 192.168.1.9 (executor 1) (27/200)
20/12/04 16:13:56 INFO TaskSetManager: Starting task 44.0 in stage 3.0 (TID 30, 192.168.1.9, executor 1, partition 44, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:57 INFO TaskSetManager: Starting task 48.0 in stage 3.0 (TID 31, 192.168.1.9, executor 1, partition 48, NODE_LOCAL, 7336 bytes)
20/12/04 16:13:57 INFO TaskSetManager: Finished task 44.0 in stage 3.0 (TID 30) in 774 ms on 192.168.1.9 (executor 1) (28/200)
20/12/04 16:13:58 INFO TaskSetManager: Finished task 48.0 in stage 3.0 (TID 31) in 948 ms on 192.168.1.9 (executor 1) (29/200)
20/12/04 16:13:58 INFO TaskSetManager: Starting task 49.0 in stage 3.0 (TID 32, 192.168.1.9, executor 1, partition 49, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:00 INFO TaskSetManager: Starting task 51.0 in stage 3.0 (TID 33, 192.168.1.9, executor 1, partition 51, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:00 INFO TaskSetManager: Finished task 49.0 in stage 3.0 (TID 32) in 2159 ms on 192.168.1.9 (executor 1) (30/200)
20/12/04 16:14:02 INFO TaskSetManager: Starting task 53.0 in stage 3.0 (TID 34, 192.168.1.9, executor 1, partition 53, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:02 INFO TaskSetManager: Finished task 51.0 in stage 3.0 (TID 33) in 2352 ms on 192.168.1.9 (executor 1) (31/200)
20/12/04 16:14:05 INFO TaskSetManager: Starting task 55.0 in stage 3.0 (TID 35, 192.168.1.9, executor 1, partition 55, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:05 INFO TaskSetManager: Finished task 53.0 in stage 3.0 (TID 34) in 2225 ms on 192.168.1.9 (executor 1) (32/200)
20/12/04 16:14:05 INFO TaskSetManager: Starting task 57.0 in stage 3.0 (TID 36, 192.168.1.9, executor 1, partition 57, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:05 INFO TaskSetManager: Finished task 55.0 in stage 3.0 (TID 35) in 721 ms on 192.168.1.9 (executor 1) (33/200)
20/12/04 16:14:06 INFO TaskSetManager: Starting task 58.0 in stage 3.0 (TID 37, 192.168.1.9, executor 1, partition 58, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:06 INFO TaskSetManager: Finished task 57.0 in stage 3.0 (TID 36) in 1069 ms on 192.168.1.9 (executor 1) (34/200)
20/12/04 16:14:07 INFO TaskSetManager: Starting task 61.0 in stage 3.0 (TID 38, 192.168.1.9, executor 1, partition 61, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:07 INFO TaskSetManager: Finished task 58.0 in stage 3.0 (TID 37) in 887 ms on 192.168.1.9 (executor 1) (35/200)
20/12/04 16:14:10 INFO TaskSetManager: Starting task 63.0 in stage 3.0 (TID 39, 192.168.1.9, executor 1, partition 63, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:10 INFO TaskSetManager: Finished task 61.0 in stage 3.0 (TID 38) in 2306 ms on 192.168.1.9 (executor 1) (36/200)
20/12/04 16:14:11 INFO TaskSetManager: Starting task 65.0 in stage 3.0 (TID 40, 192.168.1.9, executor 1, partition 65, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:11 INFO TaskSetManager: Finished task 63.0 in stage 3.0 (TID 39) in 859 ms on 192.168.1.9 (executor 1) (37/200)
20/12/04 16:14:12 INFO TaskSetManager: Starting task 66.0 in stage 3.0 (TID 41, 192.168.1.9, executor 1, partition 66, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:12 INFO TaskSetManager: Finished task 65.0 in stage 3.0 (TID 40) in 1245 ms on 192.168.1.9 (executor 1) (38/200)
20/12/04 16:14:13 INFO TaskSetManager: Starting task 69.0 in stage 3.0 (TID 42, 192.168.1.9, executor 1, partition 69, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:13 INFO TaskSetManager: Finished task 66.0 in stage 3.0 (TID 41) in 865 ms on 192.168.1.9 (executor 1) (39/200)
20/12/04 16:14:14 INFO TaskSetManager: Starting task 70.0 in stage 3.0 (TID 43, 192.168.1.9, executor 1, partition 70, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:14 INFO TaskSetManager: Finished task 69.0 in stage 3.0 (TID 42) in 1085 ms on 192.168.1.9 (executor 1) (40/200)
20/12/04 16:14:15 INFO TaskSetManager: Starting task 73.0 in stage 3.0 (TID 44, 192.168.1.9, executor 1, partition 73, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:15 INFO TaskSetManager: Finished task 70.0 in stage 3.0 (TID 43) in 1387 ms on 192.168.1.9 (executor 1) (41/200)
20/12/04 16:14:16 INFO TaskSetManager: Starting task 75.0 in stage 3.0 (TID 45, 192.168.1.9, executor 1, partition 75, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:16 INFO TaskSetManager: Finished task 73.0 in stage 3.0 (TID 44) in 1312 ms on 192.168.1.9 (executor 1) (42/200)
20/12/04 16:14:18 INFO TaskSetManager: Starting task 77.0 in stage 3.0 (TID 46, 192.168.1.9, executor 1, partition 77, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:18 INFO TaskSetManager: Finished task 75.0 in stage 3.0 (TID 45) in 1744 ms on 192.168.1.9 (executor 1) (43/200)
20/12/04 16:14:21 INFO TaskSetManager: Starting task 79.0 in stage 3.0 (TID 47, 192.168.1.9, executor 1, partition 79, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:21 INFO TaskSetManager: Finished task 77.0 in stage 3.0 (TID 46) in 2822 ms on 192.168.1.9 (executor 1) (44/200)
20/12/04 16:14:23 INFO TaskSetManager: Starting task 84.0 in stage 3.0 (TID 48, 192.168.1.9, executor 1, partition 84, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:23 INFO TaskSetManager: Finished task 79.0 in stage 3.0 (TID 47) in 1936 ms on 192.168.1.9 (executor 1) (45/200)
20/12/04 16:14:23 INFO TaskSetManager: Starting task 85.0 in stage 3.0 (TID 49, 192.168.1.9, executor 1, partition 85, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:23 INFO TaskSetManager: Finished task 84.0 in stage 3.0 (TID 48) in 500 ms on 192.168.1.9 (executor 1) (46/200)
20/12/04 16:14:26 INFO TaskSetManager: Starting task 86.0 in stage 3.0 (TID 50, 192.168.1.9, executor 1, partition 86, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:26 INFO TaskSetManager: Finished task 85.0 in stage 3.0 (TID 49) in 2247 ms on 192.168.1.9 (executor 1) (47/200)
20/12/04 16:14:27 INFO TaskSetManager: Starting task 88.0 in stage 3.0 (TID 51, 192.168.1.9, executor 1, partition 88, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:27 INFO TaskSetManager: Finished task 86.0 in stage 3.0 (TID 50) in 1246 ms on 192.168.1.9 (executor 1) (48/200)
20/12/04 16:14:28 INFO TaskSetManager: Starting task 89.0 in stage 3.0 (TID 52, 192.168.1.9, executor 1, partition 89, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:28 INFO TaskSetManager: Finished task 88.0 in stage 3.0 (TID 51) in 607 ms on 192.168.1.9 (executor 1) (49/200)
20/12/04 16:14:29 INFO TaskSetManager: Starting task 91.0 in stage 3.0 (TID 53, 192.168.1.9, executor 1, partition 91, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:29 INFO TaskSetManager: Finished task 89.0 in stage 3.0 (TID 52) in 1769 ms on 192.168.1.9 (executor 1) (50/200)
20/12/04 16:14:30 INFO TaskSetManager: Starting task 95.0 in stage 3.0 (TID 54, 192.168.1.9, executor 1, partition 95, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:30 INFO TaskSetManager: Finished task 91.0 in stage 3.0 (TID 53) in 1142 ms on 192.168.1.9 (executor 1) (51/200)
20/12/04 16:14:32 INFO TaskSetManager: Starting task 101.0 in stage 3.0 (TID 55, 192.168.1.9, executor 1, partition 101, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:32 INFO TaskSetManager: Finished task 95.0 in stage 3.0 (TID 54) in 1732 ms on 192.168.1.9 (executor 1) (52/200)
20/12/04 16:14:33 INFO TaskSetManager: Starting task 102.0 in stage 3.0 (TID 56, 192.168.1.9, executor 1, partition 102, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:33 INFO TaskSetManager: Finished task 101.0 in stage 3.0 (TID 55) in 848 ms on 192.168.1.9 (executor 1) (53/200)
20/12/04 16:14:36 INFO TaskSetManager: Starting task 103.0 in stage 3.0 (TID 57, 192.168.1.9, executor 1, partition 103, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:36 INFO TaskSetManager: Finished task 102.0 in stage 3.0 (TID 56) in 3300 ms on 192.168.1.9 (executor 1) (54/200)
20/12/04 16:14:38 INFO TaskSetManager: Starting task 105.0 in stage 3.0 (TID 58, 192.168.1.9, executor 1, partition 105, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:38 INFO TaskSetManager: Finished task 103.0 in stage 3.0 (TID 57) in 1807 ms on 192.168.1.9 (executor 1) (55/200)
20/12/04 16:14:40 INFO TaskSetManager: Starting task 106.0 in stage 3.0 (TID 59, 192.168.1.9, executor 1, partition 106, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:40 INFO TaskSetManager: Finished task 105.0 in stage 3.0 (TID 58) in 2116 ms on 192.168.1.9 (executor 1) (56/200)
20/12/04 16:14:41 INFO TaskSetManager: Starting task 107.0 in stage 3.0 (TID 60, 192.168.1.9, executor 1, partition 107, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:41 INFO TaskSetManager: Finished task 106.0 in stage 3.0 (TID 59) in 597 ms on 192.168.1.9 (executor 1) (57/200)
20/12/04 16:14:42 INFO TaskSetManager: Starting task 108.0 in stage 3.0 (TID 61, 192.168.1.9, executor 1, partition 108, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:42 INFO TaskSetManager: Finished task 107.0 in stage 3.0 (TID 60) in 748 ms on 192.168.1.9 (executor 1) (58/200)
20/12/04 16:14:42 INFO TaskSetManager: Starting task 109.0 in stage 3.0 (TID 62, 192.168.1.9, executor 1, partition 109, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:42 INFO TaskSetManager: Finished task 108.0 in stage 3.0 (TID 61) in 786 ms on 192.168.1.9 (executor 1) (59/200)
20/12/04 16:14:45 INFO TaskSetManager: Starting task 111.0 in stage 3.0 (TID 63, 192.168.1.9, executor 1, partition 111, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:45 INFO TaskSetManager: Finished task 109.0 in stage 3.0 (TID 62) in 2179 ms on 192.168.1.9 (executor 1) (60/200)
20/12/04 16:14:46 INFO TaskSetManager: Starting task 113.0 in stage 3.0 (TID 64, 192.168.1.9, executor 1, partition 113, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:46 INFO TaskSetManager: Finished task 111.0 in stage 3.0 (TID 63) in 1525 ms on 192.168.1.9 (executor 1) (61/200)
20/12/04 16:14:47 INFO TaskSetManager: Starting task 115.0 in stage 3.0 (TID 65, 192.168.1.9, executor 1, partition 115, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:47 INFO TaskSetManager: Finished task 113.0 in stage 3.0 (TID 64) in 894 ms on 192.168.1.9 (executor 1) (62/200)
20/12/04 16:14:48 INFO TaskSetManager: Starting task 116.0 in stage 3.0 (TID 66, 192.168.1.9, executor 1, partition 116, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:48 INFO TaskSetManager: Finished task 115.0 in stage 3.0 (TID 65) in 1380 ms on 192.168.1.9 (executor 1) (63/200)
20/12/04 16:14:49 INFO TaskSetManager: Starting task 119.0 in stage 3.0 (TID 67, 192.168.1.9, executor 1, partition 119, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:49 INFO TaskSetManager: Finished task 116.0 in stage 3.0 (TID 66) in 501 ms on 192.168.1.9 (executor 1) (64/200)
20/12/04 16:14:50 INFO TaskSetManager: Starting task 122.0 in stage 3.0 (TID 68, 192.168.1.9, executor 1, partition 122, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:50 INFO TaskSetManager: Finished task 119.0 in stage 3.0 (TID 67) in 1215 ms on 192.168.1.9 (executor 1) (65/200)
20/12/04 16:14:52 INFO TaskSetManager: Starting task 124.0 in stage 3.0 (TID 69, 192.168.1.9, executor 1, partition 124, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:52 INFO TaskSetManager: Finished task 122.0 in stage 3.0 (TID 68) in 2333 ms on 192.168.1.9 (executor 1) (66/200)
20/12/04 16:14:53 INFO TaskSetManager: Starting task 126.0 in stage 3.0 (TID 70, 192.168.1.9, executor 1, partition 126, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:53 INFO TaskSetManager: Finished task 124.0 in stage 3.0 (TID 69) in 749 ms on 192.168.1.9 (executor 1) (67/200)
20/12/04 16:14:54 INFO TaskSetManager: Starting task 128.0 in stage 3.0 (TID 71, 192.168.1.9, executor 1, partition 128, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:54 INFO TaskSetManager: Finished task 126.0 in stage 3.0 (TID 70) in 802 ms on 192.168.1.9 (executor 1) (68/200)
20/12/04 16:14:55 INFO TaskSetManager: Starting task 129.0 in stage 3.0 (TID 72, 192.168.1.9, executor 1, partition 129, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:55 INFO TaskSetManager: Finished task 128.0 in stage 3.0 (TID 71) in 673 ms on 192.168.1.9 (executor 1) (69/200)
20/12/04 16:14:56 INFO TaskSetManager: Starting task 131.0 in stage 3.0 (TID 73, 192.168.1.9, executor 1, partition 131, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:56 INFO TaskSetManager: Finished task 129.0 in stage 3.0 (TID 72) in 1046 ms on 192.168.1.9 (executor 1) (70/200)
20/12/04 16:14:56 INFO TaskSetManager: Starting task 132.0 in stage 3.0 (TID 74, 192.168.1.9, executor 1, partition 132, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:56 INFO TaskSetManager: Finished task 131.0 in stage 3.0 (TID 73) in 678 ms on 192.168.1.9 (executor 1) (71/200)
20/12/04 16:14:58 INFO TaskSetManager: Starting task 133.0 in stage 3.0 (TID 75, 192.168.1.9, executor 1, partition 133, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:58 INFO TaskSetManager: Finished task 132.0 in stage 3.0 (TID 74) in 1277 ms on 192.168.1.9 (executor 1) (72/200)
20/12/04 16:14:58 INFO TaskSetManager: Starting task 135.0 in stage 3.0 (TID 76, 192.168.1.9, executor 1, partition 135, NODE_LOCAL, 7336 bytes)
20/12/04 16:14:58 INFO TaskSetManager: Finished task 133.0 in stage 3.0 (TID 75) in 893 ms on 192.168.1.9 (executor 1) (73/200)
20/12/04 16:15:01 INFO TaskSetManager: Starting task 136.0 in stage 3.0 (TID 77, 192.168.1.9, executor 1, partition 136, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:01 INFO TaskSetManager: Finished task 135.0 in stage 3.0 (TID 76) in 2389 ms on 192.168.1.9 (executor 1) (74/200)
20/12/04 16:15:02 INFO TaskSetManager: Starting task 137.0 in stage 3.0 (TID 78, 192.168.1.9, executor 1, partition 137, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:02 INFO TaskSetManager: Finished task 136.0 in stage 3.0 (TID 77) in 712 ms on 192.168.1.9 (executor 1) (75/200)
20/12/04 16:15:04 INFO TaskSetManager: Starting task 139.0 in stage 3.0 (TID 79, 192.168.1.9, executor 1, partition 139, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:05 INFO TaskSetManager: Finished task 137.0 in stage 3.0 (TID 78) in 2919 ms on 192.168.1.9 (executor 1) (76/200)
20/12/04 16:15:05 INFO TaskSetManager: Starting task 140.0 in stage 3.0 (TID 80, 192.168.1.9, executor 1, partition 140, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:05 INFO TaskSetManager: Finished task 139.0 in stage 3.0 (TID 79) in 695 ms on 192.168.1.9 (executor 1) (77/200)
20/12/04 16:15:07 INFO TaskSetManager: Starting task 141.0 in stage 3.0 (TID 81, 192.168.1.9, executor 1, partition 141, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:07 INFO TaskSetManager: Finished task 140.0 in stage 3.0 (TID 80) in 1486 ms on 192.168.1.9 (executor 1) (78/200)
20/12/04 16:15:07 INFO TaskSetManager: Starting task 143.0 in stage 3.0 (TID 82, 192.168.1.9, executor 1, partition 143, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:07 INFO TaskSetManager: Finished task 141.0 in stage 3.0 (TID 81) in 563 ms on 192.168.1.9 (executor 1) (79/200)
20/12/04 16:15:08 INFO TaskSetManager: Starting task 146.0 in stage 3.0 (TID 83, 192.168.1.9, executor 1, partition 146, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:08 INFO TaskSetManager: Finished task 143.0 in stage 3.0 (TID 82) in 887 ms on 192.168.1.9 (executor 1) (80/200)
20/12/04 16:15:09 INFO TaskSetManager: Starting task 150.0 in stage 3.0 (TID 84, 192.168.1.9, executor 1, partition 150, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:09 INFO TaskSetManager: Finished task 146.0 in stage 3.0 (TID 83) in 1273 ms on 192.168.1.9 (executor 1) (81/200)
20/12/04 16:15:12 INFO TaskSetManager: Starting task 151.0 in stage 3.0 (TID 85, 192.168.1.9, executor 1, partition 151, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:12 INFO TaskSetManager: Finished task 150.0 in stage 3.0 (TID 84) in 3091 ms on 192.168.1.9 (executor 1) (82/200)
20/12/04 16:15:14 INFO TaskSetManager: Starting task 153.0 in stage 3.0 (TID 86, 192.168.1.9, executor 1, partition 153, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:14 INFO TaskSetManager: Finished task 151.0 in stage 3.0 (TID 85) in 1487 ms on 192.168.1.9 (executor 1) (83/200)
20/12/04 16:15:15 INFO TaskSetManager: Starting task 155.0 in stage 3.0 (TID 87, 192.168.1.9, executor 1, partition 155, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:15 INFO TaskSetManager: Finished task 153.0 in stage 3.0 (TID 86) in 1418 ms on 192.168.1.9 (executor 1) (84/200)
20/12/04 16:15:16 INFO TaskSetManager: Starting task 156.0 in stage 3.0 (TID 88, 192.168.1.9, executor 1, partition 156, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:16 INFO TaskSetManager: Finished task 155.0 in stage 3.0 (TID 87) in 814 ms on 192.168.1.9 (executor 1) (85/200)
20/12/04 16:15:18 INFO TaskSetManager: Starting task 161.0 in stage 3.0 (TID 89, 192.168.1.9, executor 1, partition 161, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:18 INFO TaskSetManager: Finished task 156.0 in stage 3.0 (TID 88) in 1740 ms on 192.168.1.9 (executor 1) (86/200)
20/12/04 16:15:19 INFO TaskSetManager: Starting task 162.0 in stage 3.0 (TID 90, 192.168.1.9, executor 1, partition 162, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:19 INFO TaskSetManager: Finished task 161.0 in stage 3.0 (TID 89) in 1379 ms on 192.168.1.9 (executor 1) (87/200)
20/12/04 16:15:21 INFO TaskSetManager: Starting task 163.0 in stage 3.0 (TID 91, 192.168.1.9, executor 1, partition 163, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:21 INFO TaskSetManager: Finished task 162.0 in stage 3.0 (TID 90) in 1915 ms on 192.168.1.9 (executor 1) (88/200)
20/12/04 16:15:24 INFO TaskSetManager: Starting task 164.0 in stage 3.0 (TID 92, 192.168.1.9, executor 1, partition 164, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:24 INFO TaskSetManager: Finished task 163.0 in stage 3.0 (TID 91) in 2274 ms on 192.168.1.9 (executor 1) (89/200)
20/12/04 16:15:28 INFO TaskSetManager: Starting task 165.0 in stage 3.0 (TID 93, 192.168.1.9, executor 1, partition 165, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:28 INFO TaskSetManager: Finished task 164.0 in stage 3.0 (TID 92) in 4378 ms on 192.168.1.9 (executor 1) (90/200)
20/12/04 16:15:30 INFO TaskSetManager: Starting task 167.0 in stage 3.0 (TID 94, 192.168.1.9, executor 1, partition 167, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:30 INFO TaskSetManager: Finished task 165.0 in stage 3.0 (TID 93) in 2068 ms on 192.168.1.9 (executor 1) (91/200)
20/12/04 16:15:32 INFO TaskSetManager: Starting task 168.0 in stage 3.0 (TID 95, 192.168.1.9, executor 1, partition 168, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:32 INFO TaskSetManager: Finished task 167.0 in stage 3.0 (TID 94) in 2169 ms on 192.168.1.9 (executor 1) (92/200)
20/12/04 16:15:33 INFO TaskSetManager: Starting task 170.0 in stage 3.0 (TID 96, 192.168.1.9, executor 1, partition 170, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:33 INFO TaskSetManager: Finished task 168.0 in stage 3.0 (TID 95) in 540 ms on 192.168.1.9 (executor 1) (93/200)
20/12/04 16:15:34 INFO TaskSetManager: Starting task 172.0 in stage 3.0 (TID 97, 192.168.1.9, executor 1, partition 172, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:34 INFO TaskSetManager: Finished task 170.0 in stage 3.0 (TID 96) in 1140 ms on 192.168.1.9 (executor 1) (94/200)
20/12/04 16:15:35 INFO TaskSetManager: Starting task 173.0 in stage 3.0 (TID 98, 192.168.1.9, executor 1, partition 173, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:35 INFO TaskSetManager: Finished task 172.0 in stage 3.0 (TID 97) in 1046 ms on 192.168.1.9 (executor 1) (95/200)
20/12/04 16:15:37 INFO TaskSetManager: Starting task 174.0 in stage 3.0 (TID 99, 192.168.1.9, executor 1, partition 174, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:37 INFO TaskSetManager: Finished task 173.0 in stage 3.0 (TID 98) in 2365 ms on 192.168.1.9 (executor 1) (96/200)
20/12/04 16:15:39 INFO TaskSetManager: Starting task 175.0 in stage 3.0 (TID 100, 192.168.1.9, executor 1, partition 175, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:39 INFO TaskSetManager: Finished task 174.0 in stage 3.0 (TID 99) in 1942 ms on 192.168.1.9 (executor 1) (97/200)
20/12/04 16:15:41 INFO TaskSetManager: Starting task 178.0 in stage 3.0 (TID 101, 192.168.1.9, executor 1, partition 178, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:41 INFO TaskSetManager: Finished task 175.0 in stage 3.0 (TID 100) in 1753 ms on 192.168.1.9 (executor 1) (98/200)
20/12/04 16:15:42 INFO TaskSetManager: Starting task 179.0 in stage 3.0 (TID 102, 192.168.1.9, executor 1, partition 179, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:42 INFO TaskSetManager: Finished task 178.0 in stage 3.0 (TID 101) in 743 ms on 192.168.1.9 (executor 1) (99/200)
20/12/04 16:15:43 INFO TaskSetManager: Starting task 181.0 in stage 3.0 (TID 103, 192.168.1.9, executor 1, partition 181, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:43 INFO TaskSetManager: Finished task 179.0 in stage 3.0 (TID 102) in 1184 ms on 192.168.1.9 (executor 1) (100/200)
20/12/04 16:15:45 INFO TaskSetManager: Starting task 183.0 in stage 3.0 (TID 104, 192.168.1.9, executor 1, partition 183, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:45 INFO TaskSetManager: Finished task 181.0 in stage 3.0 (TID 103) in 1891 ms on 192.168.1.9 (executor 1) (101/200)
20/12/04 16:15:45 INFO TaskSetManager: Starting task 184.0 in stage 3.0 (TID 105, 192.168.1.9, executor 1, partition 184, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:45 INFO TaskSetManager: Finished task 183.0 in stage 3.0 (TID 104) in 655 ms on 192.168.1.9 (executor 1) (102/200)
20/12/04 16:15:47 INFO TaskSetManager: Starting task 190.0 in stage 3.0 (TID 106, 192.168.1.9, executor 1, partition 190, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:47 INFO TaskSetManager: Finished task 184.0 in stage 3.0 (TID 105) in 1805 ms on 192.168.1.9 (executor 1) (103/200)
20/12/04 16:15:48 INFO TaskSetManager: Starting task 192.0 in stage 3.0 (TID 107, 192.168.1.9, executor 1, partition 192, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:48 INFO TaskSetManager: Finished task 190.0 in stage 3.0 (TID 106) in 1185 ms on 192.168.1.9 (executor 1) (104/200)
20/12/04 16:15:49 INFO TaskSetManager: Starting task 193.0 in stage 3.0 (TID 108, 192.168.1.9, executor 1, partition 193, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:49 INFO TaskSetManager: Finished task 192.0 in stage 3.0 (TID 107) in 539 ms on 192.168.1.9 (executor 1) (105/200)
20/12/04 16:15:51 INFO TaskSetManager: Starting task 194.0 in stage 3.0 (TID 109, 192.168.1.9, executor 1, partition 194, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:51 INFO TaskSetManager: Finished task 193.0 in stage 3.0 (TID 108) in 1882 ms on 192.168.1.9 (executor 1) (106/200)
20/12/04 16:15:52 INFO TaskSetManager: Starting task 195.0 in stage 3.0 (TID 110, 192.168.1.9, executor 1, partition 195, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:52 INFO TaskSetManager: Finished task 194.0 in stage 3.0 (TID 109) in 907 ms on 192.168.1.9 (executor 1) (107/200)
20/12/04 16:15:53 INFO TaskSetManager: Starting task 198.0 in stage 3.0 (TID 111, 192.168.1.9, executor 1, partition 198, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:53 INFO TaskSetManager: Finished task 195.0 in stage 3.0 (TID 110) in 988 ms on 192.168.1.9 (executor 1) (108/200)
20/12/04 16:15:53 INFO TaskSetManager: Starting task 199.0 in stage 3.0 (TID 112, 192.168.1.9, executor 1, partition 199, NODE_LOCAL, 7336 bytes)
20/12/04 16:15:53 INFO TaskSetManager: Finished task 198.0 in stage 3.0 (TID 111) in 503 ms on 192.168.1.9 (executor 1) (109/200)
20/12/04 16:15:54 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 113, 192.168.1.9, executor 1, partition 1, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:54 INFO TaskSetManager: Finished task 199.0 in stage 3.0 (TID 112) in 1196 ms on 192.168.1.9 (executor 1) (110/200)
20/12/04 16:15:54 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 114, 192.168.1.9, executor 1, partition 8, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:54 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 113) in 119 ms on 192.168.1.9 (executor 1) (111/200)
20/12/04 16:15:55 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 115, 192.168.1.9, executor 1, partition 9, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:55 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 114) in 127 ms on 192.168.1.9 (executor 1) (112/200)
20/12/04 16:15:55 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 116, 192.168.1.9, executor 1, partition 15, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:55 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 115) in 112 ms on 192.168.1.9 (executor 1) (113/200)
20/12/04 16:15:55 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 117, 192.168.1.9, executor 1, partition 16, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:55 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 116) in 114 ms on 192.168.1.9 (executor 1) (114/200)
20/12/04 16:15:55 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 118, 192.168.1.9, executor 1, partition 17, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:55 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 117) in 115 ms on 192.168.1.9 (executor 1) (115/200)
20/12/04 16:15:55 INFO TaskSetManager: Starting task 20.0 in stage 3.0 (TID 119, 192.168.1.9, executor 1, partition 20, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:55 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 118) in 110 ms on 192.168.1.9 (executor 1) (116/200)
20/12/04 16:15:55 INFO TaskSetManager: Starting task 22.0 in stage 3.0 (TID 120, 192.168.1.9, executor 1, partition 22, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:55 INFO TaskSetManager: Finished task 20.0 in stage 3.0 (TID 119) in 120 ms on 192.168.1.9 (executor 1) (117/200)
20/12/04 16:15:55 INFO TaskSetManager: Starting task 25.0 in stage 3.0 (TID 121, 192.168.1.9, executor 1, partition 25, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:55 INFO TaskSetManager: Finished task 22.0 in stage 3.0 (TID 120) in 110 ms on 192.168.1.9 (executor 1) (118/200)
20/12/04 16:15:55 INFO TaskSetManager: Starting task 26.0 in stage 3.0 (TID 122, 192.168.1.9, executor 1, partition 26, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:55 INFO TaskSetManager: Finished task 25.0 in stage 3.0 (TID 121) in 110 ms on 192.168.1.9 (executor 1) (119/200)
20/12/04 16:15:56 INFO TaskSetManager: Starting task 28.0 in stage 3.0 (TID 123, 192.168.1.9, executor 1, partition 28, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:56 INFO TaskSetManager: Finished task 26.0 in stage 3.0 (TID 122) in 117 ms on 192.168.1.9 (executor 1) (120/200)
20/12/04 16:15:56 INFO TaskSetManager: Starting task 29.0 in stage 3.0 (TID 124, 192.168.1.9, executor 1, partition 29, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:56 INFO TaskSetManager: Finished task 28.0 in stage 3.0 (TID 123) in 113 ms on 192.168.1.9 (executor 1) (121/200)
20/12/04 16:15:56 INFO TaskSetManager: Starting task 33.0 in stage 3.0 (TID 125, 192.168.1.9, executor 1, partition 33, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:56 INFO TaskSetManager: Finished task 29.0 in stage 3.0 (TID 124) in 116 ms on 192.168.1.9 (executor 1) (122/200)
20/12/04 16:15:56 INFO TaskSetManager: Starting task 38.0 in stage 3.0 (TID 126, 192.168.1.9, executor 1, partition 38, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:56 INFO TaskSetManager: Finished task 33.0 in stage 3.0 (TID 125) in 134 ms on 192.168.1.9 (executor 1) (123/200)
20/12/04 16:15:56 INFO TaskSetManager: Starting task 39.0 in stage 3.0 (TID 127, 192.168.1.9, executor 1, partition 39, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:56 INFO TaskSetManager: Finished task 38.0 in stage 3.0 (TID 126) in 112 ms on 192.168.1.9 (executor 1) (124/200)
20/12/04 16:15:56 INFO TaskSetManager: Starting task 40.0 in stage 3.0 (TID 128, 192.168.1.9, executor 1, partition 40, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:56 INFO TaskSetManager: Finished task 39.0 in stage 3.0 (TID 127) in 113 ms on 192.168.1.9 (executor 1) (125/200)
20/12/04 16:15:56 INFO TaskSetManager: Starting task 42.0 in stage 3.0 (TID 129, 192.168.1.9, executor 1, partition 42, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:56 INFO TaskSetManager: Finished task 40.0 in stage 3.0 (TID 128) in 123 ms on 192.168.1.9 (executor 1) (126/200)
20/12/04 16:15:56 INFO TaskSetManager: Starting task 45.0 in stage 3.0 (TID 130, 192.168.1.9, executor 1, partition 45, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:56 INFO TaskSetManager: Finished task 42.0 in stage 3.0 (TID 129) in 112 ms on 192.168.1.9 (executor 1) (127/200)
20/12/04 16:15:56 INFO TaskSetManager: Starting task 46.0 in stage 3.0 (TID 131, 192.168.1.9, executor 1, partition 46, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:56 INFO TaskSetManager: Finished task 45.0 in stage 3.0 (TID 130) in 172 ms on 192.168.1.9 (executor 1) (128/200)
20/12/04 16:15:57 INFO TaskSetManager: Starting task 47.0 in stage 3.0 (TID 132, 192.168.1.9, executor 1, partition 47, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:57 INFO TaskSetManager: Finished task 46.0 in stage 3.0 (TID 131) in 112 ms on 192.168.1.9 (executor 1) (129/200)
20/12/04 16:15:57 INFO TaskSetManager: Starting task 50.0 in stage 3.0 (TID 133, 192.168.1.9, executor 1, partition 50, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:57 INFO TaskSetManager: Finished task 47.0 in stage 3.0 (TID 132) in 110 ms on 192.168.1.9 (executor 1) (130/200)
20/12/04 16:15:57 INFO TaskSetManager: Starting task 52.0 in stage 3.0 (TID 134, 192.168.1.9, executor 1, partition 52, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:57 INFO TaskSetManager: Finished task 50.0 in stage 3.0 (TID 133) in 119 ms on 192.168.1.9 (executor 1) (131/200)
20/12/04 16:15:57 INFO TaskSetManager: Starting task 54.0 in stage 3.0 (TID 135, 192.168.1.9, executor 1, partition 54, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:57 INFO TaskSetManager: Finished task 52.0 in stage 3.0 (TID 134) in 108 ms on 192.168.1.9 (executor 1) (132/200)
20/12/04 16:15:57 INFO TaskSetManager: Starting task 56.0 in stage 3.0 (TID 136, 192.168.1.9, executor 1, partition 56, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:57 INFO TaskSetManager: Finished task 54.0 in stage 3.0 (TID 135) in 113 ms on 192.168.1.9 (executor 1) (133/200)
20/12/04 16:15:57 INFO TaskSetManager: Starting task 59.0 in stage 3.0 (TID 137, 192.168.1.9, executor 1, partition 59, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:57 INFO TaskSetManager: Finished task 56.0 in stage 3.0 (TID 136) in 116 ms on 192.168.1.9 (executor 1) (134/200)
20/12/04 16:15:57 INFO TaskSetManager: Starting task 60.0 in stage 3.0 (TID 138, 192.168.1.9, executor 1, partition 60, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:57 INFO TaskSetManager: Finished task 59.0 in stage 3.0 (TID 137) in 110 ms on 192.168.1.9 (executor 1) (135/200)
20/12/04 16:15:57 INFO TaskSetManager: Starting task 62.0 in stage 3.0 (TID 139, 192.168.1.9, executor 1, partition 62, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:57 INFO TaskSetManager: Finished task 60.0 in stage 3.0 (TID 138) in 115 ms on 192.168.1.9 (executor 1) (136/200)
20/12/04 16:15:58 INFO TaskSetManager: Starting task 64.0 in stage 3.0 (TID 140, 192.168.1.9, executor 1, partition 64, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:58 INFO TaskSetManager: Finished task 62.0 in stage 3.0 (TID 139) in 109 ms on 192.168.1.9 (executor 1) (137/200)
20/12/04 16:15:58 INFO TaskSetManager: Starting task 67.0 in stage 3.0 (TID 141, 192.168.1.9, executor 1, partition 67, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:58 INFO TaskSetManager: Finished task 64.0 in stage 3.0 (TID 140) in 110 ms on 192.168.1.9 (executor 1) (138/200)
20/12/04 16:15:58 INFO TaskSetManager: Starting task 68.0 in stage 3.0 (TID 142, 192.168.1.9, executor 1, partition 68, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:58 INFO TaskSetManager: Finished task 67.0 in stage 3.0 (TID 141) in 109 ms on 192.168.1.9 (executor 1) (139/200)
20/12/04 16:15:58 INFO TaskSetManager: Starting task 71.0 in stage 3.0 (TID 143, 192.168.1.9, executor 1, partition 71, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:58 INFO TaskSetManager: Finished task 68.0 in stage 3.0 (TID 142) in 109 ms on 192.168.1.9 (executor 1) (140/200)
20/12/04 16:15:58 INFO TaskSetManager: Starting task 72.0 in stage 3.0 (TID 144, 192.168.1.9, executor 1, partition 72, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:58 INFO TaskSetManager: Finished task 71.0 in stage 3.0 (TID 143) in 114 ms on 192.168.1.9 (executor 1) (141/200)
20/12/04 16:15:58 INFO TaskSetManager: Starting task 74.0 in stage 3.0 (TID 145, 192.168.1.9, executor 1, partition 74, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:58 INFO TaskSetManager: Finished task 72.0 in stage 3.0 (TID 144) in 115 ms on 192.168.1.9 (executor 1) (142/200)
20/12/04 16:15:58 INFO TaskSetManager: Starting task 76.0 in stage 3.0 (TID 146, 192.168.1.9, executor 1, partition 76, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:58 INFO TaskSetManager: Finished task 74.0 in stage 3.0 (TID 145) in 109 ms on 192.168.1.9 (executor 1) (143/200)
20/12/04 16:15:58 INFO TaskSetManager: Starting task 78.0 in stage 3.0 (TID 147, 192.168.1.9, executor 1, partition 78, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:58 INFO TaskSetManager: Finished task 76.0 in stage 3.0 (TID 146) in 111 ms on 192.168.1.9 (executor 1) (144/200)
20/12/04 16:15:58 INFO TaskSetManager: Starting task 80.0 in stage 3.0 (TID 148, 192.168.1.9, executor 1, partition 80, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:58 INFO TaskSetManager: Finished task 78.0 in stage 3.0 (TID 147) in 115 ms on 192.168.1.9 (executor 1) (145/200)
20/12/04 16:15:58 INFO TaskSetManager: Starting task 81.0 in stage 3.0 (TID 149, 192.168.1.9, executor 1, partition 81, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:58 INFO TaskSetManager: Finished task 80.0 in stage 3.0 (TID 148) in 111 ms on 192.168.1.9 (executor 1) (146/200)
20/12/04 16:15:59 INFO TaskSetManager: Starting task 82.0 in stage 3.0 (TID 150, 192.168.1.9, executor 1, partition 82, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:59 INFO TaskSetManager: Finished task 81.0 in stage 3.0 (TID 149) in 108 ms on 192.168.1.9 (executor 1) (147/200)
20/12/04 16:15:59 INFO TaskSetManager: Starting task 83.0 in stage 3.0 (TID 151, 192.168.1.9, executor 1, partition 83, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:59 INFO TaskSetManager: Finished task 82.0 in stage 3.0 (TID 150) in 111 ms on 192.168.1.9 (executor 1) (148/200)
20/12/04 16:15:59 INFO TaskSetManager: Starting task 87.0 in stage 3.0 (TID 152, 192.168.1.9, executor 1, partition 87, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:59 INFO TaskSetManager: Finished task 83.0 in stage 3.0 (TID 151) in 108 ms on 192.168.1.9 (executor 1) (149/200)
20/12/04 16:15:59 INFO TaskSetManager: Starting task 90.0 in stage 3.0 (TID 153, 192.168.1.9, executor 1, partition 90, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:59 INFO TaskSetManager: Finished task 87.0 in stage 3.0 (TID 152) in 109 ms on 192.168.1.9 (executor 1) (150/200)
20/12/04 16:15:59 INFO TaskSetManager: Starting task 92.0 in stage 3.0 (TID 154, 192.168.1.9, executor 1, partition 92, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:59 INFO TaskSetManager: Finished task 90.0 in stage 3.0 (TID 153) in 109 ms on 192.168.1.9 (executor 1) (151/200)
20/12/04 16:15:59 INFO TaskSetManager: Starting task 93.0 in stage 3.0 (TID 155, 192.168.1.9, executor 1, partition 93, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:59 INFO TaskSetManager: Finished task 92.0 in stage 3.0 (TID 154) in 108 ms on 192.168.1.9 (executor 1) (152/200)
20/12/04 16:15:59 INFO TaskSetManager: Starting task 94.0 in stage 3.0 (TID 156, 192.168.1.9, executor 1, partition 94, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:59 INFO TaskSetManager: Finished task 93.0 in stage 3.0 (TID 155) in 123 ms on 192.168.1.9 (executor 1) (153/200)
20/12/04 16:15:59 INFO TaskSetManager: Starting task 96.0 in stage 3.0 (TID 157, 192.168.1.9, executor 1, partition 96, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:59 INFO TaskSetManager: Finished task 94.0 in stage 3.0 (TID 156) in 109 ms on 192.168.1.9 (executor 1) (154/200)
20/12/04 16:15:59 INFO TaskSetManager: Starting task 97.0 in stage 3.0 (TID 158, 192.168.1.9, executor 1, partition 97, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:15:59 INFO TaskSetManager: Finished task 96.0 in stage 3.0 (TID 157) in 109 ms on 192.168.1.9 (executor 1) (155/200)
20/12/04 16:16:00 INFO TaskSetManager: Starting task 98.0 in stage 3.0 (TID 159, 192.168.1.9, executor 1, partition 98, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:00 INFO TaskSetManager: Finished task 97.0 in stage 3.0 (TID 158) in 112 ms on 192.168.1.9 (executor 1) (156/200)
20/12/04 16:16:00 INFO TaskSetManager: Starting task 99.0 in stage 3.0 (TID 160, 192.168.1.9, executor 1, partition 99, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:00 INFO TaskSetManager: Finished task 98.0 in stage 3.0 (TID 159) in 108 ms on 192.168.1.9 (executor 1) (157/200)
20/12/04 16:16:00 INFO TaskSetManager: Starting task 100.0 in stage 3.0 (TID 161, 192.168.1.9, executor 1, partition 100, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:00 INFO TaskSetManager: Finished task 99.0 in stage 3.0 (TID 160) in 109 ms on 192.168.1.9 (executor 1) (158/200)
20/12/04 16:16:00 INFO TaskSetManager: Starting task 104.0 in stage 3.0 (TID 162, 192.168.1.9, executor 1, partition 104, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:00 INFO TaskSetManager: Finished task 100.0 in stage 3.0 (TID 161) in 118 ms on 192.168.1.9 (executor 1) (159/200)
20/12/04 16:16:00 INFO TaskSetManager: Starting task 110.0 in stage 3.0 (TID 163, 192.168.1.9, executor 1, partition 110, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:00 INFO TaskSetManager: Finished task 104.0 in stage 3.0 (TID 162) in 117 ms on 192.168.1.9 (executor 1) (160/200)
20/12/04 16:16:00 INFO TaskSetManager: Starting task 112.0 in stage 3.0 (TID 164, 192.168.1.9, executor 1, partition 112, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:00 INFO TaskSetManager: Finished task 110.0 in stage 3.0 (TID 163) in 112 ms on 192.168.1.9 (executor 1) (161/200)
20/12/04 16:16:00 INFO TaskSetManager: Starting task 114.0 in stage 3.0 (TID 165, 192.168.1.9, executor 1, partition 114, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:00 INFO TaskSetManager: Finished task 112.0 in stage 3.0 (TID 164) in 112 ms on 192.168.1.9 (executor 1) (162/200)
20/12/04 16:16:00 INFO TaskSetManager: Starting task 117.0 in stage 3.0 (TID 166, 192.168.1.9, executor 1, partition 117, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:00 INFO TaskSetManager: Finished task 114.0 in stage 3.0 (TID 165) in 110 ms on 192.168.1.9 (executor 1) (163/200)
20/12/04 16:16:00 INFO TaskSetManager: Starting task 118.0 in stage 3.0 (TID 167, 192.168.1.9, executor 1, partition 118, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:00 INFO TaskSetManager: Finished task 117.0 in stage 3.0 (TID 166) in 109 ms on 192.168.1.9 (executor 1) (164/200)
20/12/04 16:16:01 INFO TaskSetManager: Starting task 120.0 in stage 3.0 (TID 168, 192.168.1.9, executor 1, partition 120, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:01 INFO TaskSetManager: Finished task 118.0 in stage 3.0 (TID 167) in 126 ms on 192.168.1.9 (executor 1) (165/200)
20/12/04 16:16:01 INFO TaskSetManager: Starting task 121.0 in stage 3.0 (TID 169, 192.168.1.9, executor 1, partition 121, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:01 INFO TaskSetManager: Finished task 120.0 in stage 3.0 (TID 168) in 110 ms on 192.168.1.9 (executor 1) (166/200)
20/12/04 16:16:01 INFO TaskSetManager: Starting task 123.0 in stage 3.0 (TID 170, 192.168.1.9, executor 1, partition 123, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:01 INFO TaskSetManager: Finished task 121.0 in stage 3.0 (TID 169) in 124 ms on 192.168.1.9 (executor 1) (167/200)
20/12/04 16:16:01 INFO TaskSetManager: Starting task 125.0 in stage 3.0 (TID 171, 192.168.1.9, executor 1, partition 125, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:01 INFO TaskSetManager: Finished task 123.0 in stage 3.0 (TID 170) in 107 ms on 192.168.1.9 (executor 1) (168/200)
20/12/04 16:16:01 INFO TaskSetManager: Starting task 127.0 in stage 3.0 (TID 172, 192.168.1.9, executor 1, partition 127, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:01 INFO TaskSetManager: Finished task 125.0 in stage 3.0 (TID 171) in 122 ms on 192.168.1.9 (executor 1) (169/200)
20/12/04 16:16:01 INFO TaskSetManager: Starting task 130.0 in stage 3.0 (TID 173, 192.168.1.9, executor 1, partition 130, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:01 INFO TaskSetManager: Finished task 127.0 in stage 3.0 (TID 172) in 109 ms on 192.168.1.9 (executor 1) (170/200)
20/12/04 16:16:01 INFO TaskSetManager: Starting task 134.0 in stage 3.0 (TID 174, 192.168.1.9, executor 1, partition 134, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:01 INFO TaskSetManager: Finished task 130.0 in stage 3.0 (TID 173) in 109 ms on 192.168.1.9 (executor 1) (171/200)
20/12/04 16:16:01 INFO TaskSetManager: Starting task 138.0 in stage 3.0 (TID 175, 192.168.1.9, executor 1, partition 138, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:01 INFO TaskSetManager: Finished task 134.0 in stage 3.0 (TID 174) in 118 ms on 192.168.1.9 (executor 1) (172/200)
20/12/04 16:16:02 INFO TaskSetManager: Starting task 142.0 in stage 3.0 (TID 176, 192.168.1.9, executor 1, partition 142, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:02 INFO TaskSetManager: Finished task 138.0 in stage 3.0 (TID 175) in 115 ms on 192.168.1.9 (executor 1) (173/200)
20/12/04 16:16:02 INFO TaskSetManager: Starting task 144.0 in stage 3.0 (TID 177, 192.168.1.9, executor 1, partition 144, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:02 INFO TaskSetManager: Finished task 142.0 in stage 3.0 (TID 176) in 109 ms on 192.168.1.9 (executor 1) (174/200)
20/12/04 16:16:02 INFO TaskSetManager: Starting task 145.0 in stage 3.0 (TID 178, 192.168.1.9, executor 1, partition 145, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:02 INFO TaskSetManager: Finished task 144.0 in stage 3.0 (TID 177) in 108 ms on 192.168.1.9 (executor 1) (175/200)
20/12/04 16:16:02 INFO TaskSetManager: Starting task 147.0 in stage 3.0 (TID 179, 192.168.1.9, executor 1, partition 147, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:02 INFO TaskSetManager: Finished task 145.0 in stage 3.0 (TID 178) in 109 ms on 192.168.1.9 (executor 1) (176/200)
20/12/04 16:16:02 INFO TaskSetManager: Starting task 148.0 in stage 3.0 (TID 180, 192.168.1.9, executor 1, partition 148, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:02 INFO TaskSetManager: Finished task 147.0 in stage 3.0 (TID 179) in 107 ms on 192.168.1.9 (executor 1) (177/200)
20/12/04 16:16:02 INFO TaskSetManager: Starting task 149.0 in stage 3.0 (TID 181, 192.168.1.9, executor 1, partition 149, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:02 INFO TaskSetManager: Finished task 148.0 in stage 3.0 (TID 180) in 111 ms on 192.168.1.9 (executor 1) (178/200)
20/12/04 16:16:02 INFO TaskSetManager: Starting task 152.0 in stage 3.0 (TID 182, 192.168.1.9, executor 1, partition 152, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:02 INFO TaskSetManager: Finished task 149.0 in stage 3.0 (TID 181) in 107 ms on 192.168.1.9 (executor 1) (179/200)
20/12/04 16:16:02 INFO TaskSetManager: Starting task 154.0 in stage 3.0 (TID 183, 192.168.1.9, executor 1, partition 154, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:02 INFO TaskSetManager: Finished task 152.0 in stage 3.0 (TID 182) in 108 ms on 192.168.1.9 (executor 1) (180/200)
20/12/04 16:16:02 INFO TaskSetManager: Starting task 157.0 in stage 3.0 (TID 184, 192.168.1.9, executor 1, partition 157, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:02 INFO TaskSetManager: Finished task 154.0 in stage 3.0 (TID 183) in 107 ms on 192.168.1.9 (executor 1) (181/200)
20/12/04 16:16:02 INFO TaskSetManager: Starting task 158.0 in stage 3.0 (TID 185, 192.168.1.9, executor 1, partition 158, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:02 INFO TaskSetManager: Finished task 157.0 in stage 3.0 (TID 184) in 109 ms on 192.168.1.9 (executor 1) (182/200)
20/12/04 16:16:03 INFO TaskSetManager: Starting task 159.0 in stage 3.0 (TID 186, 192.168.1.9, executor 1, partition 159, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:03 INFO TaskSetManager: Finished task 158.0 in stage 3.0 (TID 185) in 107 ms on 192.168.1.9 (executor 1) (183/200)
20/12/04 16:16:03 INFO TaskSetManager: Starting task 160.0 in stage 3.0 (TID 187, 192.168.1.9, executor 1, partition 160, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:03 INFO TaskSetManager: Finished task 159.0 in stage 3.0 (TID 186) in 111 ms on 192.168.1.9 (executor 1) (184/200)
20/12/04 16:16:03 INFO TaskSetManager: Starting task 166.0 in stage 3.0 (TID 188, 192.168.1.9, executor 1, partition 166, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:03 INFO TaskSetManager: Finished task 160.0 in stage 3.0 (TID 187) in 109 ms on 192.168.1.9 (executor 1) (185/200)
20/12/04 16:16:03 INFO TaskSetManager: Starting task 169.0 in stage 3.0 (TID 189, 192.168.1.9, executor 1, partition 169, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:03 INFO TaskSetManager: Finished task 166.0 in stage 3.0 (TID 188) in 109 ms on 192.168.1.9 (executor 1) (186/200)
20/12/04 16:16:03 INFO TaskSetManager: Starting task 171.0 in stage 3.0 (TID 190, 192.168.1.9, executor 1, partition 171, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:03 INFO TaskSetManager: Finished task 169.0 in stage 3.0 (TID 189) in 107 ms on 192.168.1.9 (executor 1) (187/200)
20/12/04 16:16:03 INFO TaskSetManager: Starting task 176.0 in stage 3.0 (TID 191, 192.168.1.9, executor 1, partition 176, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:03 INFO TaskSetManager: Finished task 171.0 in stage 3.0 (TID 190) in 112 ms on 192.168.1.9 (executor 1) (188/200)
20/12/04 16:16:03 INFO TaskSetManager: Starting task 177.0 in stage 3.0 (TID 192, 192.168.1.9, executor 1, partition 177, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:03 INFO TaskSetManager: Finished task 176.0 in stage 3.0 (TID 191) in 133 ms on 192.168.1.9 (executor 1) (189/200)
20/12/04 16:16:03 INFO TaskSetManager: Starting task 180.0 in stage 3.0 (TID 193, 192.168.1.9, executor 1, partition 180, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:03 INFO TaskSetManager: Finished task 177.0 in stage 3.0 (TID 192) in 113 ms on 192.168.1.9 (executor 1) (190/200)
20/12/04 16:16:03 INFO TaskSetManager: Starting task 182.0 in stage 3.0 (TID 194, 192.168.1.9, executor 1, partition 182, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:03 INFO TaskSetManager: Finished task 180.0 in stage 3.0 (TID 193) in 108 ms on 192.168.1.9 (executor 1) (191/200)
20/12/04 16:16:04 INFO TaskSetManager: Starting task 185.0 in stage 3.0 (TID 195, 192.168.1.9, executor 1, partition 185, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:04 INFO TaskSetManager: Finished task 182.0 in stage 3.0 (TID 194) in 109 ms on 192.168.1.9 (executor 1) (192/200)
20/12/04 16:16:04 INFO TaskSetManager: Starting task 186.0 in stage 3.0 (TID 196, 192.168.1.9, executor 1, partition 186, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:04 INFO TaskSetManager: Finished task 185.0 in stage 3.0 (TID 195) in 120 ms on 192.168.1.9 (executor 1) (193/200)
20/12/04 16:16:04 INFO TaskSetManager: Starting task 187.0 in stage 3.0 (TID 197, 192.168.1.9, executor 1, partition 187, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:04 INFO TaskSetManager: Finished task 186.0 in stage 3.0 (TID 196) in 109 ms on 192.168.1.9 (executor 1) (194/200)
20/12/04 16:16:04 INFO TaskSetManager: Starting task 188.0 in stage 3.0 (TID 198, 192.168.1.9, executor 1, partition 188, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:04 INFO TaskSetManager: Finished task 187.0 in stage 3.0 (TID 197) in 107 ms on 192.168.1.9 (executor 1) (195/200)
20/12/04 16:16:04 INFO TaskSetManager: Starting task 189.0 in stage 3.0 (TID 199, 192.168.1.9, executor 1, partition 189, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:04 INFO TaskSetManager: Finished task 188.0 in stage 3.0 (TID 198) in 113 ms on 192.168.1.9 (executor 1) (196/200)
20/12/04 16:16:04 INFO TaskSetManager: Starting task 191.0 in stage 3.0 (TID 200, 192.168.1.9, executor 1, partition 191, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:04 INFO TaskSetManager: Finished task 189.0 in stage 3.0 (TID 199) in 110 ms on 192.168.1.9 (executor 1) (197/200)
20/12/04 16:16:04 INFO TaskSetManager: Starting task 196.0 in stage 3.0 (TID 201, 192.168.1.9, executor 1, partition 196, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:04 INFO TaskSetManager: Finished task 191.0 in stage 3.0 (TID 200) in 107 ms on 192.168.1.9 (executor 1) (198/200)
20/12/04 16:16:04 INFO TaskSetManager: Starting task 197.0 in stage 3.0 (TID 202, 192.168.1.9, executor 1, partition 197, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:16:04 INFO TaskSetManager: Finished task 196.0 in stage 3.0 (TID 201) in 108 ms on 192.168.1.9 (executor 1) (199/200)
20/12/04 16:16:04 INFO TaskSetManager: Finished task 197.0 in stage 3.0 (TID 202) in 108 ms on 192.168.1.9 (executor 1) (200/200)
20/12/04 16:16:04 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 16:16:04 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/userFiles-b2fe09a0-49cd-4712-868c-833bf8f70710/darima.zip/darima/dlsa.py:22) finished in 168.870 s
20/12/04 16:16:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:16:04 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
20/12/04 16:16:04 INFO DAGScheduler: Job 1 finished: toPandas at /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/userFiles-b2fe09a0-49cd-4712-868c-833bf8f70710/darima.zip/darima/dlsa.py:22, took 170.306928 s
20/12/04 16:16:05 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:16:05 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:16:05 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:16:05 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:16:05 INFO CodeGenerator: Code generated in 16.636657 ms
20/12/04 16:16:05 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:16:05 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.9:43023 in memory (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 16:16:05 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.9:46773 in memory (size: 131.5 KiB, free: 912.3 MiB)
20/12/04 16:16:05 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:16:05 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:43023 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:16:05 INFO SparkContext: Created broadcast 6 from toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201
20/12/04 16:16:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:16:05 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:46773 in memory (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 16:16:05 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:43023 in memory (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 16:16:05 INFO SparkContext: Starting job: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201
20/12/04 16:16:05 INFO DAGScheduler: Got job 2 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201) with 1 output partitions
20/12/04 16:16:05 INFO DAGScheduler: Final stage: ResultStage 4 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201)
20/12/04 16:16:05 INFO DAGScheduler: Parents of final stage: List()
20/12/04 16:16:05 INFO DAGScheduler: Missing parents: List()
20/12/04 16:16:05 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201), which has no missing parents
20/12/04 16:16:05 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 21.7 KiB, free 5.8 GiB)
20/12/04 16:16:05 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 5.8 GiB)
20/12/04 16:16:05 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:43023 (size: 10.4 KiB, free: 5.8 GiB)
20/12/04 16:16:05 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1223
20/12/04 16:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201) (first 15 tasks are for partitions Vector(0))
20/12/04 16:16:05 INFO YarnScheduler: Adding task set 4.0 with 1 tasks
20/12/04 16:16:05 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 203, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7790 bytes)
20/12/04 16:16:05 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:46773 (size: 10.4 KiB, free: 912.3 MiB)
20/12/04 16:16:05 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:46773 (size: 28.7 KiB, free: 912.2 MiB)
20/12/04 16:16:06 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 203) in 435 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:16:06 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/12/04 16:16:06 INFO DAGScheduler: ResultStage 4 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201) finished in 0.447 s
20/12/04 16:16:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:16:06 INFO YarnScheduler: Killing all running tasks in stage 4: Stage finished
20/12/04 16:16:06 INFO DAGScheduler: Job 2 finished: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201, took 0.456316 s
20/12/04 16:16:11 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:16:11 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:16:11 INFO FileSourceStrategy: Post-Scan Filters: 
20/12/04 16:16:11 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:16:11 INFO CodeGenerator: Code generated in 13.851911 ms
20/12/04 16:16:11 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:16:11 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:16:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:43023 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:16:11 INFO SparkContext: Created broadcast 8 from toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217
20/12/04 16:16:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:16:11 INFO SparkContext: Starting job: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217
20/12/04 16:16:11 INFO DAGScheduler: Got job 3 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217) with 1 output partitions
20/12/04 16:16:11 INFO DAGScheduler: Final stage: ResultStage 5 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217)
20/12/04 16:16:11 INFO DAGScheduler: Parents of final stage: List()
20/12/04 16:16:11 INFO DAGScheduler: Missing parents: List()
20/12/04 16:16:11 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217), which has no missing parents
20/12/04 16:16:11 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 12.3 KiB, free 5.8 GiB)
20/12/04 16:16:11 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 5.8 GiB)
20/12/04 16:16:11 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:43023 (size: 6.3 KiB, free: 5.8 GiB)
20/12/04 16:16:11 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1223
20/12/04 16:16:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217) (first 15 tasks are for partitions Vector(0))
20/12/04 16:16:11 INFO YarnScheduler: Adding task set 5.0 with 1 tasks
20/12/04 16:16:11 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 204, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7789 bytes)
20/12/04 16:16:11 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:46773 (size: 6.3 KiB, free: 912.2 MiB)
20/12/04 16:16:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:46773 (size: 28.7 KiB, free: 912.2 MiB)
20/12/04 16:16:11 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 204) in 89 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:16:11 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/12/04 16:16:11 INFO DAGScheduler: ResultStage 5 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217) finished in 0.100 s
20/12/04 16:16:11 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:16:11 INFO YarnScheduler: Killing all running tasks in stage 5: Stage finished
20/12/04 16:16:11 INFO DAGScheduler: Job 3 finished: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217, took 0.105467 s
20/12/04 16:16:11 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 16:16:11 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 16:16:11 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 16:16:11 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 16:16:11 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 16:16:11 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 16:16:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 16:16:11 INFO MemoryStore: MemoryStore cleared
20/12/04 16:16:11 INFO BlockManager: BlockManager stopped
20/12/04 16:16:11 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 16:16:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 16:16:11 INFO SparkContext: Successfully stopped SparkContext
20/12/04 16:16:11 INFO ShutdownHookManager: Shutdown hook called
20/12/04 16:16:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c/pyspark-5f7ae42b-a1a0-485c-89b2-8f78027cdaf7
20/12/04 16:16:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-28f676a9-6f82-4f62-b527-dc6d0c8f197c
20/12/04 16:16:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-4406c776-bbb6-47fe-89d8-a8f89bc66b7b
20/12/04 16:16:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 16:16:14 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:16:14 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:16:14 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:16:14 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:16:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:16:15 INFO SparkContext: Running Spark version 3.0.1
20/12/04 16:16:15 INFO ResourceUtils: ==============================================================
20/12/04 16:16:15 INFO ResourceUtils: Resources for spark.driver:

20/12/04 16:16:15 INFO ResourceUtils: ==============================================================
20/12/04 16:16:15 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 16:16:15 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:16:15 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:16:15 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:16:15 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:16:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:16:15 INFO Utils: Successfully started service 'sparkDriver' on port 36187.
20/12/04 16:16:15 INFO SparkEnv: Registering MapOutputTracker
20/12/04 16:16:15 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 16:16:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 16:16:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 16:16:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 16:16:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c767a54c-23a3-41d1-a5ea-5a0c3016687c
20/12/04 16:16:15 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 16:16:16 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 16:16:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 16:16:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 16:16:16 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 16:16:17 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 16:16:17 INFO Configuration: resource-types.xml not found
20/12/04 16:16:17 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 16:16:17 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 16:16:17 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 16:16:17 INFO Client: Setting up container launch context for our AM
20/12/04 16:16:17 INFO Client: Setting up the launch environment for our AM container
20/12/04 16:16:17 INFO Client: Preparing resources for our AM container
20/12/04 16:16:17 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 16:16:19 INFO Client: Uploading resource file:/tmp/spark-db8828d4-3992-464f-871a-15ef3fa8f401/__spark_libs__9612455938352871488.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0075/__spark_libs__9612455938352871488.zip
20/12/04 16:16:20 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0075/pyspark.zip
20/12/04 16:16:20 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0075/py4j-0.10.9-src.zip
20/12/04 16:16:21 INFO Client: Uploading resource file:/tmp/spark-db8828d4-3992-464f-871a-15ef3fa8f401/__spark_conf__14691023385545738508.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0075/__spark_conf__.zip
20/12/04 16:16:21 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:16:21 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:16:21 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:16:21 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:16:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:16:21 INFO Client: Submitting application application_1607015337794_0075 to ResourceManager
20/12/04 16:16:21 INFO YarnClientImpl: Submitted application application_1607015337794_0075
20/12/04 16:16:22 INFO Client: Application report for application_1607015337794_0075 (state: ACCEPTED)
20/12/04 16:16:22 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607116581270
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0075/
	 user: bsuconn
20/12/04 16:16:23 INFO Client: Application report for application_1607015337794_0075 (state: ACCEPTED)
20/12/04 16:16:24 INFO Client: Application report for application_1607015337794_0075 (state: ACCEPTED)
20/12/04 16:16:25 INFO Client: Application report for application_1607015337794_0075 (state: ACCEPTED)
20/12/04 16:16:26 INFO Client: Application report for application_1607015337794_0075 (state: RUNNING)
20/12/04 16:16:26 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607116581270
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0075/
	 user: bsuconn
20/12/04 16:16:26 INFO YarnClientSchedulerBackend: Application application_1607015337794_0075 has started running.
20/12/04 16:16:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40373.
20/12/04 16:16:26 INFO NettyBlockTransferService: Server created on 192.168.1.9:40373
20/12/04 16:16:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 16:16:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 40373, None)
20/12/04 16:16:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40373 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 40373, None)
20/12/04 16:16:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 40373, None)
20/12/04 16:16:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 40373, None)
20/12/04 16:16:26 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0075), /proxy/application_1607015337794_0075
20/12/04 16:16:26 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:16:27 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 16:16:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 16:16:33 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:46870) with ID 1
20/12/04 16:16:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40955 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 40955, None)
20/12/04 16:16:35 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:46874) with ID 2
20/12/04 16:16:35 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 16:16:35 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:41327 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 41327, None)
20/12/04 16:16:35 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 16:16:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 16:16:35 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 16:16:35 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:16:35 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:16:35 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:16:35 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:16:35 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:16:36 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:36187/files/darima.zip with timestamp 1607116596165
20/12/04 16:16:36 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-db8828d4-3992-464f-871a-15ef3fa8f401/userFiles-23bf766d-8469-4dd4-aab0-858ddf272c65/darima.zip
20/12/04 16:16:38 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
20/12/04 16:16:39 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/12/04 16:16:40 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:16:40 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:16:40 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:16:40 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:16:41 INFO CodeGenerator: Code generated in 314.110944 ms
20/12/04 16:16:41 INFO CodeGenerator: Code generated in 31.16958 ms
20/12/04 16:16:41 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:16:41 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:16:41 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:40373 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:16:41 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 16:16:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:16:41 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 16:16:41 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 16:16:41 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 16:16:41 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 16:16:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 16:16:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 16:16:41 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:16:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 16:16:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 16:16:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:40373 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:16:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 16:16:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:16:41 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 16:16:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:16:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:41327 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:16:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:41327 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:16:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3461 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 16:16:45 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 16:16:45 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.638 s
20/12/04 16:16:45 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:16:45 INFO DAGScheduler: running: Set()
20/12/04 16:16:45 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 16:16:45 INFO DAGScheduler: failed: Set()
20/12/04 16:16:45 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:16:45 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 16:16:45 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 16:16:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:40373 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:16:45 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 16:16:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:16:45 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 16:16:45 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:16:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:40955 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:16:46 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:46870
20/12/04 16:16:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1879 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:16:47 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 16:16:47 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 1.896 s
20/12/04 16:16:47 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:16:47 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 16:16:47 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 5.623410 s
20/12/04 16:16:49 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 16:16:49 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 16:16:49 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 16:16:49 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 16:16:49 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 16:16:49 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 16:16:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 16:16:49 INFO MemoryStore: MemoryStore cleared
20/12/04 16:16:49 INFO BlockManager: BlockManager stopped
20/12/04 16:16:49 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 16:16:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 16:16:49 INFO SparkContext: Successfully stopped SparkContext
20/12/04 16:16:49 INFO ShutdownHookManager: Shutdown hook called
20/12/04 16:16:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-db8828d4-3992-464f-871a-15ef3fa8f401/pyspark-3b9512d3-5e03-45b4-9285-08f06b45ac61
20/12/04 16:16:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-9b863619-9126-4667-a694-754fa6bf5b3d
20/12/04 16:16:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-db8828d4-3992-464f-871a-15ef3fa8f401
20/12/04 16:21:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 16:21:57 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:21:57 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:21:57 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:21:57 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:21:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:21:58 INFO SparkContext: Running Spark version 3.0.1
20/12/04 16:21:58 INFO ResourceUtils: ==============================================================
20/12/04 16:21:58 INFO ResourceUtils: Resources for spark.driver:

20/12/04 16:21:58 INFO ResourceUtils: ==============================================================
20/12/04 16:21:58 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 16:21:58 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:21:58 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:21:58 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:21:58 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:21:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:21:58 INFO Utils: Successfully started service 'sparkDriver' on port 46281.
20/12/04 16:21:58 INFO SparkEnv: Registering MapOutputTracker
20/12/04 16:21:58 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 16:21:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 16:21:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 16:21:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 16:21:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-256293d1-98b4-4004-962a-8f04f2f1bf1a
20/12/04 16:21:58 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 16:21:58 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 16:21:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 16:21:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 16:21:59 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 16:22:00 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 16:22:00 INFO Configuration: resource-types.xml not found
20/12/04 16:22:00 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 16:22:00 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 16:22:00 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 16:22:00 INFO Client: Setting up container launch context for our AM
20/12/04 16:22:00 INFO Client: Setting up the launch environment for our AM container
20/12/04 16:22:00 INFO Client: Preparing resources for our AM container
20/12/04 16:22:00 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 16:22:03 INFO Client: Uploading resource file:/tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/__spark_libs__8500252082011071267.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0076/__spark_libs__8500252082011071267.zip
20/12/04 16:22:04 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0076/pyspark.zip
20/12/04 16:22:04 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0076/py4j-0.10.9-src.zip
20/12/04 16:22:05 INFO Client: Uploading resource file:/tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/__spark_conf__16258628184806155535.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0076/__spark_conf__.zip
20/12/04 16:22:05 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:22:05 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:22:05 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:22:05 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:22:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:22:05 INFO Client: Submitting application application_1607015337794_0076 to ResourceManager
20/12/04 16:22:05 INFO YarnClientImpl: Submitted application application_1607015337794_0076
20/12/04 16:22:06 INFO Client: Application report for application_1607015337794_0076 (state: ACCEPTED)
20/12/04 16:22:06 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607116925139
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0076/
	 user: bsuconn
20/12/04 16:22:07 INFO Client: Application report for application_1607015337794_0076 (state: ACCEPTED)
20/12/04 16:22:08 INFO Client: Application report for application_1607015337794_0076 (state: ACCEPTED)
20/12/04 16:22:09 INFO Client: Application report for application_1607015337794_0076 (state: ACCEPTED)
20/12/04 16:22:10 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0076), /proxy/application_1607015337794_0076
20/12/04 16:22:10 INFO Client: Application report for application_1607015337794_0076 (state: RUNNING)
20/12/04 16:22:10 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607116925139
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0076/
	 user: bsuconn
20/12/04 16:22:10 INFO YarnClientSchedulerBackend: Application application_1607015337794_0076 has started running.
20/12/04 16:22:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35837.
20/12/04 16:22:10 INFO NettyBlockTransferService: Server created on 192.168.1.9:35837
20/12/04 16:22:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 16:22:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 35837, None)
20/12/04 16:22:10 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:35837 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 35837, None)
20/12/04 16:22:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 35837, None)
20/12/04 16:22:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 35837, None)
20/12/04 16:22:10 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:22:11 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 16:22:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 16:22:14 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:45960) with ID 1
20/12/04 16:22:14 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 16:22:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:42185 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 42185, None)
20/12/04 16:22:15 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 16:22:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 16:22:15 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 16:22:15 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:22:15 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:22:15 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:22:15 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:22:15 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:22:15 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:46281/files/darima.zip with timestamp 1607116935891
20/12/04 16:22:15 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/userFiles-b4b3d0e1-4754-4c65-936d-321ae7be5c5b/darima.zip
20/12/04 16:22:17 INFO InMemoryFileIndex: It took 58 ms to list leaf files for 1 paths.
20/12/04 16:22:19 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/12/04 16:22:19 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:22:19 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:22:19 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:22:19 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:22:20 INFO CodeGenerator: Code generated in 317.337509 ms
20/12/04 16:22:20 INFO CodeGenerator: Code generated in 32.906077 ms
20/12/04 16:22:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:22:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:22:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:35837 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:22:20 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 16:22:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:22:21 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 16:22:21 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 16:22:21 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 16:22:21 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 16:22:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 16:22:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 16:22:21 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:22:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 16:22:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 16:22:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:35837 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:22:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 16:22:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:22:21 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 16:22:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:22:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:42185 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:22:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:42185 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:22:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3231 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:22:24 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 16:22:24 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.370 s
20/12/04 16:22:24 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:22:24 INFO DAGScheduler: running: Set()
20/12/04 16:22:24 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 16:22:24 INFO DAGScheduler: failed: Set()
20/12/04 16:22:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:22:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 16:22:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 16:22:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:35837 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:22:24 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 16:22:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:22:24 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 16:22:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:42185 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:22:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:45960
20/12/04 16:22:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 392 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:22:24 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 16:22:24 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.407 s
20/12/04 16:22:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:22:24 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 16:22:24 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.866749 s
20/12/04 16:22:27 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:42185 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:22:27 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:35837 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:22:27 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:42185 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:22:27 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:35837 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:22:27 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:35837 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:22:27 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:42185 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:22:27 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 16:22:28 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:22:28 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:22:28 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:22:28 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:22:28 INFO CodeGenerator: Code generated in 27.515863 ms
20/12/04 16:22:28 INFO CodeGenerator: Code generated in 15.576231 ms
20/12/04 16:22:28 INFO CodeGenerator: Code generated in 19.077677 ms
20/12/04 16:22:28 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:22:28 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:22:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:35837 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:22:28 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/userFiles-b4b3d0e1-4754-4c65-936d-321ae7be5c5b/darima.zip/darima/dlsa.py:22
20/12/04 16:22:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:22:28 INFO SparkContext: Starting job: toPandas at /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/userFiles-b4b3d0e1-4754-4c65-936d-321ae7be5c5b/darima.zip/darima/dlsa.py:22
20/12/04 16:22:28 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/userFiles-b4b3d0e1-4754-4c65-936d-321ae7be5c5b/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 16:22:28 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/userFiles-b4b3d0e1-4754-4c65-936d-321ae7be5c5b/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 16:22:28 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/userFiles-b4b3d0e1-4754-4c65-936d-321ae7be5c5b/darima.zip/darima/dlsa.py:22)
20/12/04 16:22:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 16:22:28 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 16:22:28 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/userFiles-b4b3d0e1-4754-4c65-936d-321ae7be5c5b/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:22:28 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 16:22:28 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 16:22:28 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:35837 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 16:22:28 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 16:22:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/userFiles-b4b3d0e1-4754-4c65-936d-321ae7be5c5b/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 16:22:28 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 16:22:28 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:22:28 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:42185 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 16:22:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:42185 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:22:30 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1318 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:22:30 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 16:22:30 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 37721
20/12/04 16:22:30 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/userFiles-b4b3d0e1-4754-4c65-936d-321ae7be5c5b/darima.zip/darima/dlsa.py:22) finished in 1.345 s
20/12/04 16:22:30 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:22:30 INFO DAGScheduler: running: Set()
20/12/04 16:22:30 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 16:22:30 INFO DAGScheduler: failed: Set()
20/12/04 16:22:30 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/userFiles-b4b3d0e1-4754-4c65-936d-321ae7be5c5b/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:22:30 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 16:22:30 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 16:22:30 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:35837 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 16:22:30 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 16:22:30 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/userFiles-b4b3d0e1-4754-4c65-936d-321ae7be5c5b/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 16:22:30 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 16:22:30 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:30 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:42185 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 16:22:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:45960
20/12/04 16:22:34 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:34 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 4597 ms on 192.168.1.9 (executor 1) (1/200)
20/12/04 16:22:35 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:35 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 4) in 1082 ms on 192.168.1.9 (executor 1) (2/200)
20/12/04 16:22:38 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 4, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:38 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 5) in 2319 ms on 192.168.1.9 (executor 1) (3/200)
20/12/04 16:22:39 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 5, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:39 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 6) in 1168 ms on 192.168.1.9 (executor 1) (4/200)
20/12/04 16:22:40 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 6, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:40 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 7) in 958 ms on 192.168.1.9 (executor 1) (5/200)
20/12/04 16:22:41 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 7, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:41 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 8) in 1400 ms on 192.168.1.9 (executor 1) (6/200)
20/12/04 16:22:43 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 10, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:43 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 9) in 1674 ms on 192.168.1.9 (executor 1) (7/200)
20/12/04 16:22:44 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 11, 192.168.1.9, executor 1, partition 11, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:44 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 10) in 1612 ms on 192.168.1.9 (executor 1) (8/200)
20/12/04 16:22:45 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 12, 192.168.1.9, executor 1, partition 12, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:45 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 11) in 959 ms on 192.168.1.9 (executor 1) (9/200)
20/12/04 16:22:49 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 13, 192.168.1.9, executor 1, partition 13, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:49 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 12) in 3244 ms on 192.168.1.9 (executor 1) (10/200)
20/12/04 16:22:49 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 14, 192.168.1.9, executor 1, partition 14, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:49 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 13) in 760 ms on 192.168.1.9 (executor 1) (11/200)
20/12/04 16:22:51 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 15, 192.168.1.9, executor 1, partition 18, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:51 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 14) in 1269 ms on 192.168.1.9 (executor 1) (12/200)
20/12/04 16:22:52 INFO TaskSetManager: Starting task 19.0 in stage 3.0 (TID 16, 192.168.1.9, executor 1, partition 19, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:52 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 15) in 1024 ms on 192.168.1.9 (executor 1) (13/200)
20/12/04 16:22:54 INFO TaskSetManager: Starting task 21.0 in stage 3.0 (TID 17, 192.168.1.9, executor 1, partition 21, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:54 INFO TaskSetManager: Finished task 19.0 in stage 3.0 (TID 16) in 2738 ms on 192.168.1.9 (executor 1) (14/200)
20/12/04 16:22:56 INFO TaskSetManager: Starting task 23.0 in stage 3.0 (TID 18, 192.168.1.9, executor 1, partition 23, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:56 INFO TaskSetManager: Finished task 21.0 in stage 3.0 (TID 17) in 1843 ms on 192.168.1.9 (executor 1) (15/200)
20/12/04 16:22:57 INFO TaskSetManager: Starting task 24.0 in stage 3.0 (TID 19, 192.168.1.9, executor 1, partition 24, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:57 INFO TaskSetManager: Finished task 23.0 in stage 3.0 (TID 18) in 1209 ms on 192.168.1.9 (executor 1) (16/200)
20/12/04 16:22:59 INFO TaskSetManager: Finished task 24.0 in stage 3.0 (TID 19) in 1193 ms on 192.168.1.9 (executor 1) (17/200)
20/12/04 16:22:59 INFO TaskSetManager: Starting task 27.0 in stage 3.0 (TID 20, 192.168.1.9, executor 1, partition 27, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:59 INFO TaskSetManager: Starting task 30.0 in stage 3.0 (TID 21, 192.168.1.9, executor 1, partition 30, NODE_LOCAL, 7336 bytes)
20/12/04 16:22:59 INFO TaskSetManager: Finished task 27.0 in stage 3.0 (TID 20) in 605 ms on 192.168.1.9 (executor 1) (18/200)
20/12/04 16:23:01 INFO TaskSetManager: Starting task 31.0 in stage 3.0 (TID 22, 192.168.1.9, executor 1, partition 31, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:01 INFO TaskSetManager: Finished task 30.0 in stage 3.0 (TID 21) in 1418 ms on 192.168.1.9 (executor 1) (19/200)
20/12/04 16:23:02 INFO TaskSetManager: Starting task 32.0 in stage 3.0 (TID 23, 192.168.1.9, executor 1, partition 32, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:02 INFO TaskSetManager: Finished task 31.0 in stage 3.0 (TID 22) in 1444 ms on 192.168.1.9 (executor 1) (20/200)
20/12/04 16:23:03 INFO TaskSetManager: Starting task 34.0 in stage 3.0 (TID 24, 192.168.1.9, executor 1, partition 34, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:03 INFO TaskSetManager: Finished task 32.0 in stage 3.0 (TID 23) in 1281 ms on 192.168.1.9 (executor 1) (21/200)
20/12/04 16:23:04 INFO TaskSetManager: Starting task 35.0 in stage 3.0 (TID 25, 192.168.1.9, executor 1, partition 35, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:04 INFO TaskSetManager: Finished task 34.0 in stage 3.0 (TID 24) in 770 ms on 192.168.1.9 (executor 1) (22/200)
20/12/04 16:23:06 INFO TaskSetManager: Starting task 36.0 in stage 3.0 (TID 26, 192.168.1.9, executor 1, partition 36, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:06 INFO TaskSetManager: Finished task 35.0 in stage 3.0 (TID 25) in 1451 ms on 192.168.1.9 (executor 1) (23/200)
20/12/04 16:23:07 INFO TaskSetManager: Starting task 37.0 in stage 3.0 (TID 27, 192.168.1.9, executor 1, partition 37, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:07 INFO TaskSetManager: Finished task 36.0 in stage 3.0 (TID 26) in 937 ms on 192.168.1.9 (executor 1) (24/200)
20/12/04 16:23:07 INFO TaskSetManager: Starting task 41.0 in stage 3.0 (TID 28, 192.168.1.9, executor 1, partition 41, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:07 INFO TaskSetManager: Finished task 37.0 in stage 3.0 (TID 27) in 867 ms on 192.168.1.9 (executor 1) (25/200)
20/12/04 16:23:08 INFO TaskSetManager: Starting task 43.0 in stage 3.0 (TID 29, 192.168.1.9, executor 1, partition 43, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:08 INFO TaskSetManager: Finished task 41.0 in stage 3.0 (TID 28) in 907 ms on 192.168.1.9 (executor 1) (26/200)
20/12/04 16:23:10 INFO TaskSetManager: Starting task 44.0 in stage 3.0 (TID 30, 192.168.1.9, executor 1, partition 44, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:10 INFO TaskSetManager: Finished task 43.0 in stage 3.0 (TID 29) in 1312 ms on 192.168.1.9 (executor 1) (27/200)
20/12/04 16:23:10 INFO TaskSetManager: Starting task 48.0 in stage 3.0 (TID 31, 192.168.1.9, executor 1, partition 48, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:10 INFO TaskSetManager: Finished task 44.0 in stage 3.0 (TID 30) in 784 ms on 192.168.1.9 (executor 1) (28/200)
20/12/04 16:23:11 INFO TaskSetManager: Starting task 49.0 in stage 3.0 (TID 32, 192.168.1.9, executor 1, partition 49, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:11 INFO TaskSetManager: Finished task 48.0 in stage 3.0 (TID 31) in 935 ms on 192.168.1.9 (executor 1) (29/200)
20/12/04 16:23:13 INFO TaskSetManager: Starting task 51.0 in stage 3.0 (TID 33, 192.168.1.9, executor 1, partition 51, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:13 INFO TaskSetManager: Finished task 49.0 in stage 3.0 (TID 32) in 2102 ms on 192.168.1.9 (executor 1) (30/200)
20/12/04 16:23:16 INFO TaskSetManager: Starting task 53.0 in stage 3.0 (TID 34, 192.168.1.9, executor 1, partition 53, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:16 INFO TaskSetManager: Finished task 51.0 in stage 3.0 (TID 33) in 2301 ms on 192.168.1.9 (executor 1) (31/200)
20/12/04 16:23:18 INFO TaskSetManager: Starting task 55.0 in stage 3.0 (TID 35, 192.168.1.9, executor 1, partition 55, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:18 INFO TaskSetManager: Finished task 53.0 in stage 3.0 (TID 34) in 2194 ms on 192.168.1.9 (executor 1) (32/200)
20/12/04 16:23:19 INFO TaskSetManager: Starting task 57.0 in stage 3.0 (TID 36, 192.168.1.9, executor 1, partition 57, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:19 INFO TaskSetManager: Finished task 55.0 in stage 3.0 (TID 35) in 773 ms on 192.168.1.9 (executor 1) (33/200)
20/12/04 16:23:20 INFO TaskSetManager: Starting task 58.0 in stage 3.0 (TID 37, 192.168.1.9, executor 1, partition 58, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:20 INFO TaskSetManager: Finished task 57.0 in stage 3.0 (TID 36) in 1037 ms on 192.168.1.9 (executor 1) (34/200)
20/12/04 16:23:21 INFO TaskSetManager: Starting task 61.0 in stage 3.0 (TID 38, 192.168.1.9, executor 1, partition 61, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:21 INFO TaskSetManager: Finished task 58.0 in stage 3.0 (TID 37) in 967 ms on 192.168.1.9 (executor 1) (35/200)
20/12/04 16:23:23 INFO TaskSetManager: Starting task 63.0 in stage 3.0 (TID 39, 192.168.1.9, executor 1, partition 63, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:23 INFO TaskSetManager: Finished task 61.0 in stage 3.0 (TID 38) in 2151 ms on 192.168.1.9 (executor 1) (36/200)
20/12/04 16:23:24 INFO TaskSetManager: Starting task 65.0 in stage 3.0 (TID 40, 192.168.1.9, executor 1, partition 65, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:24 INFO TaskSetManager: Finished task 63.0 in stage 3.0 (TID 39) in 836 ms on 192.168.1.9 (executor 1) (37/200)
20/12/04 16:23:25 INFO TaskSetManager: Starting task 66.0 in stage 3.0 (TID 41, 192.168.1.9, executor 1, partition 66, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:25 INFO TaskSetManager: Finished task 65.0 in stage 3.0 (TID 40) in 933 ms on 192.168.1.9 (executor 1) (38/200)
20/12/04 16:23:25 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:35837 in memory (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 16:23:25 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:42185 in memory (size: 11.6 KiB, free: 912.1 MiB)
20/12/04 16:23:25 INFO TaskSetManager: Starting task 69.0 in stage 3.0 (TID 42, 192.168.1.9, executor 1, partition 69, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:25 INFO TaskSetManager: Finished task 66.0 in stage 3.0 (TID 41) in 756 ms on 192.168.1.9 (executor 1) (39/200)
20/12/04 16:23:26 INFO TaskSetManager: Starting task 70.0 in stage 3.0 (TID 43, 192.168.1.9, executor 1, partition 70, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:26 INFO TaskSetManager: Finished task 69.0 in stage 3.0 (TID 42) in 1017 ms on 192.168.1.9 (executor 1) (40/200)
20/12/04 16:23:28 INFO TaskSetManager: Starting task 73.0 in stage 3.0 (TID 44, 192.168.1.9, executor 1, partition 73, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:28 INFO TaskSetManager: Finished task 70.0 in stage 3.0 (TID 43) in 1136 ms on 192.168.1.9 (executor 1) (41/200)
20/12/04 16:23:29 INFO TaskSetManager: Starting task 75.0 in stage 3.0 (TID 45, 192.168.1.9, executor 1, partition 75, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:29 INFO TaskSetManager: Finished task 73.0 in stage 3.0 (TID 44) in 1242 ms on 192.168.1.9 (executor 1) (42/200)
20/12/04 16:23:30 INFO TaskSetManager: Starting task 77.0 in stage 3.0 (TID 46, 192.168.1.9, executor 1, partition 77, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:30 INFO TaskSetManager: Finished task 75.0 in stage 3.0 (TID 45) in 1715 ms on 192.168.1.9 (executor 1) (43/200)
20/12/04 16:23:33 INFO TaskSetManager: Starting task 79.0 in stage 3.0 (TID 47, 192.168.1.9, executor 1, partition 79, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:33 INFO TaskSetManager: Finished task 77.0 in stage 3.0 (TID 46) in 2601 ms on 192.168.1.9 (executor 1) (44/200)
20/12/04 16:23:35 INFO TaskSetManager: Starting task 84.0 in stage 3.0 (TID 48, 192.168.1.9, executor 1, partition 84, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:35 INFO TaskSetManager: Finished task 79.0 in stage 3.0 (TID 47) in 1878 ms on 192.168.1.9 (executor 1) (45/200)
20/12/04 16:23:35 INFO TaskSetManager: Starting task 85.0 in stage 3.0 (TID 49, 192.168.1.9, executor 1, partition 85, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:35 INFO TaskSetManager: Finished task 84.0 in stage 3.0 (TID 48) in 535 ms on 192.168.1.9 (executor 1) (46/200)
20/12/04 16:23:38 INFO TaskSetManager: Starting task 86.0 in stage 3.0 (TID 50, 192.168.1.9, executor 1, partition 86, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:38 INFO TaskSetManager: Finished task 85.0 in stage 3.0 (TID 49) in 2244 ms on 192.168.1.9 (executor 1) (47/200)
20/12/04 16:23:39 INFO TaskSetManager: Starting task 88.0 in stage 3.0 (TID 51, 192.168.1.9, executor 1, partition 88, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:39 INFO TaskSetManager: Finished task 86.0 in stage 3.0 (TID 50) in 1227 ms on 192.168.1.9 (executor 1) (48/200)
20/12/04 16:23:40 INFO TaskSetManager: Starting task 89.0 in stage 3.0 (TID 52, 192.168.1.9, executor 1, partition 89, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:40 INFO TaskSetManager: Finished task 88.0 in stage 3.0 (TID 51) in 629 ms on 192.168.1.9 (executor 1) (49/200)
20/12/04 16:23:41 INFO TaskSetManager: Starting task 91.0 in stage 3.0 (TID 53, 192.168.1.9, executor 1, partition 91, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:41 INFO TaskSetManager: Finished task 89.0 in stage 3.0 (TID 52) in 1779 ms on 192.168.1.9 (executor 1) (50/200)
20/12/04 16:23:42 INFO TaskSetManager: Starting task 95.0 in stage 3.0 (TID 54, 192.168.1.9, executor 1, partition 95, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:42 INFO TaskSetManager: Finished task 91.0 in stage 3.0 (TID 53) in 1135 ms on 192.168.1.9 (executor 1) (51/200)
20/12/04 16:23:44 INFO TaskSetManager: Starting task 101.0 in stage 3.0 (TID 55, 192.168.1.9, executor 1, partition 101, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:44 INFO TaskSetManager: Finished task 95.0 in stage 3.0 (TID 54) in 1708 ms on 192.168.1.9 (executor 1) (52/200)
20/12/04 16:23:45 INFO TaskSetManager: Starting task 102.0 in stage 3.0 (TID 56, 192.168.1.9, executor 1, partition 102, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:45 INFO TaskSetManager: Finished task 101.0 in stage 3.0 (TID 55) in 838 ms on 192.168.1.9 (executor 1) (53/200)
20/12/04 16:23:48 INFO TaskSetManager: Starting task 103.0 in stage 3.0 (TID 57, 192.168.1.9, executor 1, partition 103, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:48 INFO TaskSetManager: Finished task 102.0 in stage 3.0 (TID 56) in 3218 ms on 192.168.1.9 (executor 1) (54/200)
20/12/04 16:23:50 INFO TaskSetManager: Starting task 105.0 in stage 3.0 (TID 58, 192.168.1.9, executor 1, partition 105, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:50 INFO TaskSetManager: Finished task 103.0 in stage 3.0 (TID 57) in 1821 ms on 192.168.1.9 (executor 1) (55/200)
20/12/04 16:23:52 INFO TaskSetManager: Starting task 106.0 in stage 3.0 (TID 59, 192.168.1.9, executor 1, partition 106, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:52 INFO TaskSetManager: Finished task 105.0 in stage 3.0 (TID 58) in 2122 ms on 192.168.1.9 (executor 1) (56/200)
20/12/04 16:23:53 INFO TaskSetManager: Starting task 107.0 in stage 3.0 (TID 60, 192.168.1.9, executor 1, partition 107, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:53 INFO TaskSetManager: Finished task 106.0 in stage 3.0 (TID 59) in 546 ms on 192.168.1.9 (executor 1) (57/200)
20/12/04 16:23:53 INFO TaskSetManager: Starting task 108.0 in stage 3.0 (TID 61, 192.168.1.9, executor 1, partition 108, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:53 INFO TaskSetManager: Finished task 107.0 in stage 3.0 (TID 60) in 720 ms on 192.168.1.9 (executor 1) (58/200)
20/12/04 16:23:54 INFO TaskSetManager: Starting task 109.0 in stage 3.0 (TID 62, 192.168.1.9, executor 1, partition 109, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:54 INFO TaskSetManager: Finished task 108.0 in stage 3.0 (TID 61) in 775 ms on 192.168.1.9 (executor 1) (59/200)
20/12/04 16:23:56 INFO TaskSetManager: Finished task 109.0 in stage 3.0 (TID 62) in 2183 ms on 192.168.1.9 (executor 1) (60/200)
20/12/04 16:23:56 INFO TaskSetManager: Starting task 111.0 in stage 3.0 (TID 63, 192.168.1.9, executor 1, partition 111, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:58 INFO TaskSetManager: Starting task 113.0 in stage 3.0 (TID 64, 192.168.1.9, executor 1, partition 113, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:58 INFO TaskSetManager: Finished task 111.0 in stage 3.0 (TID 63) in 1538 ms on 192.168.1.9 (executor 1) (61/200)
20/12/04 16:23:59 INFO TaskSetManager: Starting task 115.0 in stage 3.0 (TID 65, 192.168.1.9, executor 1, partition 115, NODE_LOCAL, 7336 bytes)
20/12/04 16:23:59 INFO TaskSetManager: Finished task 113.0 in stage 3.0 (TID 64) in 869 ms on 192.168.1.9 (executor 1) (62/200)
20/12/04 16:24:00 INFO TaskSetManager: Starting task 116.0 in stage 3.0 (TID 66, 192.168.1.9, executor 1, partition 116, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:00 INFO TaskSetManager: Finished task 115.0 in stage 3.0 (TID 65) in 1417 ms on 192.168.1.9 (executor 1) (63/200)
20/12/04 16:24:01 INFO TaskSetManager: Starting task 119.0 in stage 3.0 (TID 67, 192.168.1.9, executor 1, partition 119, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:01 INFO TaskSetManager: Finished task 116.0 in stage 3.0 (TID 66) in 500 ms on 192.168.1.9 (executor 1) (64/200)
20/12/04 16:24:02 INFO TaskSetManager: Starting task 122.0 in stage 3.0 (TID 68, 192.168.1.9, executor 1, partition 122, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:02 INFO TaskSetManager: Finished task 119.0 in stage 3.0 (TID 67) in 1225 ms on 192.168.1.9 (executor 1) (65/200)
20/12/04 16:24:04 INFO TaskSetManager: Starting task 124.0 in stage 3.0 (TID 69, 192.168.1.9, executor 1, partition 124, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:04 INFO TaskSetManager: Finished task 122.0 in stage 3.0 (TID 68) in 2245 ms on 192.168.1.9 (executor 1) (66/200)
20/12/04 16:24:05 INFO TaskSetManager: Starting task 126.0 in stage 3.0 (TID 70, 192.168.1.9, executor 1, partition 126, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:05 INFO TaskSetManager: Finished task 124.0 in stage 3.0 (TID 69) in 728 ms on 192.168.1.9 (executor 1) (67/200)
20/12/04 16:24:06 INFO TaskSetManager: Starting task 128.0 in stage 3.0 (TID 71, 192.168.1.9, executor 1, partition 128, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:06 INFO TaskSetManager: Finished task 126.0 in stage 3.0 (TID 70) in 894 ms on 192.168.1.9 (executor 1) (68/200)
20/12/04 16:24:07 INFO TaskSetManager: Starting task 129.0 in stage 3.0 (TID 72, 192.168.1.9, executor 1, partition 129, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:07 INFO TaskSetManager: Finished task 128.0 in stage 3.0 (TID 71) in 700 ms on 192.168.1.9 (executor 1) (69/200)
20/12/04 16:24:08 INFO TaskSetManager: Starting task 131.0 in stage 3.0 (TID 73, 192.168.1.9, executor 1, partition 131, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:08 INFO TaskSetManager: Finished task 129.0 in stage 3.0 (TID 72) in 1058 ms on 192.168.1.9 (executor 1) (70/200)
20/12/04 16:24:08 INFO TaskSetManager: Starting task 132.0 in stage 3.0 (TID 74, 192.168.1.9, executor 1, partition 132, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:08 INFO TaskSetManager: Finished task 131.0 in stage 3.0 (TID 73) in 645 ms on 192.168.1.9 (executor 1) (71/200)
20/12/04 16:24:10 INFO TaskSetManager: Starting task 133.0 in stage 3.0 (TID 75, 192.168.1.9, executor 1, partition 133, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:10 INFO TaskSetManager: Finished task 132.0 in stage 3.0 (TID 74) in 1297 ms on 192.168.1.9 (executor 1) (72/200)
20/12/04 16:24:10 INFO TaskSetManager: Starting task 135.0 in stage 3.0 (TID 76, 192.168.1.9, executor 1, partition 135, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:10 INFO TaskSetManager: Finished task 133.0 in stage 3.0 (TID 75) in 886 ms on 192.168.1.9 (executor 1) (73/200)
20/12/04 16:24:13 INFO TaskSetManager: Starting task 136.0 in stage 3.0 (TID 77, 192.168.1.9, executor 1, partition 136, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:13 INFO TaskSetManager: Finished task 135.0 in stage 3.0 (TID 76) in 2372 ms on 192.168.1.9 (executor 1) (74/200)
20/12/04 16:24:13 INFO TaskSetManager: Starting task 137.0 in stage 3.0 (TID 78, 192.168.1.9, executor 1, partition 137, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:13 INFO TaskSetManager: Finished task 136.0 in stage 3.0 (TID 77) in 688 ms on 192.168.1.9 (executor 1) (75/200)
20/12/04 16:24:16 INFO TaskSetManager: Starting task 139.0 in stage 3.0 (TID 79, 192.168.1.9, executor 1, partition 139, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:16 INFO TaskSetManager: Finished task 137.0 in stage 3.0 (TID 78) in 2866 ms on 192.168.1.9 (executor 1) (76/200)
20/12/04 16:24:17 INFO TaskSetManager: Starting task 140.0 in stage 3.0 (TID 80, 192.168.1.9, executor 1, partition 140, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:17 INFO TaskSetManager: Finished task 139.0 in stage 3.0 (TID 79) in 683 ms on 192.168.1.9 (executor 1) (77/200)
20/12/04 16:24:19 INFO TaskSetManager: Starting task 141.0 in stage 3.0 (TID 81, 192.168.1.9, executor 1, partition 141, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:19 INFO TaskSetManager: Finished task 140.0 in stage 3.0 (TID 80) in 1502 ms on 192.168.1.9 (executor 1) (78/200)
20/12/04 16:24:19 INFO TaskSetManager: Starting task 143.0 in stage 3.0 (TID 82, 192.168.1.9, executor 1, partition 143, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:19 INFO TaskSetManager: Finished task 141.0 in stage 3.0 (TID 81) in 567 ms on 192.168.1.9 (executor 1) (79/200)
20/12/04 16:24:20 INFO TaskSetManager: Starting task 146.0 in stage 3.0 (TID 83, 192.168.1.9, executor 1, partition 146, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:20 INFO TaskSetManager: Finished task 143.0 in stage 3.0 (TID 82) in 867 ms on 192.168.1.9 (executor 1) (80/200)
20/12/04 16:24:21 INFO TaskSetManager: Starting task 150.0 in stage 3.0 (TID 84, 192.168.1.9, executor 1, partition 150, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:21 INFO TaskSetManager: Finished task 146.0 in stage 3.0 (TID 83) in 1318 ms on 192.168.1.9 (executor 1) (81/200)
20/12/04 16:24:24 INFO TaskSetManager: Starting task 151.0 in stage 3.0 (TID 85, 192.168.1.9, executor 1, partition 151, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:24 INFO TaskSetManager: Finished task 150.0 in stage 3.0 (TID 84) in 3059 ms on 192.168.1.9 (executor 1) (82/200)
20/12/04 16:24:26 INFO TaskSetManager: Starting task 153.0 in stage 3.0 (TID 86, 192.168.1.9, executor 1, partition 153, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:26 INFO TaskSetManager: Finished task 151.0 in stage 3.0 (TID 85) in 1536 ms on 192.168.1.9 (executor 1) (83/200)
20/12/04 16:24:27 INFO TaskSetManager: Starting task 155.0 in stage 3.0 (TID 87, 192.168.1.9, executor 1, partition 155, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:27 INFO TaskSetManager: Finished task 153.0 in stage 3.0 (TID 86) in 1472 ms on 192.168.1.9 (executor 1) (84/200)
20/12/04 16:24:28 INFO TaskSetManager: Starting task 156.0 in stage 3.0 (TID 88, 192.168.1.9, executor 1, partition 156, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:28 INFO TaskSetManager: Finished task 155.0 in stage 3.0 (TID 87) in 780 ms on 192.168.1.9 (executor 1) (85/200)
20/12/04 16:24:30 INFO TaskSetManager: Starting task 161.0 in stage 3.0 (TID 89, 192.168.1.9, executor 1, partition 161, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:30 INFO TaskSetManager: Finished task 156.0 in stage 3.0 (TID 88) in 1794 ms on 192.168.1.9 (executor 1) (86/200)
20/12/04 16:24:31 INFO TaskSetManager: Starting task 162.0 in stage 3.0 (TID 90, 192.168.1.9, executor 1, partition 162, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:31 INFO TaskSetManager: Finished task 161.0 in stage 3.0 (TID 89) in 1350 ms on 192.168.1.9 (executor 1) (87/200)
20/12/04 16:24:33 INFO TaskSetManager: Starting task 163.0 in stage 3.0 (TID 91, 192.168.1.9, executor 1, partition 163, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:33 INFO TaskSetManager: Finished task 162.0 in stage 3.0 (TID 90) in 1918 ms on 192.168.1.9 (executor 1) (88/200)
20/12/04 16:24:36 INFO TaskSetManager: Starting task 164.0 in stage 3.0 (TID 92, 192.168.1.9, executor 1, partition 164, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:36 INFO TaskSetManager: Finished task 163.0 in stage 3.0 (TID 91) in 2517 ms on 192.168.1.9 (executor 1) (89/200)
20/12/04 16:24:40 INFO TaskSetManager: Starting task 165.0 in stage 3.0 (TID 93, 192.168.1.9, executor 1, partition 165, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:40 INFO TaskSetManager: Finished task 164.0 in stage 3.0 (TID 92) in 4479 ms on 192.168.1.9 (executor 1) (90/200)
20/12/04 16:24:42 INFO TaskSetManager: Starting task 167.0 in stage 3.0 (TID 94, 192.168.1.9, executor 1, partition 167, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:42 INFO TaskSetManager: Finished task 165.0 in stage 3.0 (TID 93) in 2033 ms on 192.168.1.9 (executor 1) (91/200)
20/12/04 16:24:44 INFO TaskSetManager: Starting task 168.0 in stage 3.0 (TID 95, 192.168.1.9, executor 1, partition 168, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:44 INFO TaskSetManager: Finished task 167.0 in stage 3.0 (TID 94) in 2185 ms on 192.168.1.9 (executor 1) (92/200)
20/12/04 16:24:45 INFO TaskSetManager: Starting task 170.0 in stage 3.0 (TID 96, 192.168.1.9, executor 1, partition 170, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:45 INFO TaskSetManager: Finished task 168.0 in stage 3.0 (TID 95) in 599 ms on 192.168.1.9 (executor 1) (93/200)
20/12/04 16:24:46 INFO TaskSetManager: Starting task 172.0 in stage 3.0 (TID 97, 192.168.1.9, executor 1, partition 172, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:46 INFO TaskSetManager: Finished task 170.0 in stage 3.0 (TID 96) in 1106 ms on 192.168.1.9 (executor 1) (94/200)
20/12/04 16:24:47 INFO TaskSetManager: Starting task 173.0 in stage 3.0 (TID 98, 192.168.1.9, executor 1, partition 173, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:47 INFO TaskSetManager: Finished task 172.0 in stage 3.0 (TID 97) in 1030 ms on 192.168.1.9 (executor 1) (95/200)
20/12/04 16:24:49 INFO TaskSetManager: Starting task 174.0 in stage 3.0 (TID 99, 192.168.1.9, executor 1, partition 174, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:49 INFO TaskSetManager: Finished task 173.0 in stage 3.0 (TID 98) in 2339 ms on 192.168.1.9 (executor 1) (96/200)
20/12/04 16:24:51 INFO TaskSetManager: Starting task 175.0 in stage 3.0 (TID 100, 192.168.1.9, executor 1, partition 175, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:51 INFO TaskSetManager: Finished task 174.0 in stage 3.0 (TID 99) in 1987 ms on 192.168.1.9 (executor 1) (97/200)
20/12/04 16:24:53 INFO TaskSetManager: Starting task 178.0 in stage 3.0 (TID 101, 192.168.1.9, executor 1, partition 178, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:53 INFO TaskSetManager: Finished task 175.0 in stage 3.0 (TID 100) in 1806 ms on 192.168.1.9 (executor 1) (98/200)
20/12/04 16:24:54 INFO TaskSetManager: Starting task 179.0 in stage 3.0 (TID 102, 192.168.1.9, executor 1, partition 179, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:54 INFO TaskSetManager: Finished task 178.0 in stage 3.0 (TID 101) in 742 ms on 192.168.1.9 (executor 1) (99/200)
20/12/04 16:24:55 INFO TaskSetManager: Starting task 181.0 in stage 3.0 (TID 103, 192.168.1.9, executor 1, partition 181, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:55 INFO TaskSetManager: Finished task 179.0 in stage 3.0 (TID 102) in 1215 ms on 192.168.1.9 (executor 1) (100/200)
20/12/04 16:24:57 INFO TaskSetManager: Starting task 183.0 in stage 3.0 (TID 104, 192.168.1.9, executor 1, partition 183, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:57 INFO TaskSetManager: Finished task 181.0 in stage 3.0 (TID 103) in 1867 ms on 192.168.1.9 (executor 1) (101/200)
20/12/04 16:24:58 INFO TaskSetManager: Starting task 184.0 in stage 3.0 (TID 105, 192.168.1.9, executor 1, partition 184, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:58 INFO TaskSetManager: Finished task 183.0 in stage 3.0 (TID 104) in 607 ms on 192.168.1.9 (executor 1) (102/200)
20/12/04 16:24:59 INFO TaskSetManager: Starting task 190.0 in stage 3.0 (TID 106, 192.168.1.9, executor 1, partition 190, NODE_LOCAL, 7336 bytes)
20/12/04 16:24:59 INFO TaskSetManager: Finished task 184.0 in stage 3.0 (TID 105) in 1799 ms on 192.168.1.9 (executor 1) (103/200)
20/12/04 16:25:01 INFO TaskSetManager: Starting task 192.0 in stage 3.0 (TID 107, 192.168.1.9, executor 1, partition 192, NODE_LOCAL, 7336 bytes)
20/12/04 16:25:01 INFO TaskSetManager: Finished task 190.0 in stage 3.0 (TID 106) in 1217 ms on 192.168.1.9 (executor 1) (104/200)
20/12/04 16:25:01 INFO TaskSetManager: Starting task 193.0 in stage 3.0 (TID 108, 192.168.1.9, executor 1, partition 193, NODE_LOCAL, 7336 bytes)
20/12/04 16:25:01 INFO TaskSetManager: Finished task 192.0 in stage 3.0 (TID 107) in 501 ms on 192.168.1.9 (executor 1) (105/200)
20/12/04 16:25:03 INFO TaskSetManager: Starting task 194.0 in stage 3.0 (TID 109, 192.168.1.9, executor 1, partition 194, NODE_LOCAL, 7336 bytes)
20/12/04 16:25:03 INFO TaskSetManager: Finished task 193.0 in stage 3.0 (TID 108) in 1879 ms on 192.168.1.9 (executor 1) (106/200)
20/12/04 16:25:04 INFO TaskSetManager: Starting task 195.0 in stage 3.0 (TID 110, 192.168.1.9, executor 1, partition 195, NODE_LOCAL, 7336 bytes)
20/12/04 16:25:04 INFO TaskSetManager: Finished task 194.0 in stage 3.0 (TID 109) in 926 ms on 192.168.1.9 (executor 1) (107/200)
20/12/04 16:25:05 INFO TaskSetManager: Starting task 198.0 in stage 3.0 (TID 111, 192.168.1.9, executor 1, partition 198, NODE_LOCAL, 7336 bytes)
20/12/04 16:25:05 INFO TaskSetManager: Finished task 195.0 in stage 3.0 (TID 110) in 921 ms on 192.168.1.9 (executor 1) (108/200)
20/12/04 16:25:05 INFO TaskSetManager: Starting task 199.0 in stage 3.0 (TID 112, 192.168.1.9, executor 1, partition 199, NODE_LOCAL, 7336 bytes)
20/12/04 16:25:05 INFO TaskSetManager: Finished task 198.0 in stage 3.0 (TID 111) in 559 ms on 192.168.1.9 (executor 1) (109/200)
20/12/04 16:25:07 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 113, 192.168.1.9, executor 1, partition 1, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:07 INFO TaskSetManager: Finished task 199.0 in stage 3.0 (TID 112) in 1192 ms on 192.168.1.9 (executor 1) (110/200)
20/12/04 16:25:07 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 114, 192.168.1.9, executor 1, partition 8, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:07 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 113) in 117 ms on 192.168.1.9 (executor 1) (111/200)
20/12/04 16:25:07 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 115, 192.168.1.9, executor 1, partition 9, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:07 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 114) in 126 ms on 192.168.1.9 (executor 1) (112/200)
20/12/04 16:25:07 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 116, 192.168.1.9, executor 1, partition 15, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:07 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 115) in 116 ms on 192.168.1.9 (executor 1) (113/200)
20/12/04 16:25:07 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 117, 192.168.1.9, executor 1, partition 16, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:07 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 116) in 112 ms on 192.168.1.9 (executor 1) (114/200)
20/12/04 16:25:07 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 118, 192.168.1.9, executor 1, partition 17, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:07 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 117) in 112 ms on 192.168.1.9 (executor 1) (115/200)
20/12/04 16:25:07 INFO TaskSetManager: Starting task 20.0 in stage 3.0 (TID 119, 192.168.1.9, executor 1, partition 20, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:07 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 118) in 118 ms on 192.168.1.9 (executor 1) (116/200)
20/12/04 16:25:07 INFO TaskSetManager: Starting task 22.0 in stage 3.0 (TID 120, 192.168.1.9, executor 1, partition 22, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:07 INFO TaskSetManager: Finished task 20.0 in stage 3.0 (TID 119) in 121 ms on 192.168.1.9 (executor 1) (117/200)
20/12/04 16:25:08 INFO TaskSetManager: Starting task 25.0 in stage 3.0 (TID 121, 192.168.1.9, executor 1, partition 25, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:08 INFO TaskSetManager: Finished task 22.0 in stage 3.0 (TID 120) in 111 ms on 192.168.1.9 (executor 1) (118/200)
20/12/04 16:25:08 INFO TaskSetManager: Starting task 26.0 in stage 3.0 (TID 122, 192.168.1.9, executor 1, partition 26, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:08 INFO TaskSetManager: Finished task 25.0 in stage 3.0 (TID 121) in 114 ms on 192.168.1.9 (executor 1) (119/200)
20/12/04 16:25:08 INFO TaskSetManager: Starting task 28.0 in stage 3.0 (TID 123, 192.168.1.9, executor 1, partition 28, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:08 INFO TaskSetManager: Finished task 26.0 in stage 3.0 (TID 122) in 116 ms on 192.168.1.9 (executor 1) (120/200)
20/12/04 16:25:08 INFO TaskSetManager: Starting task 29.0 in stage 3.0 (TID 124, 192.168.1.9, executor 1, partition 29, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:08 INFO TaskSetManager: Finished task 28.0 in stage 3.0 (TID 123) in 112 ms on 192.168.1.9 (executor 1) (121/200)
20/12/04 16:25:08 INFO TaskSetManager: Starting task 33.0 in stage 3.0 (TID 125, 192.168.1.9, executor 1, partition 33, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:08 INFO TaskSetManager: Finished task 29.0 in stage 3.0 (TID 124) in 113 ms on 192.168.1.9 (executor 1) (122/200)
20/12/04 16:25:08 INFO TaskSetManager: Starting task 38.0 in stage 3.0 (TID 126, 192.168.1.9, executor 1, partition 38, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:08 INFO TaskSetManager: Finished task 33.0 in stage 3.0 (TID 125) in 132 ms on 192.168.1.9 (executor 1) (123/200)
20/12/04 16:25:08 INFO TaskSetManager: Starting task 39.0 in stage 3.0 (TID 127, 192.168.1.9, executor 1, partition 39, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:08 INFO TaskSetManager: Finished task 38.0 in stage 3.0 (TID 126) in 112 ms on 192.168.1.9 (executor 1) (124/200)
20/12/04 16:25:08 INFO TaskSetManager: Starting task 40.0 in stage 3.0 (TID 128, 192.168.1.9, executor 1, partition 40, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:08 INFO TaskSetManager: Finished task 39.0 in stage 3.0 (TID 127) in 116 ms on 192.168.1.9 (executor 1) (125/200)
20/12/04 16:25:08 INFO TaskSetManager: Starting task 42.0 in stage 3.0 (TID 129, 192.168.1.9, executor 1, partition 42, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:08 INFO TaskSetManager: Finished task 40.0 in stage 3.0 (TID 128) in 116 ms on 192.168.1.9 (executor 1) (126/200)
20/12/04 16:25:09 INFO TaskSetManager: Starting task 45.0 in stage 3.0 (TID 130, 192.168.1.9, executor 1, partition 45, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:09 INFO TaskSetManager: Finished task 42.0 in stage 3.0 (TID 129) in 111 ms on 192.168.1.9 (executor 1) (127/200)
20/12/04 16:25:09 INFO TaskSetManager: Starting task 46.0 in stage 3.0 (TID 131, 192.168.1.9, executor 1, partition 46, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:09 INFO TaskSetManager: Finished task 45.0 in stage 3.0 (TID 130) in 114 ms on 192.168.1.9 (executor 1) (128/200)
20/12/04 16:25:09 INFO TaskSetManager: Starting task 47.0 in stage 3.0 (TID 132, 192.168.1.9, executor 1, partition 47, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:09 INFO TaskSetManager: Finished task 46.0 in stage 3.0 (TID 131) in 112 ms on 192.168.1.9 (executor 1) (129/200)
20/12/04 16:25:09 INFO TaskSetManager: Starting task 50.0 in stage 3.0 (TID 133, 192.168.1.9, executor 1, partition 50, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:09 INFO TaskSetManager: Finished task 47.0 in stage 3.0 (TID 132) in 118 ms on 192.168.1.9 (executor 1) (130/200)
20/12/04 16:25:09 INFO TaskSetManager: Starting task 52.0 in stage 3.0 (TID 134, 192.168.1.9, executor 1, partition 52, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:09 INFO TaskSetManager: Finished task 50.0 in stage 3.0 (TID 133) in 117 ms on 192.168.1.9 (executor 1) (131/200)
20/12/04 16:25:09 INFO TaskSetManager: Starting task 54.0 in stage 3.0 (TID 135, 192.168.1.9, executor 1, partition 54, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:09 INFO TaskSetManager: Finished task 52.0 in stage 3.0 (TID 134) in 120 ms on 192.168.1.9 (executor 1) (132/200)
20/12/04 16:25:09 INFO TaskSetManager: Starting task 56.0 in stage 3.0 (TID 136, 192.168.1.9, executor 1, partition 56, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:09 INFO TaskSetManager: Finished task 54.0 in stage 3.0 (TID 135) in 111 ms on 192.168.1.9 (executor 1) (133/200)
20/12/04 16:25:09 INFO TaskSetManager: Starting task 59.0 in stage 3.0 (TID 137, 192.168.1.9, executor 1, partition 59, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:09 INFO TaskSetManager: Finished task 56.0 in stage 3.0 (TID 136) in 116 ms on 192.168.1.9 (executor 1) (134/200)
20/12/04 16:25:10 INFO TaskSetManager: Starting task 60.0 in stage 3.0 (TID 138, 192.168.1.9, executor 1, partition 60, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:10 INFO TaskSetManager: Finished task 59.0 in stage 3.0 (TID 137) in 109 ms on 192.168.1.9 (executor 1) (135/200)
20/12/04 16:25:10 INFO TaskSetManager: Starting task 62.0 in stage 3.0 (TID 139, 192.168.1.9, executor 1, partition 62, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:10 INFO TaskSetManager: Finished task 60.0 in stage 3.0 (TID 138) in 115 ms on 192.168.1.9 (executor 1) (136/200)
20/12/04 16:25:10 INFO TaskSetManager: Starting task 64.0 in stage 3.0 (TID 140, 192.168.1.9, executor 1, partition 64, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:10 INFO TaskSetManager: Finished task 62.0 in stage 3.0 (TID 139) in 129 ms on 192.168.1.9 (executor 1) (137/200)
20/12/04 16:25:10 INFO TaskSetManager: Starting task 67.0 in stage 3.0 (TID 141, 192.168.1.9, executor 1, partition 67, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:10 INFO TaskSetManager: Finished task 64.0 in stage 3.0 (TID 140) in 114 ms on 192.168.1.9 (executor 1) (138/200)
20/12/04 16:25:10 INFO TaskSetManager: Starting task 68.0 in stage 3.0 (TID 142, 192.168.1.9, executor 1, partition 68, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:10 INFO TaskSetManager: Finished task 67.0 in stage 3.0 (TID 141) in 114 ms on 192.168.1.9 (executor 1) (139/200)
20/12/04 16:25:10 INFO TaskSetManager: Starting task 71.0 in stage 3.0 (TID 143, 192.168.1.9, executor 1, partition 71, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:10 INFO TaskSetManager: Finished task 68.0 in stage 3.0 (TID 142) in 113 ms on 192.168.1.9 (executor 1) (140/200)
20/12/04 16:25:10 INFO TaskSetManager: Starting task 72.0 in stage 3.0 (TID 144, 192.168.1.9, executor 1, partition 72, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:10 INFO TaskSetManager: Finished task 71.0 in stage 3.0 (TID 143) in 110 ms on 192.168.1.9 (executor 1) (141/200)
20/12/04 16:25:10 INFO TaskSetManager: Starting task 74.0 in stage 3.0 (TID 145, 192.168.1.9, executor 1, partition 74, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:10 INFO TaskSetManager: Finished task 72.0 in stage 3.0 (TID 144) in 111 ms on 192.168.1.9 (executor 1) (142/200)
20/12/04 16:25:10 INFO TaskSetManager: Starting task 76.0 in stage 3.0 (TID 146, 192.168.1.9, executor 1, partition 76, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:10 INFO TaskSetManager: Finished task 74.0 in stage 3.0 (TID 145) in 110 ms on 192.168.1.9 (executor 1) (143/200)
20/12/04 16:25:11 INFO TaskSetManager: Starting task 78.0 in stage 3.0 (TID 147, 192.168.1.9, executor 1, partition 78, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:11 INFO TaskSetManager: Finished task 76.0 in stage 3.0 (TID 146) in 111 ms on 192.168.1.9 (executor 1) (144/200)
20/12/04 16:25:11 INFO TaskSetManager: Starting task 80.0 in stage 3.0 (TID 148, 192.168.1.9, executor 1, partition 80, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:11 INFO TaskSetManager: Finished task 78.0 in stage 3.0 (TID 147) in 131 ms on 192.168.1.9 (executor 1) (145/200)
20/12/04 16:25:11 INFO TaskSetManager: Starting task 81.0 in stage 3.0 (TID 149, 192.168.1.9, executor 1, partition 81, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:11 INFO TaskSetManager: Finished task 80.0 in stage 3.0 (TID 148) in 109 ms on 192.168.1.9 (executor 1) (146/200)
20/12/04 16:25:11 INFO TaskSetManager: Starting task 82.0 in stage 3.0 (TID 150, 192.168.1.9, executor 1, partition 82, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:11 INFO TaskSetManager: Finished task 81.0 in stage 3.0 (TID 149) in 110 ms on 192.168.1.9 (executor 1) (147/200)
20/12/04 16:25:11 INFO TaskSetManager: Starting task 83.0 in stage 3.0 (TID 151, 192.168.1.9, executor 1, partition 83, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:11 INFO TaskSetManager: Finished task 82.0 in stage 3.0 (TID 150) in 110 ms on 192.168.1.9 (executor 1) (148/200)
20/12/04 16:25:11 INFO TaskSetManager: Starting task 87.0 in stage 3.0 (TID 152, 192.168.1.9, executor 1, partition 87, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:11 INFO TaskSetManager: Finished task 83.0 in stage 3.0 (TID 151) in 109 ms on 192.168.1.9 (executor 1) (149/200)
20/12/04 16:25:11 INFO TaskSetManager: Starting task 90.0 in stage 3.0 (TID 153, 192.168.1.9, executor 1, partition 90, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:11 INFO TaskSetManager: Finished task 87.0 in stage 3.0 (TID 152) in 115 ms on 192.168.1.9 (executor 1) (150/200)
20/12/04 16:25:11 INFO TaskSetManager: Starting task 92.0 in stage 3.0 (TID 154, 192.168.1.9, executor 1, partition 92, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:11 INFO TaskSetManager: Finished task 90.0 in stage 3.0 (TID 153) in 111 ms on 192.168.1.9 (executor 1) (151/200)
20/12/04 16:25:11 INFO TaskSetManager: Starting task 93.0 in stage 3.0 (TID 155, 192.168.1.9, executor 1, partition 93, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:11 INFO TaskSetManager: Finished task 92.0 in stage 3.0 (TID 154) in 111 ms on 192.168.1.9 (executor 1) (152/200)
20/12/04 16:25:12 INFO TaskSetManager: Starting task 94.0 in stage 3.0 (TID 156, 192.168.1.9, executor 1, partition 94, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:12 INFO TaskSetManager: Finished task 93.0 in stage 3.0 (TID 155) in 110 ms on 192.168.1.9 (executor 1) (153/200)
20/12/04 16:25:12 INFO TaskSetManager: Starting task 96.0 in stage 3.0 (TID 157, 192.168.1.9, executor 1, partition 96, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:12 INFO TaskSetManager: Finished task 94.0 in stage 3.0 (TID 156) in 109 ms on 192.168.1.9 (executor 1) (154/200)
20/12/04 16:25:12 INFO TaskSetManager: Starting task 97.0 in stage 3.0 (TID 158, 192.168.1.9, executor 1, partition 97, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:12 INFO TaskSetManager: Finished task 96.0 in stage 3.0 (TID 157) in 142 ms on 192.168.1.9 (executor 1) (155/200)
20/12/04 16:25:12 INFO TaskSetManager: Starting task 98.0 in stage 3.0 (TID 159, 192.168.1.9, executor 1, partition 98, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:12 INFO TaskSetManager: Finished task 97.0 in stage 3.0 (TID 158) in 110 ms on 192.168.1.9 (executor 1) (156/200)
20/12/04 16:25:12 INFO TaskSetManager: Starting task 99.0 in stage 3.0 (TID 160, 192.168.1.9, executor 1, partition 99, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:12 INFO TaskSetManager: Finished task 98.0 in stage 3.0 (TID 159) in 110 ms on 192.168.1.9 (executor 1) (157/200)
20/12/04 16:25:12 INFO TaskSetManager: Starting task 100.0 in stage 3.0 (TID 161, 192.168.1.9, executor 1, partition 100, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:12 INFO TaskSetManager: Finished task 99.0 in stage 3.0 (TID 160) in 112 ms on 192.168.1.9 (executor 1) (158/200)
20/12/04 16:25:12 INFO TaskSetManager: Starting task 104.0 in stage 3.0 (TID 162, 192.168.1.9, executor 1, partition 104, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:12 INFO TaskSetManager: Finished task 100.0 in stage 3.0 (TID 161) in 112 ms on 192.168.1.9 (executor 1) (159/200)
20/12/04 16:25:12 INFO TaskSetManager: Starting task 110.0 in stage 3.0 (TID 163, 192.168.1.9, executor 1, partition 110, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:12 INFO TaskSetManager: Finished task 104.0 in stage 3.0 (TID 162) in 109 ms on 192.168.1.9 (executor 1) (160/200)
20/12/04 16:25:12 INFO TaskSetManager: Starting task 112.0 in stage 3.0 (TID 164, 192.168.1.9, executor 1, partition 112, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:12 INFO TaskSetManager: Finished task 110.0 in stage 3.0 (TID 163) in 115 ms on 192.168.1.9 (executor 1) (161/200)
20/12/04 16:25:13 INFO TaskSetManager: Starting task 114.0 in stage 3.0 (TID 165, 192.168.1.9, executor 1, partition 114, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:13 INFO TaskSetManager: Finished task 112.0 in stage 3.0 (TID 164) in 109 ms on 192.168.1.9 (executor 1) (162/200)
20/12/04 16:25:13 INFO TaskSetManager: Starting task 117.0 in stage 3.0 (TID 166, 192.168.1.9, executor 1, partition 117, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:13 INFO TaskSetManager: Finished task 114.0 in stage 3.0 (TID 165) in 110 ms on 192.168.1.9 (executor 1) (163/200)
20/12/04 16:25:13 INFO TaskSetManager: Starting task 118.0 in stage 3.0 (TID 167, 192.168.1.9, executor 1, partition 118, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:13 INFO TaskSetManager: Finished task 117.0 in stage 3.0 (TID 166) in 128 ms on 192.168.1.9 (executor 1) (164/200)
20/12/04 16:25:13 INFO TaskSetManager: Starting task 120.0 in stage 3.0 (TID 168, 192.168.1.9, executor 1, partition 120, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:13 INFO TaskSetManager: Finished task 118.0 in stage 3.0 (TID 167) in 109 ms on 192.168.1.9 (executor 1) (165/200)
20/12/04 16:25:13 INFO TaskSetManager: Starting task 121.0 in stage 3.0 (TID 169, 192.168.1.9, executor 1, partition 121, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:13 INFO TaskSetManager: Finished task 120.0 in stage 3.0 (TID 168) in 122 ms on 192.168.1.9 (executor 1) (166/200)
20/12/04 16:25:13 INFO TaskSetManager: Starting task 123.0 in stage 3.0 (TID 170, 192.168.1.9, executor 1, partition 123, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:13 INFO TaskSetManager: Finished task 121.0 in stage 3.0 (TID 169) in 109 ms on 192.168.1.9 (executor 1) (167/200)
20/12/04 16:25:13 INFO TaskSetManager: Starting task 125.0 in stage 3.0 (TID 171, 192.168.1.9, executor 1, partition 125, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:13 INFO TaskSetManager: Finished task 123.0 in stage 3.0 (TID 170) in 120 ms on 192.168.1.9 (executor 1) (168/200)
20/12/04 16:25:13 INFO TaskSetManager: Starting task 127.0 in stage 3.0 (TID 172, 192.168.1.9, executor 1, partition 127, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:13 INFO TaskSetManager: Finished task 125.0 in stage 3.0 (TID 171) in 112 ms on 192.168.1.9 (executor 1) (169/200)
20/12/04 16:25:13 INFO TaskSetManager: Starting task 130.0 in stage 3.0 (TID 173, 192.168.1.9, executor 1, partition 130, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:13 INFO TaskSetManager: Finished task 127.0 in stage 3.0 (TID 172) in 113 ms on 192.168.1.9 (executor 1) (170/200)
20/12/04 16:25:14 INFO TaskSetManager: Starting task 134.0 in stage 3.0 (TID 174, 192.168.1.9, executor 1, partition 134, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:14 INFO TaskSetManager: Finished task 130.0 in stage 3.0 (TID 173) in 109 ms on 192.168.1.9 (executor 1) (171/200)
20/12/04 16:25:14 INFO TaskSetManager: Starting task 138.0 in stage 3.0 (TID 175, 192.168.1.9, executor 1, partition 138, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:14 INFO TaskSetManager: Finished task 134.0 in stage 3.0 (TID 174) in 120 ms on 192.168.1.9 (executor 1) (172/200)
20/12/04 16:25:14 INFO TaskSetManager: Starting task 142.0 in stage 3.0 (TID 176, 192.168.1.9, executor 1, partition 142, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:14 INFO TaskSetManager: Finished task 138.0 in stage 3.0 (TID 175) in 114 ms on 192.168.1.9 (executor 1) (173/200)
20/12/04 16:25:14 INFO TaskSetManager: Starting task 144.0 in stage 3.0 (TID 177, 192.168.1.9, executor 1, partition 144, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:14 INFO TaskSetManager: Finished task 142.0 in stage 3.0 (TID 176) in 114 ms on 192.168.1.9 (executor 1) (174/200)
20/12/04 16:25:14 INFO TaskSetManager: Starting task 145.0 in stage 3.0 (TID 178, 192.168.1.9, executor 1, partition 145, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:14 INFO TaskSetManager: Finished task 144.0 in stage 3.0 (TID 177) in 113 ms on 192.168.1.9 (executor 1) (175/200)
20/12/04 16:25:14 INFO TaskSetManager: Starting task 147.0 in stage 3.0 (TID 179, 192.168.1.9, executor 1, partition 147, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:14 INFO TaskSetManager: Finished task 145.0 in stage 3.0 (TID 178) in 109 ms on 192.168.1.9 (executor 1) (176/200)
20/12/04 16:25:14 INFO TaskSetManager: Starting task 148.0 in stage 3.0 (TID 180, 192.168.1.9, executor 1, partition 148, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:14 INFO TaskSetManager: Finished task 147.0 in stage 3.0 (TID 179) in 111 ms on 192.168.1.9 (executor 1) (177/200)
20/12/04 16:25:14 INFO TaskSetManager: Starting task 149.0 in stage 3.0 (TID 181, 192.168.1.9, executor 1, partition 149, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:14 INFO TaskSetManager: Finished task 148.0 in stage 3.0 (TID 180) in 152 ms on 192.168.1.9 (executor 1) (178/200)
20/12/04 16:25:15 INFO TaskSetManager: Starting task 152.0 in stage 3.0 (TID 182, 192.168.1.9, executor 1, partition 152, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:15 INFO TaskSetManager: Finished task 149.0 in stage 3.0 (TID 181) in 110 ms on 192.168.1.9 (executor 1) (179/200)
20/12/04 16:25:15 INFO TaskSetManager: Starting task 154.0 in stage 3.0 (TID 183, 192.168.1.9, executor 1, partition 154, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:15 INFO TaskSetManager: Finished task 152.0 in stage 3.0 (TID 182) in 111 ms on 192.168.1.9 (executor 1) (180/200)
20/12/04 16:25:15 INFO TaskSetManager: Starting task 157.0 in stage 3.0 (TID 184, 192.168.1.9, executor 1, partition 157, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:15 INFO TaskSetManager: Finished task 154.0 in stage 3.0 (TID 183) in 112 ms on 192.168.1.9 (executor 1) (181/200)
20/12/04 16:25:15 INFO TaskSetManager: Starting task 158.0 in stage 3.0 (TID 185, 192.168.1.9, executor 1, partition 158, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:15 INFO TaskSetManager: Finished task 157.0 in stage 3.0 (TID 184) in 110 ms on 192.168.1.9 (executor 1) (182/200)
20/12/04 16:25:15 INFO TaskSetManager: Starting task 159.0 in stage 3.0 (TID 186, 192.168.1.9, executor 1, partition 159, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:15 INFO TaskSetManager: Finished task 158.0 in stage 3.0 (TID 185) in 111 ms on 192.168.1.9 (executor 1) (183/200)
20/12/04 16:25:15 INFO TaskSetManager: Starting task 160.0 in stage 3.0 (TID 187, 192.168.1.9, executor 1, partition 160, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:15 INFO TaskSetManager: Finished task 159.0 in stage 3.0 (TID 186) in 118 ms on 192.168.1.9 (executor 1) (184/200)
20/12/04 16:25:15 INFO TaskSetManager: Starting task 166.0 in stage 3.0 (TID 188, 192.168.1.9, executor 1, partition 166, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:15 INFO TaskSetManager: Finished task 160.0 in stage 3.0 (TID 187) in 110 ms on 192.168.1.9 (executor 1) (185/200)
20/12/04 16:25:15 INFO TaskSetManager: Starting task 169.0 in stage 3.0 (TID 189, 192.168.1.9, executor 1, partition 169, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:15 INFO TaskSetManager: Finished task 166.0 in stage 3.0 (TID 188) in 111 ms on 192.168.1.9 (executor 1) (186/200)
20/12/04 16:25:15 INFO TaskSetManager: Starting task 171.0 in stage 3.0 (TID 190, 192.168.1.9, executor 1, partition 171, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:15 INFO TaskSetManager: Finished task 169.0 in stage 3.0 (TID 189) in 111 ms on 192.168.1.9 (executor 1) (187/200)
20/12/04 16:25:16 INFO TaskSetManager: Starting task 176.0 in stage 3.0 (TID 191, 192.168.1.9, executor 1, partition 176, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:16 INFO TaskSetManager: Finished task 171.0 in stage 3.0 (TID 190) in 111 ms on 192.168.1.9 (executor 1) (188/200)
20/12/04 16:25:16 INFO TaskSetManager: Starting task 177.0 in stage 3.0 (TID 192, 192.168.1.9, executor 1, partition 177, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:16 INFO TaskSetManager: Finished task 176.0 in stage 3.0 (TID 191) in 112 ms on 192.168.1.9 (executor 1) (189/200)
20/12/04 16:25:16 INFO TaskSetManager: Starting task 180.0 in stage 3.0 (TID 193, 192.168.1.9, executor 1, partition 180, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:16 INFO TaskSetManager: Finished task 177.0 in stage 3.0 (TID 192) in 131 ms on 192.168.1.9 (executor 1) (190/200)
20/12/04 16:25:16 INFO TaskSetManager: Starting task 182.0 in stage 3.0 (TID 194, 192.168.1.9, executor 1, partition 182, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:16 INFO TaskSetManager: Finished task 180.0 in stage 3.0 (TID 193) in 111 ms on 192.168.1.9 (executor 1) (191/200)
20/12/04 16:25:16 INFO TaskSetManager: Starting task 185.0 in stage 3.0 (TID 195, 192.168.1.9, executor 1, partition 185, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:16 INFO TaskSetManager: Finished task 182.0 in stage 3.0 (TID 194) in 111 ms on 192.168.1.9 (executor 1) (192/200)
20/12/04 16:25:16 INFO TaskSetManager: Starting task 186.0 in stage 3.0 (TID 196, 192.168.1.9, executor 1, partition 186, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:16 INFO TaskSetManager: Finished task 185.0 in stage 3.0 (TID 195) in 109 ms on 192.168.1.9 (executor 1) (193/200)
20/12/04 16:25:16 INFO TaskSetManager: Starting task 187.0 in stage 3.0 (TID 197, 192.168.1.9, executor 1, partition 187, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:16 INFO TaskSetManager: Finished task 186.0 in stage 3.0 (TID 196) in 110 ms on 192.168.1.9 (executor 1) (194/200)
20/12/04 16:25:16 INFO TaskSetManager: Starting task 188.0 in stage 3.0 (TID 198, 192.168.1.9, executor 1, partition 188, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:16 INFO TaskSetManager: Finished task 187.0 in stage 3.0 (TID 197) in 109 ms on 192.168.1.9 (executor 1) (195/200)
20/12/04 16:25:16 INFO TaskSetManager: Starting task 189.0 in stage 3.0 (TID 199, 192.168.1.9, executor 1, partition 189, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:16 INFO TaskSetManager: Finished task 188.0 in stage 3.0 (TID 198) in 116 ms on 192.168.1.9 (executor 1) (196/200)
20/12/04 16:25:17 INFO TaskSetManager: Starting task 191.0 in stage 3.0 (TID 200, 192.168.1.9, executor 1, partition 191, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:17 INFO TaskSetManager: Finished task 189.0 in stage 3.0 (TID 199) in 110 ms on 192.168.1.9 (executor 1) (197/200)
20/12/04 16:25:17 INFO TaskSetManager: Starting task 196.0 in stage 3.0 (TID 201, 192.168.1.9, executor 1, partition 196, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:17 INFO TaskSetManager: Finished task 191.0 in stage 3.0 (TID 200) in 108 ms on 192.168.1.9 (executor 1) (198/200)
20/12/04 16:25:17 INFO TaskSetManager: Starting task 197.0 in stage 3.0 (TID 202, 192.168.1.9, executor 1, partition 197, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:25:17 INFO TaskSetManager: Finished task 196.0 in stage 3.0 (TID 201) in 109 ms on 192.168.1.9 (executor 1) (199/200)
20/12/04 16:25:17 INFO TaskSetManager: Finished task 197.0 in stage 3.0 (TID 202) in 111 ms on 192.168.1.9 (executor 1) (200/200)
20/12/04 16:25:17 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 16:25:17 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/userFiles-b4b3d0e1-4754-4c65-936d-321ae7be5c5b/darima.zip/darima/dlsa.py:22) finished in 167.306 s
20/12/04 16:25:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:25:17 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
20/12/04 16:25:17 INFO DAGScheduler: Job 1 finished: toPandas at /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/userFiles-b4b3d0e1-4754-4c65-936d-321ae7be5c5b/darima.zip/darima/dlsa.py:22, took 168.686860 s
20/12/04 16:25:18 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:25:18 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:25:18 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:25:18 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:25:18 INFO CodeGenerator: Code generated in 14.596901 ms
20/12/04 16:25:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:25:18 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:25:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:35837 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:25:18 INFO SparkContext: Created broadcast 6 from toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201
20/12/04 16:25:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:25:18 INFO SparkContext: Starting job: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201
20/12/04 16:25:18 INFO DAGScheduler: Got job 2 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201) with 1 output partitions
20/12/04 16:25:18 INFO DAGScheduler: Final stage: ResultStage 4 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201)
20/12/04 16:25:18 INFO DAGScheduler: Parents of final stage: List()
20/12/04 16:25:18 INFO DAGScheduler: Missing parents: List()
20/12/04 16:25:18 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201), which has no missing parents
20/12/04 16:25:18 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 21.7 KiB, free 5.8 GiB)
20/12/04 16:25:18 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 5.8 GiB)
20/12/04 16:25:18 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:35837 (size: 10.4 KiB, free: 5.8 GiB)
20/12/04 16:25:18 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1223
20/12/04 16:25:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201) (first 15 tasks are for partitions Vector(0))
20/12/04 16:25:18 INFO YarnScheduler: Adding task set 4.0 with 1 tasks
20/12/04 16:25:18 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 203, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7790 bytes)
20/12/04 16:25:18 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:42185 (size: 10.4 KiB, free: 912.1 MiB)
20/12/04 16:25:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:42185 (size: 28.7 KiB, free: 912.1 MiB)
20/12/04 16:25:18 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 203) in 433 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:25:18 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/12/04 16:25:18 INFO DAGScheduler: ResultStage 4 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201) finished in 0.445 s
20/12/04 16:25:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:25:18 INFO YarnScheduler: Killing all running tasks in stage 4: Stage finished
20/12/04 16:25:18 INFO DAGScheduler: Job 2 finished: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201, took 0.451627 s
20/12/04 16:25:23 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:25:23 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:25:23 INFO FileSourceStrategy: Post-Scan Filters: 
20/12/04 16:25:23 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:25:23 INFO CodeGenerator: Code generated in 12.766603 ms
20/12/04 16:25:23 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:25:23 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:25:23 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:35837 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:25:23 INFO SparkContext: Created broadcast 8 from toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217
20/12/04 16:25:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:25:23 INFO SparkContext: Starting job: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217
20/12/04 16:25:23 INFO DAGScheduler: Got job 3 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217) with 1 output partitions
20/12/04 16:25:23 INFO DAGScheduler: Final stage: ResultStage 5 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217)
20/12/04 16:25:23 INFO DAGScheduler: Parents of final stage: List()
20/12/04 16:25:23 INFO DAGScheduler: Missing parents: List()
20/12/04 16:25:23 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217), which has no missing parents
20/12/04 16:25:23 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 12.3 KiB, free 5.8 GiB)
20/12/04 16:25:23 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 5.8 GiB)
20/12/04 16:25:23 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:35837 (size: 6.3 KiB, free: 5.8 GiB)
20/12/04 16:25:23 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1223
20/12/04 16:25:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217) (first 15 tasks are for partitions Vector(0))
20/12/04 16:25:23 INFO YarnScheduler: Adding task set 5.0 with 1 tasks
20/12/04 16:25:23 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 204, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7789 bytes)
20/12/04 16:25:23 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:42185 (size: 6.3 KiB, free: 912.1 MiB)
20/12/04 16:25:23 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:42185 (size: 28.7 KiB, free: 912.1 MiB)
20/12/04 16:25:23 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 204) in 104 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:25:23 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/12/04 16:25:23 INFO DAGScheduler: ResultStage 5 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217) finished in 0.121 s
20/12/04 16:25:23 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:25:23 INFO YarnScheduler: Killing all running tasks in stage 5: Stage finished
20/12/04 16:25:23 INFO DAGScheduler: Job 3 finished: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217, took 0.128608 s
20/12/04 16:25:27 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 16:25:27 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 16:25:27 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 16:25:27 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 16:25:27 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 16:25:27 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 16:25:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 16:25:27 INFO MemoryStore: MemoryStore cleared
20/12/04 16:25:27 INFO BlockManager: BlockManager stopped
20/12/04 16:25:27 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 16:25:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 16:25:27 INFO SparkContext: Successfully stopped SparkContext
20/12/04 16:25:27 INFO ShutdownHookManager: Shutdown hook called
20/12/04 16:25:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f/pyspark-e3624acc-fc6a-40fd-9679-f538286c1e10
20/12/04 16:25:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-05be3ad3-ffd7-463d-b01b-e949023b143f
20/12/04 16:25:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-e87aa944-9a63-4da9-9909-11332dbdbbf1
20/12/04 16:25:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 16:25:30 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:25:30 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:25:30 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:25:30 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:25:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:25:30 INFO SparkContext: Running Spark version 3.0.1
20/12/04 16:25:30 INFO ResourceUtils: ==============================================================
20/12/04 16:25:30 INFO ResourceUtils: Resources for spark.driver:

20/12/04 16:25:30 INFO ResourceUtils: ==============================================================
20/12/04 16:25:30 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 16:25:31 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:25:31 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:25:31 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:25:31 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:25:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:25:31 INFO Utils: Successfully started service 'sparkDriver' on port 34691.
20/12/04 16:25:31 INFO SparkEnv: Registering MapOutputTracker
20/12/04 16:25:31 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 16:25:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 16:25:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 16:25:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 16:25:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f240ea7f-07f9-4e99-8ddd-24d9fc16a1e8
20/12/04 16:25:31 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 16:25:31 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 16:25:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 16:25:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 16:25:32 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 16:25:32 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 16:25:33 INFO Configuration: resource-types.xml not found
20/12/04 16:25:33 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 16:25:33 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 16:25:33 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 16:25:33 INFO Client: Setting up container launch context for our AM
20/12/04 16:25:33 INFO Client: Setting up the launch environment for our AM container
20/12/04 16:25:33 INFO Client: Preparing resources for our AM container
20/12/04 16:25:33 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 16:25:35 INFO Client: Uploading resource file:/tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/__spark_libs__4166203882047413068.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0077/__spark_libs__4166203882047413068.zip
20/12/04 16:25:36 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0077/pyspark.zip
20/12/04 16:25:36 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0077/py4j-0.10.9-src.zip
20/12/04 16:25:36 INFO Client: Uploading resource file:/tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/__spark_conf__10698644596823375235.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0077/__spark_conf__.zip
20/12/04 16:25:36 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:25:36 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:25:36 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:25:36 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:25:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:25:36 INFO Client: Submitting application application_1607015337794_0077 to ResourceManager
20/12/04 16:25:36 INFO YarnClientImpl: Submitted application application_1607015337794_0077
20/12/04 16:25:37 INFO Client: Application report for application_1607015337794_0077 (state: ACCEPTED)
20/12/04 16:25:37 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607117136783
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0077/
	 user: bsuconn
20/12/04 16:25:38 INFO Client: Application report for application_1607015337794_0077 (state: ACCEPTED)
20/12/04 16:25:39 INFO Client: Application report for application_1607015337794_0077 (state: ACCEPTED)
20/12/04 16:25:40 INFO Client: Application report for application_1607015337794_0077 (state: ACCEPTED)
20/12/04 16:25:41 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0077), /proxy/application_1607015337794_0077
20/12/04 16:25:41 INFO Client: Application report for application_1607015337794_0077 (state: RUNNING)
20/12/04 16:25:41 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607117136783
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0077/
	 user: bsuconn
20/12/04 16:25:41 INFO YarnClientSchedulerBackend: Application application_1607015337794_0077 has started running.
20/12/04 16:25:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39465.
20/12/04 16:25:41 INFO NettyBlockTransferService: Server created on 192.168.1.9:39465
20/12/04 16:25:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 16:25:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 39465, None)
20/12/04 16:25:41 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:39465 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 39465, None)
20/12/04 16:25:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 39465, None)
20/12/04 16:25:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 39465, None)
20/12/04 16:25:42 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:25:42 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 16:25:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 16:25:47 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:58920) with ID 1
20/12/04 16:25:47 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40239 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 40239, None)
20/12/04 16:25:48 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:58924) with ID 2
20/12/04 16:25:48 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 16:25:48 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:39029 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 39029, None)
20/12/04 16:25:48 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 16:25:48 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 16:25:48 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 16:25:48 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:25:48 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:25:48 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:25:48 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:25:48 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:25:49 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:34691/files/darima.zip with timestamp 1607117149605
20/12/04 16:25:49 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/userFiles-07633795-2abd-4249-ab93-f56b1a9ef78c/darima.zip
20/12/04 16:25:51 INFO InMemoryFileIndex: It took 57 ms to list leaf files for 1 paths.
20/12/04 16:25:53 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
20/12/04 16:25:53 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:25:53 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:25:53 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:25:53 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:25:54 INFO CodeGenerator: Code generated in 275.440698 ms
20/12/04 16:25:54 INFO CodeGenerator: Code generated in 31.756771 ms
20/12/04 16:25:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:25:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:25:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:39465 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:25:54 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 16:25:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:25:54 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 16:25:55 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 16:25:55 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 16:25:55 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 16:25:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 16:25:55 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 16:25:55 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:25:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 16:25:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 16:25:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:39465 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:25:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 16:25:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:25:55 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 16:25:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:25:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:39029 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:25:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:39029 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:25:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3366 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 16:25:58 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 16:25:58 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.512 s
20/12/04 16:25:58 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:25:58 INFO DAGScheduler: running: Set()
20/12/04 16:25:58 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 16:25:58 INFO DAGScheduler: failed: Set()
20/12/04 16:25:58 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:25:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 16:25:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 16:25:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:39465 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:25:58 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 16:25:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:25:58 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 16:25:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:25:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:39029 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:25:58 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:58924
20/12/04 16:25:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 386 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 16:25:58 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 16:25:58 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.401 s
20/12/04 16:25:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:25:58 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 16:25:59 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 4.003129 s
20/12/04 16:26:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:39029 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:26:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:39465 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:26:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:39465 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:26:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:39029 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:26:01 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:39029 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:26:01 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:39465 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:26:01 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 16:26:02 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:26:02 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:26:02 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:26:02 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:26:02 INFO CodeGenerator: Code generated in 28.633187 ms
20/12/04 16:26:02 INFO CodeGenerator: Code generated in 21.51313 ms
20/12/04 16:26:02 INFO CodeGenerator: Code generated in 27.318201 ms
20/12/04 16:26:02 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:26:02 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:26:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:39465 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:26:02 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/userFiles-07633795-2abd-4249-ab93-f56b1a9ef78c/darima.zip/darima/dlsa.py:22
20/12/04 16:26:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:26:02 INFO SparkContext: Starting job: toPandas at /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/userFiles-07633795-2abd-4249-ab93-f56b1a9ef78c/darima.zip/darima/dlsa.py:22
20/12/04 16:26:02 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/userFiles-07633795-2abd-4249-ab93-f56b1a9ef78c/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 16:26:02 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/userFiles-07633795-2abd-4249-ab93-f56b1a9ef78c/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 16:26:02 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/userFiles-07633795-2abd-4249-ab93-f56b1a9ef78c/darima.zip/darima/dlsa.py:22)
20/12/04 16:26:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 16:26:02 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 16:26:02 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/userFiles-07633795-2abd-4249-ab93-f56b1a9ef78c/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:26:02 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 16:26:02 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 16:26:02 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:39465 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 16:26:02 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 16:26:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/userFiles-07633795-2abd-4249-ab93-f56b1a9ef78c/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 16:26:02 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 16:26:02 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:26:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:40239 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 16:26:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:40239 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:26:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 4174 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:26:06 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 16:26:06 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 35453
20/12/04 16:26:06 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/userFiles-07633795-2abd-4249-ab93-f56b1a9ef78c/darima.zip/darima/dlsa.py:22) finished in 4.203 s
20/12/04 16:26:06 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:26:06 INFO DAGScheduler: running: Set()
20/12/04 16:26:06 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 16:26:06 INFO DAGScheduler: failed: Set()
20/12/04 16:26:06 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/userFiles-07633795-2abd-4249-ab93-f56b1a9ef78c/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:26:07 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 16:26:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 16:26:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:39465 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 16:26:07 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 16:26:07 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/userFiles-07633795-2abd-4249-ab93-f56b1a9ef78c/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 16:26:07 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 16:26:07 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:07 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:39029 (size: 131.5 KiB, free: 912.2 MiB)
20/12/04 16:26:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:40239 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 16:26:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:58924
20/12/04 16:26:08 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:58920
20/12/04 16:26:15 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:15 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 8887 ms on 192.168.1.9 (executor 1) (1/200)
20/12/04 16:26:16 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 6, 192.168.1.9, executor 2, partition 4, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:16 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 4) in 9007 ms on 192.168.1.9 (executor 2) (2/200)
20/12/04 16:26:17 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 7, 192.168.1.9, executor 2, partition 5, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:17 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 6) in 1780 ms on 192.168.1.9 (executor 2) (3/200)
20/12/04 16:26:19 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 8, 192.168.1.9, executor 2, partition 6, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:19 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 7) in 1320 ms on 192.168.1.9 (executor 2) (4/200)
20/12/04 16:26:19 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 7, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:19 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 5) in 3545 ms on 192.168.1.9 (executor 1) (5/200)
20/12/04 16:26:21 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 10, 192.168.1.9, executor 2, partition 10, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:21 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 8) in 1997 ms on 192.168.1.9 (executor 2) (6/200)
20/12/04 16:26:22 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 11, 192.168.1.9, executor 1, partition 11, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:22 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 9) in 2659 ms on 192.168.1.9 (executor 1) (7/200)
20/12/04 16:26:23 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 12, 192.168.1.9, executor 2, partition 12, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:23 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 10) in 2310 ms on 192.168.1.9 (executor 2) (8/200)
20/12/04 16:26:23 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 13, 192.168.1.9, executor 1, partition 13, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:23 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 11) in 1806 ms on 192.168.1.9 (executor 1) (9/200)
20/12/04 16:26:25 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 14, 192.168.1.9, executor 1, partition 14, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:25 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 13) in 1591 ms on 192.168.1.9 (executor 1) (10/200)
20/12/04 16:26:27 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 15, 192.168.1.9, executor 1, partition 18, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:27 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 14) in 2403 ms on 192.168.1.9 (executor 1) (11/200)
20/12/04 16:26:29 INFO TaskSetManager: Starting task 19.0 in stage 3.0 (TID 16, 192.168.1.9, executor 2, partition 19, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:29 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 12) in 5704 ms on 192.168.1.9 (executor 2) (12/200)
20/12/04 16:26:30 INFO TaskSetManager: Starting task 21.0 in stage 3.0 (TID 17, 192.168.1.9, executor 1, partition 21, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:30 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 15) in 2123 ms on 192.168.1.9 (executor 1) (13/200)
20/12/04 16:26:33 INFO TaskSetManager: Starting task 23.0 in stage 3.0 (TID 18, 192.168.1.9, executor 1, partition 23, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:33 INFO TaskSetManager: Finished task 21.0 in stage 3.0 (TID 17) in 3317 ms on 192.168.1.9 (executor 1) (14/200)
20/12/04 16:26:33 INFO TaskSetManager: Starting task 24.0 in stage 3.0 (TID 19, 192.168.1.9, executor 2, partition 24, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:34 INFO TaskSetManager: Finished task 19.0 in stage 3.0 (TID 16) in 4823 ms on 192.168.1.9 (executor 2) (15/200)
20/12/04 16:26:35 INFO TaskSetManager: Finished task 23.0 in stage 3.0 (TID 18) in 2540 ms on 192.168.1.9 (executor 1) (16/200)
20/12/04 16:26:35 INFO TaskSetManager: Starting task 27.0 in stage 3.0 (TID 20, 192.168.1.9, executor 1, partition 27, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:35 INFO TaskSetManager: Starting task 30.0 in stage 3.0 (TID 21, 192.168.1.9, executor 2, partition 30, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:35 INFO TaskSetManager: Finished task 24.0 in stage 3.0 (TID 19) in 1969 ms on 192.168.1.9 (executor 2) (17/200)
20/12/04 16:26:37 INFO TaskSetManager: Starting task 31.0 in stage 3.0 (TID 22, 192.168.1.9, executor 1, partition 31, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:37 INFO TaskSetManager: Finished task 27.0 in stage 3.0 (TID 20) in 1425 ms on 192.168.1.9 (executor 1) (18/200)
20/12/04 16:26:38 INFO TaskSetManager: Starting task 32.0 in stage 3.0 (TID 23, 192.168.1.9, executor 2, partition 32, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:38 INFO TaskSetManager: Finished task 30.0 in stage 3.0 (TID 21) in 2712 ms on 192.168.1.9 (executor 2) (19/200)
20/12/04 16:26:39 INFO TaskSetManager: Starting task 34.0 in stage 3.0 (TID 24, 192.168.1.9, executor 1, partition 34, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:39 INFO TaskSetManager: Finished task 31.0 in stage 3.0 (TID 22) in 2386 ms on 192.168.1.9 (executor 1) (20/200)
20/12/04 16:26:41 INFO TaskSetManager: Starting task 35.0 in stage 3.0 (TID 25, 192.168.1.9, executor 2, partition 35, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:41 INFO TaskSetManager: Finished task 32.0 in stage 3.0 (TID 23) in 2460 ms on 192.168.1.9 (executor 2) (21/200)
20/12/04 16:26:41 INFO TaskSetManager: Starting task 36.0 in stage 3.0 (TID 26, 192.168.1.9, executor 1, partition 36, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:41 INFO TaskSetManager: Finished task 34.0 in stage 3.0 (TID 24) in 1441 ms on 192.168.1.9 (executor 1) (22/200)
20/12/04 16:26:43 INFO TaskSetManager: Starting task 37.0 in stage 3.0 (TID 27, 192.168.1.9, executor 1, partition 37, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:43 INFO TaskSetManager: Finished task 36.0 in stage 3.0 (TID 26) in 1886 ms on 192.168.1.9 (executor 1) (23/200)
20/12/04 16:26:43 INFO TaskSetManager: Starting task 41.0 in stage 3.0 (TID 28, 192.168.1.9, executor 2, partition 41, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:43 INFO TaskSetManager: Finished task 35.0 in stage 3.0 (TID 25) in 2696 ms on 192.168.1.9 (executor 2) (24/200)
20/12/04 16:26:44 INFO TaskSetManager: Finished task 37.0 in stage 3.0 (TID 27) in 1654 ms on 192.168.1.9 (executor 1) (25/200)
20/12/04 16:26:44 INFO TaskSetManager: Starting task 43.0 in stage 3.0 (TID 29, 192.168.1.9, executor 1, partition 43, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:45 INFO TaskSetManager: Starting task 44.0 in stage 3.0 (TID 30, 192.168.1.9, executor 2, partition 44, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:45 INFO TaskSetManager: Finished task 41.0 in stage 3.0 (TID 28) in 1568 ms on 192.168.1.9 (executor 2) (26/200)
20/12/04 16:26:46 INFO TaskSetManager: Starting task 48.0 in stage 3.0 (TID 31, 192.168.1.9, executor 2, partition 48, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:46 INFO TaskSetManager: Finished task 44.0 in stage 3.0 (TID 30) in 1491 ms on 192.168.1.9 (executor 2) (27/200)
20/12/04 16:26:47 INFO TaskSetManager: Starting task 49.0 in stage 3.0 (TID 32, 192.168.1.9, executor 1, partition 49, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:47 INFO TaskSetManager: Finished task 43.0 in stage 3.0 (TID 29) in 2335 ms on 192.168.1.9 (executor 1) (28/200)
20/12/04 16:26:48 INFO TaskSetManager: Starting task 51.0 in stage 3.0 (TID 33, 192.168.1.9, executor 2, partition 51, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:48 INFO TaskSetManager: Finished task 48.0 in stage 3.0 (TID 31) in 1657 ms on 192.168.1.9 (executor 2) (29/200)
20/12/04 16:26:49 INFO TaskSetManager: Starting task 53.0 in stage 3.0 (TID 34, 192.168.1.9, executor 1, partition 53, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:49 INFO TaskSetManager: Finished task 49.0 in stage 3.0 (TID 32) in 2957 ms on 192.168.1.9 (executor 1) (30/200)
20/12/04 16:26:51 INFO TaskSetManager: Starting task 55.0 in stage 3.0 (TID 35, 192.168.1.9, executor 2, partition 55, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:51 INFO TaskSetManager: Finished task 51.0 in stage 3.0 (TID 33) in 3229 ms on 192.168.1.9 (executor 2) (31/200)
20/12/04 16:26:52 INFO TaskSetManager: Starting task 57.0 in stage 3.0 (TID 36, 192.168.1.9, executor 2, partition 57, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:52 INFO TaskSetManager: Finished task 55.0 in stage 3.0 (TID 35) in 902 ms on 192.168.1.9 (executor 2) (32/200)
20/12/04 16:26:52 INFO TaskSetManager: Starting task 58.0 in stage 3.0 (TID 37, 192.168.1.9, executor 1, partition 58, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:52 INFO TaskSetManager: Finished task 53.0 in stage 3.0 (TID 34) in 2777 ms on 192.168.1.9 (executor 1) (33/200)
20/12/04 16:26:53 INFO TaskSetManager: Starting task 61.0 in stage 3.0 (TID 38, 192.168.1.9, executor 1, partition 61, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:53 INFO TaskSetManager: Finished task 58.0 in stage 3.0 (TID 37) in 1114 ms on 192.168.1.9 (executor 1) (34/200)
20/12/04 16:26:54 INFO TaskSetManager: Starting task 63.0 in stage 3.0 (TID 39, 192.168.1.9, executor 2, partition 63, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:54 INFO TaskSetManager: Finished task 57.0 in stage 3.0 (TID 36) in 1450 ms on 192.168.1.9 (executor 2) (35/200)
20/12/04 16:26:55 INFO TaskSetManager: Starting task 65.0 in stage 3.0 (TID 40, 192.168.1.9, executor 2, partition 65, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:55 INFO TaskSetManager: Finished task 63.0 in stage 3.0 (TID 39) in 1103 ms on 192.168.1.9 (executor 2) (36/200)
20/12/04 16:26:56 INFO TaskSetManager: Starting task 66.0 in stage 3.0 (TID 41, 192.168.1.9, executor 2, partition 66, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:56 INFO TaskSetManager: Finished task 65.0 in stage 3.0 (TID 40) in 877 ms on 192.168.1.9 (executor 2) (37/200)
20/12/04 16:26:56 INFO TaskSetManager: Starting task 69.0 in stage 3.0 (TID 42, 192.168.1.9, executor 1, partition 69, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:56 INFO TaskSetManager: Finished task 61.0 in stage 3.0 (TID 38) in 2643 ms on 192.168.1.9 (executor 1) (38/200)
20/12/04 16:26:57 INFO TaskSetManager: Starting task 70.0 in stage 3.0 (TID 43, 192.168.1.9, executor 2, partition 70, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:57 INFO TaskSetManager: Finished task 66.0 in stage 3.0 (TID 41) in 939 ms on 192.168.1.9 (executor 2) (39/200)
20/12/04 16:26:57 INFO TaskSetManager: Starting task 73.0 in stage 3.0 (TID 44, 192.168.1.9, executor 1, partition 73, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:57 INFO TaskSetManager: Finished task 69.0 in stage 3.0 (TID 42) in 1284 ms on 192.168.1.9 (executor 1) (40/200)
20/12/04 16:26:58 INFO TaskSetManager: Starting task 75.0 in stage 3.0 (TID 45, 192.168.1.9, executor 2, partition 75, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:58 INFO TaskSetManager: Finished task 70.0 in stage 3.0 (TID 43) in 1382 ms on 192.168.1.9 (executor 2) (41/200)
20/12/04 16:26:59 INFO TaskSetManager: Starting task 77.0 in stage 3.0 (TID 46, 192.168.1.9, executor 1, partition 77, NODE_LOCAL, 7336 bytes)
20/12/04 16:26:59 INFO TaskSetManager: Finished task 73.0 in stage 3.0 (TID 44) in 1496 ms on 192.168.1.9 (executor 1) (42/200)
20/12/04 16:27:00 INFO TaskSetManager: Starting task 79.0 in stage 3.0 (TID 47, 192.168.1.9, executor 2, partition 79, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:00 INFO TaskSetManager: Finished task 75.0 in stage 3.0 (TID 45) in 2004 ms on 192.168.1.9 (executor 2) (43/200)
20/12/04 16:27:02 INFO TaskSetManager: Starting task 84.0 in stage 3.0 (TID 48, 192.168.1.9, executor 1, partition 84, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:02 INFO TaskSetManager: Finished task 77.0 in stage 3.0 (TID 46) in 3060 ms on 192.168.1.9 (executor 1) (44/200)
20/12/04 16:27:02 INFO TaskSetManager: Starting task 85.0 in stage 3.0 (TID 49, 192.168.1.9, executor 2, partition 85, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:02 INFO TaskSetManager: Finished task 79.0 in stage 3.0 (TID 47) in 2010 ms on 192.168.1.9 (executor 2) (45/200)
20/12/04 16:27:03 INFO TaskSetManager: Starting task 86.0 in stage 3.0 (TID 50, 192.168.1.9, executor 1, partition 86, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:03 INFO TaskSetManager: Finished task 84.0 in stage 3.0 (TID 48) in 702 ms on 192.168.1.9 (executor 1) (46/200)
20/12/04 16:27:04 INFO TaskSetManager: Starting task 88.0 in stage 3.0 (TID 51, 192.168.1.9, executor 1, partition 88, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:04 INFO TaskSetManager: Finished task 86.0 in stage 3.0 (TID 50) in 1390 ms on 192.168.1.9 (executor 1) (47/200)
20/12/04 16:27:05 INFO TaskSetManager: Starting task 89.0 in stage 3.0 (TID 52, 192.168.1.9, executor 2, partition 89, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:05 INFO TaskSetManager: Finished task 85.0 in stage 3.0 (TID 49) in 2691 ms on 192.168.1.9 (executor 2) (48/200)
20/12/04 16:27:05 INFO TaskSetManager: Starting task 91.0 in stage 3.0 (TID 53, 192.168.1.9, executor 1, partition 91, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:05 INFO TaskSetManager: Finished task 88.0 in stage 3.0 (TID 51) in 743 ms on 192.168.1.9 (executor 1) (49/200)
20/12/04 16:27:06 INFO TaskSetManager: Starting task 95.0 in stage 3.0 (TID 54, 192.168.1.9, executor 1, partition 95, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:06 INFO TaskSetManager: Finished task 91.0 in stage 3.0 (TID 53) in 1381 ms on 192.168.1.9 (executor 1) (50/200)
20/12/04 16:27:07 INFO TaskSetManager: Starting task 101.0 in stage 3.0 (TID 55, 192.168.1.9, executor 2, partition 101, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:07 INFO TaskSetManager: Finished task 89.0 in stage 3.0 (TID 52) in 2148 ms on 192.168.1.9 (executor 2) (51/200)
20/12/04 16:27:08 INFO TaskSetManager: Starting task 102.0 in stage 3.0 (TID 56, 192.168.1.9, executor 2, partition 102, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:08 INFO TaskSetManager: Finished task 101.0 in stage 3.0 (TID 55) in 908 ms on 192.168.1.9 (executor 2) (52/200)
20/12/04 16:27:08 INFO TaskSetManager: Starting task 103.0 in stage 3.0 (TID 57, 192.168.1.9, executor 1, partition 103, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:08 INFO TaskSetManager: Finished task 95.0 in stage 3.0 (TID 54) in 2029 ms on 192.168.1.9 (executor 1) (53/200)
20/12/04 16:27:10 INFO TaskSetManager: Starting task 105.0 in stage 3.0 (TID 58, 192.168.1.9, executor 1, partition 105, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:10 INFO TaskSetManager: Finished task 103.0 in stage 3.0 (TID 57) in 1925 ms on 192.168.1.9 (executor 1) (54/200)
20/12/04 16:27:11 INFO TaskSetManager: Starting task 106.0 in stage 3.0 (TID 59, 192.168.1.9, executor 2, partition 106, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:11 INFO TaskSetManager: Finished task 102.0 in stage 3.0 (TID 56) in 3588 ms on 192.168.1.9 (executor 2) (55/200)
20/12/04 16:27:12 INFO TaskSetManager: Starting task 107.0 in stage 3.0 (TID 60, 192.168.1.9, executor 2, partition 107, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:12 INFO TaskSetManager: Finished task 106.0 in stage 3.0 (TID 59) in 671 ms on 192.168.1.9 (executor 2) (56/200)
20/12/04 16:27:12 INFO TaskSetManager: Starting task 108.0 in stage 3.0 (TID 61, 192.168.1.9, executor 1, partition 108, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:12 INFO TaskSetManager: Finished task 105.0 in stage 3.0 (TID 58) in 2362 ms on 192.168.1.9 (executor 1) (57/200)
20/12/04 16:27:13 INFO TaskSetManager: Starting task 109.0 in stage 3.0 (TID 62, 192.168.1.9, executor 2, partition 109, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:13 INFO TaskSetManager: Finished task 107.0 in stage 3.0 (TID 60) in 894 ms on 192.168.1.9 (executor 2) (58/200)
20/12/04 16:27:13 INFO TaskSetManager: Starting task 111.0 in stage 3.0 (TID 63, 192.168.1.9, executor 1, partition 111, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:13 INFO TaskSetManager: Finished task 108.0 in stage 3.0 (TID 61) in 982 ms on 192.168.1.9 (executor 1) (59/200)
20/12/04 16:27:15 INFO TaskSetManager: Starting task 113.0 in stage 3.0 (TID 64, 192.168.1.9, executor 1, partition 113, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:15 INFO TaskSetManager: Finished task 111.0 in stage 3.0 (TID 63) in 1633 ms on 192.168.1.9 (executor 1) (60/200)
20/12/04 16:27:15 INFO TaskSetManager: Starting task 115.0 in stage 3.0 (TID 65, 192.168.1.9, executor 2, partition 115, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:15 INFO TaskSetManager: Finished task 109.0 in stage 3.0 (TID 62) in 2309 ms on 192.168.1.9 (executor 2) (61/200)
20/12/04 16:27:16 INFO TaskSetManager: Starting task 116.0 in stage 3.0 (TID 66, 192.168.1.9, executor 1, partition 116, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:16 INFO TaskSetManager: Finished task 113.0 in stage 3.0 (TID 64) in 1034 ms on 192.168.1.9 (executor 1) (62/200)
20/12/04 16:27:17 INFO TaskSetManager: Starting task 119.0 in stage 3.0 (TID 67, 192.168.1.9, executor 1, partition 119, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:17 INFO TaskSetManager: Finished task 116.0 in stage 3.0 (TID 66) in 662 ms on 192.168.1.9 (executor 1) (63/200)
20/12/04 16:27:17 INFO TaskSetManager: Starting task 122.0 in stage 3.0 (TID 68, 192.168.1.9, executor 2, partition 122, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:17 INFO TaskSetManager: Finished task 115.0 in stage 3.0 (TID 65) in 1843 ms on 192.168.1.9 (executor 2) (64/200)
20/12/04 16:27:18 INFO TaskSetManager: Starting task 124.0 in stage 3.0 (TID 69, 192.168.1.9, executor 1, partition 124, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:18 INFO TaskSetManager: Finished task 119.0 in stage 3.0 (TID 67) in 1356 ms on 192.168.1.9 (executor 1) (65/200)
20/12/04 16:27:19 INFO TaskSetManager: Starting task 126.0 in stage 3.0 (TID 70, 192.168.1.9, executor 1, partition 126, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:19 INFO TaskSetManager: Finished task 124.0 in stage 3.0 (TID 69) in 871 ms on 192.168.1.9 (executor 1) (66/200)
20/12/04 16:27:20 INFO TaskSetManager: Starting task 128.0 in stage 3.0 (TID 71, 192.168.1.9, executor 2, partition 128, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:20 INFO TaskSetManager: Finished task 122.0 in stage 3.0 (TID 68) in 2613 ms on 192.168.1.9 (executor 2) (67/200)
20/12/04 16:27:20 INFO TaskSetManager: Starting task 129.0 in stage 3.0 (TID 72, 192.168.1.9, executor 1, partition 129, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:20 INFO TaskSetManager: Finished task 126.0 in stage 3.0 (TID 70) in 1079 ms on 192.168.1.9 (executor 1) (68/200)
20/12/04 16:27:20 INFO TaskSetManager: Starting task 131.0 in stage 3.0 (TID 73, 192.168.1.9, executor 2, partition 131, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:20 INFO TaskSetManager: Finished task 128.0 in stage 3.0 (TID 71) in 825 ms on 192.168.1.9 (executor 2) (69/200)
20/12/04 16:27:21 INFO TaskSetManager: Starting task 132.0 in stage 3.0 (TID 74, 192.168.1.9, executor 2, partition 132, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:21 INFO TaskSetManager: Finished task 131.0 in stage 3.0 (TID 73) in 778 ms on 192.168.1.9 (executor 2) (70/200)
20/12/04 16:27:21 INFO TaskSetManager: Starting task 133.0 in stage 3.0 (TID 75, 192.168.1.9, executor 1, partition 133, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:21 INFO TaskSetManager: Finished task 129.0 in stage 3.0 (TID 72) in 1219 ms on 192.168.1.9 (executor 1) (71/200)
20/12/04 16:27:22 INFO TaskSetManager: Starting task 135.0 in stage 3.0 (TID 76, 192.168.1.9, executor 1, partition 135, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:22 INFO TaskSetManager: Finished task 133.0 in stage 3.0 (TID 75) in 1082 ms on 192.168.1.9 (executor 1) (72/200)
20/12/04 16:27:23 INFO TaskSetManager: Starting task 136.0 in stage 3.0 (TID 77, 192.168.1.9, executor 2, partition 136, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:23 INFO TaskSetManager: Finished task 132.0 in stage 3.0 (TID 74) in 1728 ms on 192.168.1.9 (executor 2) (73/200)
20/12/04 16:27:24 INFO TaskSetManager: Starting task 137.0 in stage 3.0 (TID 78, 192.168.1.9, executor 2, partition 137, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:24 INFO TaskSetManager: Finished task 136.0 in stage 3.0 (TID 77) in 795 ms on 192.168.1.9 (executor 2) (74/200)
20/12/04 16:27:25 INFO TaskSetManager: Starting task 139.0 in stage 3.0 (TID 79, 192.168.1.9, executor 1, partition 139, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:25 INFO TaskSetManager: Finished task 135.0 in stage 3.0 (TID 76) in 2856 ms on 192.168.1.9 (executor 1) (75/200)
20/12/04 16:27:26 INFO TaskSetManager: Starting task 140.0 in stage 3.0 (TID 80, 192.168.1.9, executor 1, partition 140, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:26 INFO TaskSetManager: Finished task 139.0 in stage 3.0 (TID 79) in 783 ms on 192.168.1.9 (executor 1) (76/200)
20/12/04 16:27:27 INFO TaskSetManager: Starting task 141.0 in stage 3.0 (TID 81, 192.168.1.9, executor 2, partition 141, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:27 INFO TaskSetManager: Finished task 137.0 in stage 3.0 (TID 78) in 3339 ms on 192.168.1.9 (executor 2) (77/200)
20/12/04 16:27:28 INFO TaskSetManager: Starting task 143.0 in stage 3.0 (TID 82, 192.168.1.9, executor 1, partition 143, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:28 INFO TaskSetManager: Finished task 140.0 in stage 3.0 (TID 80) in 1798 ms on 192.168.1.9 (executor 1) (78/200)
20/12/04 16:27:28 INFO TaskSetManager: Starting task 146.0 in stage 3.0 (TID 83, 192.168.1.9, executor 2, partition 146, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:28 INFO TaskSetManager: Finished task 141.0 in stage 3.0 (TID 81) in 702 ms on 192.168.1.9 (executor 2) (79/200)
20/12/04 16:27:29 INFO TaskSetManager: Finished task 143.0 in stage 3.0 (TID 82) in 1099 ms on 192.168.1.9 (executor 1) (80/200)
20/12/04 16:27:29 INFO TaskSetManager: Starting task 150.0 in stage 3.0 (TID 84, 192.168.1.9, executor 1, partition 150, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:29 INFO TaskSetManager: Starting task 151.0 in stage 3.0 (TID 85, 192.168.1.9, executor 2, partition 151, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:29 INFO TaskSetManager: Finished task 146.0 in stage 3.0 (TID 83) in 1559 ms on 192.168.1.9 (executor 2) (81/200)
20/12/04 16:27:31 INFO TaskSetManager: Starting task 153.0 in stage 3.0 (TID 86, 192.168.1.9, executor 2, partition 153, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:31 INFO TaskSetManager: Finished task 151.0 in stage 3.0 (TID 85) in 1565 ms on 192.168.1.9 (executor 2) (82/200)
20/12/04 16:27:32 INFO TaskSetManager: Starting task 155.0 in stage 3.0 (TID 87, 192.168.1.9, executor 1, partition 155, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:32 INFO TaskSetManager: Finished task 150.0 in stage 3.0 (TID 84) in 3258 ms on 192.168.1.9 (executor 1) (83/200)
20/12/04 16:27:32 INFO TaskSetManager: Finished task 153.0 in stage 3.0 (TID 86) in 1622 ms on 192.168.1.9 (executor 2) (84/200)
20/12/04 16:27:32 INFO TaskSetManager: Starting task 156.0 in stage 3.0 (TID 88, 192.168.1.9, executor 2, partition 156, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:33 INFO TaskSetManager: Starting task 161.0 in stage 3.0 (TID 89, 192.168.1.9, executor 1, partition 161, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:33 INFO TaskSetManager: Finished task 155.0 in stage 3.0 (TID 87) in 1092 ms on 192.168.1.9 (executor 1) (85/200)
20/12/04 16:27:35 INFO TaskSetManager: Starting task 162.0 in stage 3.0 (TID 90, 192.168.1.9, executor 2, partition 162, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:35 INFO TaskSetManager: Finished task 156.0 in stage 3.0 (TID 88) in 2416 ms on 192.168.1.9 (executor 2) (86/200)
20/12/04 16:27:35 INFO TaskSetManager: Starting task 163.0 in stage 3.0 (TID 91, 192.168.1.9, executor 1, partition 163, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:35 INFO TaskSetManager: Finished task 161.0 in stage 3.0 (TID 89) in 1877 ms on 192.168.1.9 (executor 1) (87/200)
20/12/04 16:27:37 INFO TaskSetManager: Starting task 164.0 in stage 3.0 (TID 92, 192.168.1.9, executor 2, partition 164, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:37 INFO TaskSetManager: Finished task 162.0 in stage 3.0 (TID 90) in 2285 ms on 192.168.1.9 (executor 2) (88/200)
20/12/04 16:27:38 INFO TaskSetManager: Starting task 165.0 in stage 3.0 (TID 93, 192.168.1.9, executor 1, partition 165, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:38 INFO TaskSetManager: Finished task 163.0 in stage 3.0 (TID 91) in 2788 ms on 192.168.1.9 (executor 1) (89/200)
20/12/04 16:27:40 INFO TaskSetManager: Starting task 167.0 in stage 3.0 (TID 94, 192.168.1.9, executor 1, partition 167, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:40 INFO TaskSetManager: Finished task 165.0 in stage 3.0 (TID 93) in 2210 ms on 192.168.1.9 (executor 1) (90/200)
20/12/04 16:27:42 INFO TaskSetManager: Starting task 168.0 in stage 3.0 (TID 95, 192.168.1.9, executor 2, partition 168, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:42 INFO TaskSetManager: Finished task 164.0 in stage 3.0 (TID 92) in 4783 ms on 192.168.1.9 (executor 2) (91/200)
20/12/04 16:27:43 INFO TaskSetManager: Starting task 170.0 in stage 3.0 (TID 96, 192.168.1.9, executor 1, partition 170, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:43 INFO TaskSetManager: Finished task 167.0 in stage 3.0 (TID 94) in 2496 ms on 192.168.1.9 (executor 1) (92/200)
20/12/04 16:27:43 INFO TaskSetManager: Starting task 172.0 in stage 3.0 (TID 97, 192.168.1.9, executor 2, partition 172, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:43 INFO TaskSetManager: Finished task 168.0 in stage 3.0 (TID 95) in 732 ms on 192.168.1.9 (executor 2) (93/200)
20/12/04 16:27:44 INFO TaskSetManager: Starting task 173.0 in stage 3.0 (TID 98, 192.168.1.9, executor 2, partition 173, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:44 INFO TaskSetManager: Finished task 172.0 in stage 3.0 (TID 97) in 1252 ms on 192.168.1.9 (executor 2) (94/200)
20/12/04 16:27:44 INFO TaskSetManager: Starting task 174.0 in stage 3.0 (TID 99, 192.168.1.9, executor 1, partition 174, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:44 INFO TaskSetManager: Finished task 170.0 in stage 3.0 (TID 96) in 1428 ms on 192.168.1.9 (executor 1) (95/200)
20/12/04 16:27:46 INFO TaskSetManager: Starting task 175.0 in stage 3.0 (TID 100, 192.168.1.9, executor 1, partition 175, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:46 INFO TaskSetManager: Finished task 174.0 in stage 3.0 (TID 99) in 2198 ms on 192.168.1.9 (executor 1) (96/200)
20/12/04 16:27:47 INFO TaskSetManager: Starting task 178.0 in stage 3.0 (TID 101, 192.168.1.9, executor 2, partition 178, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:47 INFO TaskSetManager: Finished task 173.0 in stage 3.0 (TID 98) in 2809 ms on 192.168.1.9 (executor 2) (97/200)
20/12/04 16:27:48 INFO TaskSetManager: Starting task 179.0 in stage 3.0 (TID 102, 192.168.1.9, executor 2, partition 179, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:48 INFO TaskSetManager: Finished task 178.0 in stage 3.0 (TID 101) in 1036 ms on 192.168.1.9 (executor 2) (98/200)
20/12/04 16:27:48 INFO TaskSetManager: Starting task 181.0 in stage 3.0 (TID 103, 192.168.1.9, executor 1, partition 181, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:48 INFO TaskSetManager: Finished task 175.0 in stage 3.0 (TID 100) in 2125 ms on 192.168.1.9 (executor 1) (99/200)
20/12/04 16:27:49 INFO TaskSetManager: Starting task 183.0 in stage 3.0 (TID 104, 192.168.1.9, executor 2, partition 183, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:49 INFO TaskSetManager: Finished task 179.0 in stage 3.0 (TID 102) in 1282 ms on 192.168.1.9 (executor 2) (100/200)
20/12/04 16:27:50 INFO TaskSetManager: Starting task 184.0 in stage 3.0 (TID 105, 192.168.1.9, executor 2, partition 184, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:50 INFO TaskSetManager: Finished task 183.0 in stage 3.0 (TID 104) in 648 ms on 192.168.1.9 (executor 2) (101/200)
20/12/04 16:27:51 INFO TaskSetManager: Starting task 190.0 in stage 3.0 (TID 106, 192.168.1.9, executor 1, partition 190, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:51 INFO TaskSetManager: Finished task 181.0 in stage 3.0 (TID 103) in 2303 ms on 192.168.1.9 (executor 1) (102/200)
20/12/04 16:27:52 INFO TaskSetManager: Starting task 192.0 in stage 3.0 (TID 107, 192.168.1.9, executor 2, partition 192, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:52 INFO TaskSetManager: Finished task 184.0 in stage 3.0 (TID 105) in 1984 ms on 192.168.1.9 (executor 2) (103/200)
20/12/04 16:27:52 INFO TaskSetManager: Starting task 193.0 in stage 3.0 (TID 108, 192.168.1.9, executor 1, partition 193, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:52 INFO TaskSetManager: Finished task 190.0 in stage 3.0 (TID 106) in 1421 ms on 192.168.1.9 (executor 1) (104/200)
20/12/04 16:27:52 INFO TaskSetManager: Starting task 194.0 in stage 3.0 (TID 109, 192.168.1.9, executor 2, partition 194, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:52 INFO TaskSetManager: Finished task 192.0 in stage 3.0 (TID 107) in 749 ms on 192.168.1.9 (executor 2) (105/200)
20/12/04 16:27:54 INFO TaskSetManager: Starting task 195.0 in stage 3.0 (TID 110, 192.168.1.9, executor 2, partition 195, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:54 INFO TaskSetManager: Finished task 194.0 in stage 3.0 (TID 109) in 1119 ms on 192.168.1.9 (executor 2) (106/200)
20/12/04 16:27:54 INFO TaskSetManager: Starting task 198.0 in stage 3.0 (TID 111, 192.168.1.9, executor 1, partition 198, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:54 INFO TaskSetManager: Finished task 193.0 in stage 3.0 (TID 108) in 2297 ms on 192.168.1.9 (executor 1) (107/200)
20/12/04 16:27:55 INFO TaskSetManager: Starting task 199.0 in stage 3.0 (TID 112, 192.168.1.9, executor 2, partition 199, NODE_LOCAL, 7336 bytes)
20/12/04 16:27:55 INFO TaskSetManager: Finished task 195.0 in stage 3.0 (TID 110) in 990 ms on 192.168.1.9 (executor 2) (108/200)
20/12/04 16:27:55 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 113, 192.168.1.9, executor 1, partition 1, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:55 INFO TaskSetManager: Finished task 198.0 in stage 3.0 (TID 111) in 780 ms on 192.168.1.9 (executor 1) (109/200)
20/12/04 16:27:55 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 114, 192.168.1.9, executor 1, partition 8, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:55 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 113) in 125 ms on 192.168.1.9 (executor 1) (110/200)
20/12/04 16:27:55 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 115, 192.168.1.9, executor 1, partition 9, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:55 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 114) in 116 ms on 192.168.1.9 (executor 1) (111/200)
20/12/04 16:27:55 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 116, 192.168.1.9, executor 1, partition 15, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:55 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 115) in 116 ms on 192.168.1.9 (executor 1) (112/200)
20/12/04 16:27:56 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 117, 192.168.1.9, executor 1, partition 16, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:56 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 116) in 127 ms on 192.168.1.9 (executor 1) (113/200)
20/12/04 16:27:56 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 118, 192.168.1.9, executor 1, partition 17, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:56 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 117) in 123 ms on 192.168.1.9 (executor 1) (114/200)
20/12/04 16:27:56 INFO TaskSetManager: Starting task 20.0 in stage 3.0 (TID 119, 192.168.1.9, executor 1, partition 20, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:56 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 118) in 157 ms on 192.168.1.9 (executor 1) (115/200)
20/12/04 16:27:56 INFO TaskSetManager: Starting task 22.0 in stage 3.0 (TID 120, 192.168.1.9, executor 1, partition 22, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:56 INFO TaskSetManager: Finished task 20.0 in stage 3.0 (TID 119) in 142 ms on 192.168.1.9 (executor 1) (116/200)
20/12/04 16:27:56 INFO TaskSetManager: Starting task 25.0 in stage 3.0 (TID 121, 192.168.1.9, executor 2, partition 25, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:56 INFO TaskSetManager: Finished task 199.0 in stage 3.0 (TID 112) in 1516 ms on 192.168.1.9 (executor 2) (117/200)
20/12/04 16:27:56 INFO TaskSetManager: Starting task 26.0 in stage 3.0 (TID 122, 192.168.1.9, executor 1, partition 26, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:56 INFO TaskSetManager: Finished task 22.0 in stage 3.0 (TID 120) in 143 ms on 192.168.1.9 (executor 1) (118/200)
20/12/04 16:27:56 INFO TaskSetManager: Starting task 28.0 in stage 3.0 (TID 123, 192.168.1.9, executor 2, partition 28, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:56 INFO TaskSetManager: Finished task 25.0 in stage 3.0 (TID 121) in 142 ms on 192.168.1.9 (executor 2) (119/200)
20/12/04 16:27:56 INFO TaskSetManager: Starting task 29.0 in stage 3.0 (TID 124, 192.168.1.9, executor 1, partition 29, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:56 INFO TaskSetManager: Finished task 26.0 in stage 3.0 (TID 122) in 170 ms on 192.168.1.9 (executor 1) (120/200)
20/12/04 16:27:56 INFO TaskSetManager: Starting task 33.0 in stage 3.0 (TID 125, 192.168.1.9, executor 2, partition 33, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:56 INFO TaskSetManager: Finished task 28.0 in stage 3.0 (TID 123) in 156 ms on 192.168.1.9 (executor 2) (121/200)
20/12/04 16:27:56 INFO TaskSetManager: Starting task 38.0 in stage 3.0 (TID 126, 192.168.1.9, executor 1, partition 38, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:56 INFO TaskSetManager: Finished task 29.0 in stage 3.0 (TID 124) in 156 ms on 192.168.1.9 (executor 1) (122/200)
20/12/04 16:27:57 INFO TaskSetManager: Starting task 39.0 in stage 3.0 (TID 127, 192.168.1.9, executor 2, partition 39, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:57 INFO TaskSetManager: Finished task 33.0 in stage 3.0 (TID 125) in 157 ms on 192.168.1.9 (executor 2) (123/200)
20/12/04 16:27:57 INFO TaskSetManager: Starting task 40.0 in stage 3.0 (TID 128, 192.168.1.9, executor 1, partition 40, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:57 INFO TaskSetManager: Finished task 38.0 in stage 3.0 (TID 126) in 153 ms on 192.168.1.9 (executor 1) (124/200)
20/12/04 16:27:57 INFO TaskSetManager: Starting task 42.0 in stage 3.0 (TID 129, 192.168.1.9, executor 2, partition 42, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:57 INFO TaskSetManager: Finished task 39.0 in stage 3.0 (TID 127) in 187 ms on 192.168.1.9 (executor 2) (125/200)
20/12/04 16:27:57 INFO TaskSetManager: Starting task 45.0 in stage 3.0 (TID 130, 192.168.1.9, executor 1, partition 45, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:57 INFO TaskSetManager: Finished task 40.0 in stage 3.0 (TID 128) in 148 ms on 192.168.1.9 (executor 1) (126/200)
20/12/04 16:27:57 INFO TaskSetManager: Starting task 46.0 in stage 3.0 (TID 131, 192.168.1.9, executor 2, partition 46, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:57 INFO TaskSetManager: Finished task 42.0 in stage 3.0 (TID 129) in 151 ms on 192.168.1.9 (executor 2) (127/200)
20/12/04 16:27:57 INFO TaskSetManager: Starting task 47.0 in stage 3.0 (TID 132, 192.168.1.9, executor 1, partition 47, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:57 INFO TaskSetManager: Finished task 45.0 in stage 3.0 (TID 130) in 123 ms on 192.168.1.9 (executor 1) (128/200)
20/12/04 16:27:57 INFO TaskSetManager: Starting task 50.0 in stage 3.0 (TID 133, 192.168.1.9, executor 2, partition 50, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:57 INFO TaskSetManager: Finished task 46.0 in stage 3.0 (TID 131) in 209 ms on 192.168.1.9 (executor 2) (129/200)
20/12/04 16:27:57 INFO TaskSetManager: Starting task 52.0 in stage 3.0 (TID 134, 192.168.1.9, executor 1, partition 52, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:57 INFO TaskSetManager: Finished task 47.0 in stage 3.0 (TID 132) in 199 ms on 192.168.1.9 (executor 1) (130/200)
20/12/04 16:27:57 INFO TaskSetManager: Starting task 54.0 in stage 3.0 (TID 135, 192.168.1.9, executor 1, partition 54, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:57 INFO TaskSetManager: Finished task 52.0 in stage 3.0 (TID 134) in 202 ms on 192.168.1.9 (executor 1) (131/200)
20/12/04 16:27:57 INFO TaskSetManager: Starting task 56.0 in stage 3.0 (TID 136, 192.168.1.9, executor 2, partition 56, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:57 INFO TaskSetManager: Finished task 50.0 in stage 3.0 (TID 133) in 221 ms on 192.168.1.9 (executor 2) (132/200)
20/12/04 16:27:57 INFO TaskSetManager: Starting task 59.0 in stage 3.0 (TID 137, 192.168.1.9, executor 1, partition 59, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:57 INFO TaskSetManager: Finished task 54.0 in stage 3.0 (TID 135) in 146 ms on 192.168.1.9 (executor 1) (133/200)
20/12/04 16:27:57 INFO TaskSetManager: Finished task 56.0 in stage 3.0 (TID 136) in 193 ms on 192.168.1.9 (executor 2) (134/200)
20/12/04 16:27:57 INFO TaskSetManager: Starting task 60.0 in stage 3.0 (TID 138, 192.168.1.9, executor 2, partition 60, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Starting task 62.0 in stage 3.0 (TID 139, 192.168.1.9, executor 1, partition 62, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Finished task 59.0 in stage 3.0 (TID 137) in 173 ms on 192.168.1.9 (executor 1) (135/200)
20/12/04 16:27:58 INFO TaskSetManager: Starting task 64.0 in stage 3.0 (TID 140, 192.168.1.9, executor 2, partition 64, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Finished task 60.0 in stage 3.0 (TID 138) in 180 ms on 192.168.1.9 (executor 2) (136/200)
20/12/04 16:27:58 INFO TaskSetManager: Starting task 67.0 in stage 3.0 (TID 141, 192.168.1.9, executor 1, partition 67, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Finished task 62.0 in stage 3.0 (TID 139) in 149 ms on 192.168.1.9 (executor 1) (137/200)
20/12/04 16:27:58 INFO TaskSetManager: Starting task 68.0 in stage 3.0 (TID 142, 192.168.1.9, executor 2, partition 68, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Finished task 64.0 in stage 3.0 (TID 140) in 172 ms on 192.168.1.9 (executor 2) (138/200)
20/12/04 16:27:58 INFO TaskSetManager: Starting task 71.0 in stage 3.0 (TID 143, 192.168.1.9, executor 1, partition 71, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Finished task 67.0 in stage 3.0 (TID 141) in 124 ms on 192.168.1.9 (executor 1) (139/200)
20/12/04 16:27:58 INFO TaskSetManager: Starting task 72.0 in stage 3.0 (TID 144, 192.168.1.9, executor 2, partition 72, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Finished task 68.0 in stage 3.0 (TID 142) in 133 ms on 192.168.1.9 (executor 2) (140/200)
20/12/04 16:27:58 INFO TaskSetManager: Starting task 74.0 in stage 3.0 (TID 145, 192.168.1.9, executor 1, partition 74, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Finished task 71.0 in stage 3.0 (TID 143) in 120 ms on 192.168.1.9 (executor 1) (141/200)
20/12/04 16:27:58 INFO TaskSetManager: Starting task 76.0 in stage 3.0 (TID 146, 192.168.1.9, executor 2, partition 76, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Finished task 72.0 in stage 3.0 (TID 144) in 144 ms on 192.168.1.9 (executor 2) (142/200)
20/12/04 16:27:58 INFO TaskSetManager: Starting task 78.0 in stage 3.0 (TID 147, 192.168.1.9, executor 1, partition 78, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Finished task 74.0 in stage 3.0 (TID 145) in 125 ms on 192.168.1.9 (executor 1) (143/200)
20/12/04 16:27:58 INFO TaskSetManager: Starting task 80.0 in stage 3.0 (TID 148, 192.168.1.9, executor 1, partition 80, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Finished task 78.0 in stage 3.0 (TID 147) in 134 ms on 192.168.1.9 (executor 1) (144/200)
20/12/04 16:27:58 INFO TaskSetManager: Starting task 81.0 in stage 3.0 (TID 149, 192.168.1.9, executor 2, partition 81, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Finished task 76.0 in stage 3.0 (TID 146) in 152 ms on 192.168.1.9 (executor 2) (145/200)
20/12/04 16:27:58 INFO TaskSetManager: Starting task 82.0 in stage 3.0 (TID 150, 192.168.1.9, executor 1, partition 82, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Finished task 80.0 in stage 3.0 (TID 148) in 121 ms on 192.168.1.9 (executor 1) (146/200)
20/12/04 16:27:58 INFO TaskSetManager: Starting task 83.0 in stage 3.0 (TID 151, 192.168.1.9, executor 2, partition 83, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Finished task 81.0 in stage 3.0 (TID 149) in 144 ms on 192.168.1.9 (executor 2) (147/200)
20/12/04 16:27:58 INFO TaskSetManager: Starting task 87.0 in stage 3.0 (TID 152, 192.168.1.9, executor 1, partition 87, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:58 INFO TaskSetManager: Finished task 82.0 in stage 3.0 (TID 150) in 130 ms on 192.168.1.9 (executor 1) (148/200)
20/12/04 16:27:59 INFO TaskSetManager: Starting task 90.0 in stage 3.0 (TID 153, 192.168.1.9, executor 2, partition 90, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:59 INFO TaskSetManager: Finished task 83.0 in stage 3.0 (TID 151) in 134 ms on 192.168.1.9 (executor 2) (149/200)
20/12/04 16:27:59 INFO TaskSetManager: Starting task 92.0 in stage 3.0 (TID 154, 192.168.1.9, executor 1, partition 92, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:59 INFO TaskSetManager: Finished task 87.0 in stage 3.0 (TID 152) in 121 ms on 192.168.1.9 (executor 1) (150/200)
20/12/04 16:27:59 INFO TaskSetManager: Starting task 93.0 in stage 3.0 (TID 155, 192.168.1.9, executor 2, partition 93, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:59 INFO TaskSetManager: Finished task 90.0 in stage 3.0 (TID 153) in 150 ms on 192.168.1.9 (executor 2) (151/200)
20/12/04 16:27:59 INFO TaskSetManager: Starting task 94.0 in stage 3.0 (TID 156, 192.168.1.9, executor 1, partition 94, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:59 INFO TaskSetManager: Finished task 92.0 in stage 3.0 (TID 154) in 195 ms on 192.168.1.9 (executor 1) (152/200)
20/12/04 16:27:59 INFO TaskSetManager: Starting task 96.0 in stage 3.0 (TID 157, 192.168.1.9, executor 2, partition 96, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:59 INFO TaskSetManager: Finished task 93.0 in stage 3.0 (TID 155) in 189 ms on 192.168.1.9 (executor 2) (153/200)
20/12/04 16:27:59 INFO TaskSetManager: Finished task 94.0 in stage 3.0 (TID 156) in 127 ms on 192.168.1.9 (executor 1) (154/200)
20/12/04 16:27:59 INFO TaskSetManager: Starting task 97.0 in stage 3.0 (TID 158, 192.168.1.9, executor 1, partition 97, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:59 INFO TaskSetManager: Starting task 98.0 in stage 3.0 (TID 159, 192.168.1.9, executor 2, partition 98, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:59 INFO TaskSetManager: Finished task 96.0 in stage 3.0 (TID 157) in 164 ms on 192.168.1.9 (executor 2) (155/200)
20/12/04 16:27:59 INFO TaskSetManager: Starting task 99.0 in stage 3.0 (TID 160, 192.168.1.9, executor 1, partition 99, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:59 INFO TaskSetManager: Finished task 97.0 in stage 3.0 (TID 158) in 132 ms on 192.168.1.9 (executor 1) (156/200)
20/12/04 16:27:59 INFO TaskSetManager: Starting task 100.0 in stage 3.0 (TID 161, 192.168.1.9, executor 2, partition 100, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:59 INFO TaskSetManager: Finished task 98.0 in stage 3.0 (TID 159) in 135 ms on 192.168.1.9 (executor 2) (157/200)
20/12/04 16:27:59 INFO TaskSetManager: Starting task 104.0 in stage 3.0 (TID 162, 192.168.1.9, executor 1, partition 104, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:59 INFO TaskSetManager: Finished task 99.0 in stage 3.0 (TID 160) in 122 ms on 192.168.1.9 (executor 1) (158/200)
20/12/04 16:27:59 INFO TaskSetManager: Starting task 110.0 in stage 3.0 (TID 163, 192.168.1.9, executor 2, partition 110, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:59 INFO TaskSetManager: Finished task 100.0 in stage 3.0 (TID 161) in 151 ms on 192.168.1.9 (executor 2) (159/200)
20/12/04 16:27:59 INFO TaskSetManager: Starting task 112.0 in stage 3.0 (TID 164, 192.168.1.9, executor 1, partition 112, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:59 INFO TaskSetManager: Finished task 104.0 in stage 3.0 (TID 162) in 160 ms on 192.168.1.9 (executor 1) (160/200)
20/12/04 16:27:59 INFO TaskSetManager: Starting task 114.0 in stage 3.0 (TID 165, 192.168.1.9, executor 2, partition 114, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:59 INFO TaskSetManager: Finished task 110.0 in stage 3.0 (TID 163) in 151 ms on 192.168.1.9 (executor 2) (161/200)
20/12/04 16:27:59 INFO TaskSetManager: Starting task 117.0 in stage 3.0 (TID 166, 192.168.1.9, executor 1, partition 117, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:27:59 INFO TaskSetManager: Finished task 112.0 in stage 3.0 (TID 164) in 127 ms on 192.168.1.9 (executor 1) (162/200)
20/12/04 16:28:00 INFO TaskSetManager: Starting task 118.0 in stage 3.0 (TID 167, 192.168.1.9, executor 1, partition 118, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:00 INFO TaskSetManager: Finished task 117.0 in stage 3.0 (TID 166) in 172 ms on 192.168.1.9 (executor 1) (163/200)
20/12/04 16:28:00 INFO TaskSetManager: Starting task 120.0 in stage 3.0 (TID 168, 192.168.1.9, executor 2, partition 120, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:00 INFO TaskSetManager: Finished task 114.0 in stage 3.0 (TID 165) in 190 ms on 192.168.1.9 (executor 2) (164/200)
20/12/04 16:28:00 INFO TaskSetManager: Starting task 121.0 in stage 3.0 (TID 169, 192.168.1.9, executor 1, partition 121, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:00 INFO TaskSetManager: Finished task 118.0 in stage 3.0 (TID 167) in 153 ms on 192.168.1.9 (executor 1) (165/200)
20/12/04 16:28:00 INFO TaskSetManager: Starting task 123.0 in stage 3.0 (TID 170, 192.168.1.9, executor 2, partition 123, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:00 INFO TaskSetManager: Finished task 120.0 in stage 3.0 (TID 168) in 158 ms on 192.168.1.9 (executor 2) (166/200)
20/12/04 16:28:00 INFO TaskSetManager: Starting task 125.0 in stage 3.0 (TID 171, 192.168.1.9, executor 1, partition 125, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:00 INFO TaskSetManager: Finished task 121.0 in stage 3.0 (TID 169) in 126 ms on 192.168.1.9 (executor 1) (167/200)
20/12/04 16:28:00 INFO TaskSetManager: Starting task 127.0 in stage 3.0 (TID 172, 192.168.1.9, executor 2, partition 127, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:00 INFO TaskSetManager: Finished task 123.0 in stage 3.0 (TID 170) in 147 ms on 192.168.1.9 (executor 2) (168/200)
20/12/04 16:28:00 INFO TaskSetManager: Starting task 130.0 in stage 3.0 (TID 173, 192.168.1.9, executor 1, partition 130, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:00 INFO TaskSetManager: Finished task 125.0 in stage 3.0 (TID 171) in 133 ms on 192.168.1.9 (executor 1) (169/200)
20/12/04 16:28:00 INFO TaskSetManager: Starting task 134.0 in stage 3.0 (TID 174, 192.168.1.9, executor 2, partition 134, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:00 INFO TaskSetManager: Finished task 127.0 in stage 3.0 (TID 172) in 214 ms on 192.168.1.9 (executor 2) (170/200)
20/12/04 16:28:00 INFO TaskSetManager: Starting task 138.0 in stage 3.0 (TID 175, 192.168.1.9, executor 1, partition 138, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:00 INFO TaskSetManager: Finished task 130.0 in stage 3.0 (TID 173) in 168 ms on 192.168.1.9 (executor 1) (171/200)
20/12/04 16:28:00 INFO TaskSetManager: Finished task 138.0 in stage 3.0 (TID 175) in 123 ms on 192.168.1.9 (executor 1) (172/200)
20/12/04 16:28:00 INFO TaskSetManager: Starting task 142.0 in stage 3.0 (TID 176, 192.168.1.9, executor 1, partition 142, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:00 INFO TaskSetManager: Starting task 144.0 in stage 3.0 (TID 177, 192.168.1.9, executor 2, partition 144, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:00 INFO TaskSetManager: Finished task 134.0 in stage 3.0 (TID 174) in 190 ms on 192.168.1.9 (executor 2) (173/200)
20/12/04 16:28:00 INFO TaskSetManager: Starting task 145.0 in stage 3.0 (TID 178, 192.168.1.9, executor 1, partition 145, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:00 INFO TaskSetManager: Finished task 142.0 in stage 3.0 (TID 176) in 125 ms on 192.168.1.9 (executor 1) (174/200)
20/12/04 16:28:00 INFO TaskSetManager: Starting task 147.0 in stage 3.0 (TID 179, 192.168.1.9, executor 2, partition 147, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:00 INFO TaskSetManager: Finished task 144.0 in stage 3.0 (TID 177) in 135 ms on 192.168.1.9 (executor 2) (175/200)
20/12/04 16:28:01 INFO TaskSetManager: Starting task 148.0 in stage 3.0 (TID 180, 192.168.1.9, executor 1, partition 148, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:01 INFO TaskSetManager: Finished task 145.0 in stage 3.0 (TID 178) in 158 ms on 192.168.1.9 (executor 1) (176/200)
20/12/04 16:28:01 INFO TaskSetManager: Starting task 149.0 in stage 3.0 (TID 181, 192.168.1.9, executor 2, partition 149, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:01 INFO TaskSetManager: Finished task 147.0 in stage 3.0 (TID 179) in 173 ms on 192.168.1.9 (executor 2) (177/200)
20/12/04 16:28:01 INFO TaskSetManager: Starting task 152.0 in stage 3.0 (TID 182, 192.168.1.9, executor 1, partition 152, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:01 INFO TaskSetManager: Finished task 148.0 in stage 3.0 (TID 180) in 130 ms on 192.168.1.9 (executor 1) (178/200)
20/12/04 16:28:01 INFO TaskSetManager: Starting task 154.0 in stage 3.0 (TID 183, 192.168.1.9, executor 2, partition 154, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:01 INFO TaskSetManager: Finished task 149.0 in stage 3.0 (TID 181) in 143 ms on 192.168.1.9 (executor 2) (179/200)
20/12/04 16:28:01 INFO TaskSetManager: Starting task 157.0 in stage 3.0 (TID 184, 192.168.1.9, executor 1, partition 157, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:01 INFO TaskSetManager: Finished task 152.0 in stage 3.0 (TID 182) in 129 ms on 192.168.1.9 (executor 1) (180/200)
20/12/04 16:28:01 INFO TaskSetManager: Starting task 158.0 in stage 3.0 (TID 185, 192.168.1.9, executor 2, partition 158, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:01 INFO TaskSetManager: Finished task 154.0 in stage 3.0 (TID 183) in 179 ms on 192.168.1.9 (executor 2) (181/200)
20/12/04 16:28:01 INFO TaskSetManager: Starting task 159.0 in stage 3.0 (TID 186, 192.168.1.9, executor 1, partition 159, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:01 INFO TaskSetManager: Finished task 157.0 in stage 3.0 (TID 184) in 140 ms on 192.168.1.9 (executor 1) (182/200)
20/12/04 16:28:01 INFO TaskSetManager: Starting task 160.0 in stage 3.0 (TID 187, 192.168.1.9, executor 2, partition 160, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:01 INFO TaskSetManager: Finished task 158.0 in stage 3.0 (TID 185) in 137 ms on 192.168.1.9 (executor 2) (183/200)
20/12/04 16:28:01 INFO TaskSetManager: Starting task 166.0 in stage 3.0 (TID 188, 192.168.1.9, executor 1, partition 166, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:01 INFO TaskSetManager: Finished task 159.0 in stage 3.0 (TID 186) in 120 ms on 192.168.1.9 (executor 1) (184/200)
20/12/04 16:28:01 INFO TaskSetManager: Starting task 169.0 in stage 3.0 (TID 189, 192.168.1.9, executor 1, partition 169, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:01 INFO TaskSetManager: Finished task 166.0 in stage 3.0 (TID 188) in 170 ms on 192.168.1.9 (executor 1) (185/200)
20/12/04 16:28:01 INFO TaskSetManager: Starting task 171.0 in stage 3.0 (TID 190, 192.168.1.9, executor 2, partition 171, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:01 INFO TaskSetManager: Finished task 160.0 in stage 3.0 (TID 187) in 192 ms on 192.168.1.9 (executor 2) (186/200)
20/12/04 16:28:01 INFO TaskSetManager: Starting task 176.0 in stage 3.0 (TID 191, 192.168.1.9, executor 1, partition 176, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:01 INFO TaskSetManager: Finished task 169.0 in stage 3.0 (TID 189) in 142 ms on 192.168.1.9 (executor 1) (187/200)
20/12/04 16:28:01 INFO TaskSetManager: Starting task 177.0 in stage 3.0 (TID 192, 192.168.1.9, executor 2, partition 177, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:01 INFO TaskSetManager: Finished task 171.0 in stage 3.0 (TID 190) in 154 ms on 192.168.1.9 (executor 2) (188/200)
20/12/04 16:28:02 INFO TaskSetManager: Starting task 180.0 in stage 3.0 (TID 193, 192.168.1.9, executor 1, partition 180, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:02 INFO TaskSetManager: Finished task 176.0 in stage 3.0 (TID 191) in 129 ms on 192.168.1.9 (executor 1) (189/200)
20/12/04 16:28:02 INFO TaskSetManager: Starting task 182.0 in stage 3.0 (TID 194, 192.168.1.9, executor 2, partition 182, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:02 INFO TaskSetManager: Finished task 177.0 in stage 3.0 (TID 192) in 142 ms on 192.168.1.9 (executor 2) (190/200)
20/12/04 16:28:02 INFO TaskSetManager: Starting task 185.0 in stage 3.0 (TID 195, 192.168.1.9, executor 2, partition 185, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:02 INFO TaskSetManager: Finished task 182.0 in stage 3.0 (TID 194) in 132 ms on 192.168.1.9 (executor 2) (191/200)
20/12/04 16:28:02 INFO TaskSetManager: Starting task 186.0 in stage 3.0 (TID 196, 192.168.1.9, executor 1, partition 186, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:02 INFO TaskSetManager: Finished task 180.0 in stage 3.0 (TID 193) in 190 ms on 192.168.1.9 (executor 1) (192/200)
20/12/04 16:28:02 INFO TaskSetManager: Starting task 187.0 in stage 3.0 (TID 197, 192.168.1.9, executor 1, partition 187, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:02 INFO TaskSetManager: Finished task 186.0 in stage 3.0 (TID 196) in 136 ms on 192.168.1.9 (executor 1) (193/200)
20/12/04 16:28:02 INFO TaskSetManager: Starting task 188.0 in stage 3.0 (TID 198, 192.168.1.9, executor 2, partition 188, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:02 INFO TaskSetManager: Finished task 185.0 in stage 3.0 (TID 195) in 202 ms on 192.168.1.9 (executor 2) (194/200)
20/12/04 16:28:02 INFO TaskSetManager: Starting task 189.0 in stage 3.0 (TID 199, 192.168.1.9, executor 1, partition 189, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:02 INFO TaskSetManager: Finished task 187.0 in stage 3.0 (TID 197) in 159 ms on 192.168.1.9 (executor 1) (195/200)
20/12/04 16:28:02 INFO TaskSetManager: Starting task 191.0 in stage 3.0 (TID 200, 192.168.1.9, executor 2, partition 191, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:02 INFO TaskSetManager: Finished task 188.0 in stage 3.0 (TID 198) in 199 ms on 192.168.1.9 (executor 2) (196/200)
20/12/04 16:28:02 INFO TaskSetManager: Starting task 196.0 in stage 3.0 (TID 201, 192.168.1.9, executor 1, partition 196, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:02 INFO TaskSetManager: Finished task 189.0 in stage 3.0 (TID 199) in 129 ms on 192.168.1.9 (executor 1) (197/200)
20/12/04 16:28:02 INFO TaskSetManager: Starting task 197.0 in stage 3.0 (TID 202, 192.168.1.9, executor 2, partition 197, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:28:02 INFO TaskSetManager: Finished task 191.0 in stage 3.0 (TID 200) in 156 ms on 192.168.1.9 (executor 2) (198/200)
20/12/04 16:28:02 INFO TaskSetManager: Finished task 196.0 in stage 3.0 (TID 201) in 118 ms on 192.168.1.9 (executor 1) (199/200)
20/12/04 16:28:02 INFO TaskSetManager: Finished task 197.0 in stage 3.0 (TID 202) in 127 ms on 192.168.1.9 (executor 2) (200/200)
20/12/04 16:28:02 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 16:28:02 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/userFiles-07633795-2abd-4249-ab93-f56b1a9ef78c/darima.zip/darima/dlsa.py:22) finished in 115.941 s
20/12/04 16:28:02 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:28:02 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
20/12/04 16:28:02 INFO DAGScheduler: Job 1 finished: toPandas at /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/userFiles-07633795-2abd-4249-ab93-f56b1a9ef78c/darima.zip/darima/dlsa.py:22, took 120.195339 s
20/12/04 16:28:03 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:28:03 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:28:03 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:28:03 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:28:03 INFO CodeGenerator: Code generated in 13.956439 ms
20/12/04 16:28:03 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:28:03 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:28:03 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:39465 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:28:03 INFO SparkContext: Created broadcast 6 from toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201
20/12/04 16:28:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:28:03 INFO SparkContext: Starting job: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201
20/12/04 16:28:03 INFO DAGScheduler: Got job 2 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201) with 1 output partitions
20/12/04 16:28:03 INFO DAGScheduler: Final stage: ResultStage 4 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201)
20/12/04 16:28:03 INFO DAGScheduler: Parents of final stage: List()
20/12/04 16:28:03 INFO DAGScheduler: Missing parents: List()
20/12/04 16:28:03 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201), which has no missing parents
20/12/04 16:28:03 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 21.7 KiB, free 5.8 GiB)
20/12/04 16:28:03 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 5.8 GiB)
20/12/04 16:28:03 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:39465 (size: 10.4 KiB, free: 5.8 GiB)
20/12/04 16:28:03 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1223
20/12/04 16:28:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201) (first 15 tasks are for partitions Vector(0))
20/12/04 16:28:03 INFO YarnScheduler: Adding task set 4.0 with 1 tasks
20/12/04 16:28:03 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 203, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7790 bytes)
20/12/04 16:28:03 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:39029 (size: 10.4 KiB, free: 912.2 MiB)
20/12/04 16:28:03 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:39029 (size: 28.7 KiB, free: 912.1 MiB)
20/12/04 16:28:04 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 203) in 747 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 16:28:04 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/12/04 16:28:04 INFO DAGScheduler: ResultStage 4 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201) finished in 0.763 s
20/12/04 16:28:04 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:28:04 INFO YarnScheduler: Killing all running tasks in stage 4: Stage finished
20/12/04 16:28:04 INFO DAGScheduler: Job 2 finished: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201, took 0.770202 s
20/12/04 16:28:09 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:28:09 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:28:09 INFO FileSourceStrategy: Post-Scan Filters: 
20/12/04 16:28:09 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:28:09 INFO CodeGenerator: Code generated in 14.445269 ms
20/12/04 16:28:09 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:28:09 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:28:09 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:39465 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:28:09 INFO SparkContext: Created broadcast 8 from toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217
20/12/04 16:28:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:28:09 INFO SparkContext: Starting job: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217
20/12/04 16:28:09 INFO DAGScheduler: Got job 3 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217) with 1 output partitions
20/12/04 16:28:09 INFO DAGScheduler: Final stage: ResultStage 5 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217)
20/12/04 16:28:09 INFO DAGScheduler: Parents of final stage: List()
20/12/04 16:28:09 INFO DAGScheduler: Missing parents: List()
20/12/04 16:28:09 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217), which has no missing parents
20/12/04 16:28:09 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 12.3 KiB, free 5.8 GiB)
20/12/04 16:28:09 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 5.8 GiB)
20/12/04 16:28:09 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:39465 (size: 6.3 KiB, free: 5.8 GiB)
20/12/04 16:28:09 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1223
20/12/04 16:28:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217) (first 15 tasks are for partitions Vector(0))
20/12/04 16:28:09 INFO YarnScheduler: Adding task set 5.0 with 1 tasks
20/12/04 16:28:09 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 204, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7789 bytes)
20/12/04 16:28:09 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:40239 (size: 6.3 KiB, free: 912.1 MiB)
20/12/04 16:28:09 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:40239 (size: 28.7 KiB, free: 912.1 MiB)
20/12/04 16:28:09 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 204) in 228 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:28:09 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/12/04 16:28:09 INFO DAGScheduler: ResultStage 5 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217) finished in 0.238 s
20/12/04 16:28:09 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:28:09 INFO YarnScheduler: Killing all running tasks in stage 5: Stage finished
20/12/04 16:28:09 INFO DAGScheduler: Job 3 finished: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217, took 0.245433 s
20/12/04 16:28:13 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 16:28:13 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 16:28:13 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 16:28:13 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 16:28:13 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 16:28:13 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 16:28:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 16:28:14 INFO MemoryStore: MemoryStore cleared
20/12/04 16:28:14 INFO BlockManager: BlockManager stopped
20/12/04 16:28:14 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 16:28:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 16:28:14 INFO SparkContext: Successfully stopped SparkContext
20/12/04 16:28:14 INFO ShutdownHookManager: Shutdown hook called
20/12/04 16:28:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa
20/12/04 16:28:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-2d88ced2-9866-4166-a90b-4f79ffc7a7fa/pyspark-2d5d5dd7-c096-4c0c-9462-46e6f45bd738
20/12/04 16:28:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-cbfa3e71-6ba8-4c8e-9bd1-076e6d734d80
20/12/04 16:28:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 16:28:16 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:28:16 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:28:16 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:28:16 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:28:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:28:17 INFO SparkContext: Running Spark version 3.0.1
20/12/04 16:28:17 INFO ResourceUtils: ==============================================================
20/12/04 16:28:17 INFO ResourceUtils: Resources for spark.driver:

20/12/04 16:28:17 INFO ResourceUtils: ==============================================================
20/12/04 16:28:17 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 16:28:17 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:28:17 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:28:17 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:28:17 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:28:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:28:18 INFO Utils: Successfully started service 'sparkDriver' on port 42749.
20/12/04 16:28:18 INFO SparkEnv: Registering MapOutputTracker
20/12/04 16:28:18 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 16:28:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 16:28:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 16:28:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 16:28:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1e35df28-0309-4253-b1ce-34a7ebfdd653
20/12/04 16:28:18 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 16:28:18 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 16:28:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 16:28:18 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 16:28:19 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 16:28:19 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 16:28:19 INFO Configuration: resource-types.xml not found
20/12/04 16:28:19 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 16:28:19 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 16:28:19 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 16:28:19 INFO Client: Setting up container launch context for our AM
20/12/04 16:28:19 INFO Client: Setting up the launch environment for our AM container
20/12/04 16:28:19 INFO Client: Preparing resources for our AM container
20/12/04 16:28:19 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 16:28:22 INFO Client: Uploading resource file:/tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/__spark_libs__14747575730389781001.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0078/__spark_libs__14747575730389781001.zip
20/12/04 16:28:23 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0078/pyspark.zip
20/12/04 16:28:23 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0078/py4j-0.10.9-src.zip
20/12/04 16:28:23 INFO Client: Uploading resource file:/tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/__spark_conf__546265015711361307.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0078/__spark_conf__.zip
20/12/04 16:28:23 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 16:28:23 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 16:28:23 INFO SecurityManager: Changing view acls groups to: 
20/12/04 16:28:23 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 16:28:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 16:28:23 INFO Client: Submitting application application_1607015337794_0078 to ResourceManager
20/12/04 16:28:23 INFO YarnClientImpl: Submitted application application_1607015337794_0078
20/12/04 16:28:24 INFO Client: Application report for application_1607015337794_0078 (state: ACCEPTED)
20/12/04 16:28:24 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607117303933
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0078/
	 user: bsuconn
20/12/04 16:28:25 INFO Client: Application report for application_1607015337794_0078 (state: ACCEPTED)
20/12/04 16:28:26 INFO Client: Application report for application_1607015337794_0078 (state: ACCEPTED)
20/12/04 16:28:27 INFO Client: Application report for application_1607015337794_0078 (state: ACCEPTED)
20/12/04 16:28:28 INFO Client: Application report for application_1607015337794_0078 (state: ACCEPTED)
20/12/04 16:28:29 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0078), /proxy/application_1607015337794_0078
20/12/04 16:28:29 INFO Client: Application report for application_1607015337794_0078 (state: RUNNING)
20/12/04 16:28:29 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607117303933
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0078/
	 user: bsuconn
20/12/04 16:28:29 INFO YarnClientSchedulerBackend: Application application_1607015337794_0078 has started running.
20/12/04 16:28:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40463.
20/12/04 16:28:29 INFO NettyBlockTransferService: Server created on 192.168.1.9:40463
20/12/04 16:28:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 16:28:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 40463, None)
20/12/04 16:28:30 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40463 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 40463, None)
20/12/04 16:28:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 40463, None)
20/12/04 16:28:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 40463, None)
20/12/04 16:28:30 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:28:30 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 16:28:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 16:28:36 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:39526) with ID 1
20/12/04 16:28:36 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:42317 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 42317, None)
20/12/04 16:28:37 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:39530) with ID 2
20/12/04 16:28:38 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:46649 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 46649, None)
20/12/04 16:28:48 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
20/12/04 16:28:49 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 16:28:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 16:28:49 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 16:28:49 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:28:49 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:28:49 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:28:49 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:28:49 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 16:28:49 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:42749/files/darima.zip with timestamp 1607117329874
20/12/04 16:28:49 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/userFiles-d1287690-4709-44d2-a4f0-f0f979a56c90/darima.zip
20/12/04 16:28:51 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
20/12/04 16:28:53 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/12/04 16:28:54 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:28:54 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:28:54 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:28:54 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:28:55 INFO CodeGenerator: Code generated in 283.584085 ms
20/12/04 16:28:55 INFO CodeGenerator: Code generated in 31.90754 ms
20/12/04 16:28:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:28:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:28:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:40463 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:28:55 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 16:28:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:28:55 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 16:28:55 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 16:28:55 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 16:28:55 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 16:28:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 16:28:55 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 16:28:55 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:28:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 16:28:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 16:28:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:40463 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:28:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 16:28:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:28:55 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 16:28:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:28:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:46649 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:28:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:46649 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:28:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3075 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 16:28:58 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 16:28:58 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.220 s
20/12/04 16:28:58 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:28:58 INFO DAGScheduler: running: Set()
20/12/04 16:28:58 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 16:28:58 INFO DAGScheduler: failed: Set()
20/12/04 16:28:58 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 16:28:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 16:28:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 16:28:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:40463 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:28:58 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 16:28:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 16:28:58 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 16:28:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:28:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:42317 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:28:59 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:39526
20/12/04 16:29:00 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1798 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:29:00 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 16:29:00 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 1.815 s
20/12/04 16:29:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:29:00 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 16:29:00 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 5.135829 s
20/12/04 16:29:02 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:46649 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 16:29:02 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:40463 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 16:29:02 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:40463 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 16:29:02 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:42317 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 16:29:03 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 16:29:03 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:29:03 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:29:03 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:29:03 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:29:03 INFO CodeGenerator: Code generated in 29.918453 ms
20/12/04 16:29:04 INFO CodeGenerator: Code generated in 16.277112 ms
20/12/04 16:29:04 INFO CodeGenerator: Code generated in 20.422971 ms
20/12/04 16:29:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:29:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:29:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:40463 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:29:04 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/userFiles-d1287690-4709-44d2-a4f0-f0f979a56c90/darima.zip/darima/dlsa.py:22
20/12/04 16:29:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:29:04 INFO SparkContext: Starting job: toPandas at /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/userFiles-d1287690-4709-44d2-a4f0-f0f979a56c90/darima.zip/darima/dlsa.py:22
20/12/04 16:29:04 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/userFiles-d1287690-4709-44d2-a4f0-f0f979a56c90/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 16:29:04 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/userFiles-d1287690-4709-44d2-a4f0-f0f979a56c90/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 16:29:04 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/userFiles-d1287690-4709-44d2-a4f0-f0f979a56c90/darima.zip/darima/dlsa.py:22)
20/12/04 16:29:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 16:29:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 16:29:04 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/userFiles-d1287690-4709-44d2-a4f0-f0f979a56c90/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:29:04 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 16:29:04 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 16:29:04 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:40463 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 16:29:04 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 16:29:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/userFiles-d1287690-4709-44d2-a4f0-f0f979a56c90/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 16:29:04 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 16:29:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 16:29:04 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:42317 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 16:29:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:42317 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 16:29:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2826 ms on 192.168.1.9 (executor 1) (1/1)
20/12/04 16:29:07 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 16:29:07 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 43743
20/12/04 16:29:07 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/userFiles-d1287690-4709-44d2-a4f0-f0f979a56c90/darima.zip/darima/dlsa.py:22) finished in 2.853 s
20/12/04 16:29:07 INFO DAGScheduler: looking for newly runnable stages
20/12/04 16:29:07 INFO DAGScheduler: running: Set()
20/12/04 16:29:07 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 16:29:07 INFO DAGScheduler: failed: Set()
20/12/04 16:29:07 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/userFiles-d1287690-4709-44d2-a4f0-f0f979a56c90/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 16:29:07 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 16:29:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 16:29:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:40463 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 16:29:07 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 16:29:07 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/userFiles-d1287690-4709-44d2-a4f0-f0f979a56c90/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 16:29:07 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 16:29:07 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:07 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:42317 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 16:29:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:46649 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 16:29:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:39526
20/12/04 16:29:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:39530
20/12/04 16:29:16 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:16 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 4) in 9106 ms on 192.168.1.9 (executor 1) (1/200)
20/12/04 16:29:16 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 6, 192.168.1.9, executor 2, partition 4, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:16 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 9244 ms on 192.168.1.9 (executor 2) (2/200)
20/12/04 16:29:18 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 7, 192.168.1.9, executor 2, partition 5, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:18 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 6) in 1995 ms on 192.168.1.9 (executor 2) (3/200)
20/12/04 16:29:19 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 8, 192.168.1.9, executor 2, partition 6, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:19 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 7) in 1546 ms on 192.168.1.9 (executor 2) (4/200)
20/12/04 16:29:20 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 7, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:20 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 5) in 3780 ms on 192.168.1.9 (executor 1) (5/200)
20/12/04 16:29:22 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 10, 192.168.1.9, executor 2, partition 10, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:22 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 8) in 2081 ms on 192.168.1.9 (executor 2) (6/200)
20/12/04 16:29:23 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 11, 192.168.1.9, executor 1, partition 11, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:23 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 9) in 3068 ms on 192.168.1.9 (executor 1) (7/200)
20/12/04 16:29:24 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 12, 192.168.1.9, executor 2, partition 12, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:24 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 10) in 2421 ms on 192.168.1.9 (executor 2) (8/200)
20/12/04 16:29:25 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 13, 192.168.1.9, executor 1, partition 13, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:25 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 11) in 1973 ms on 192.168.1.9 (executor 1) (9/200)
20/12/04 16:29:26 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 14, 192.168.1.9, executor 1, partition 14, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:26 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 13) in 1142 ms on 192.168.1.9 (executor 1) (10/200)
20/12/04 16:29:27 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 14) in 1621 ms on 192.168.1.9 (executor 1) (11/200)
20/12/04 16:29:27 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 15, 192.168.1.9, executor 1, partition 18, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:29 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 12) in 5487 ms on 192.168.1.9 (executor 2) (12/200)
20/12/04 16:29:29 INFO TaskSetManager: Starting task 19.0 in stage 3.0 (TID 16, 192.168.1.9, executor 2, partition 19, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:30 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 15) in 2344 ms on 192.168.1.9 (executor 1) (13/200)
20/12/04 16:29:30 INFO TaskSetManager: Starting task 21.0 in stage 3.0 (TID 17, 192.168.1.9, executor 1, partition 21, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:33 INFO TaskSetManager: Starting task 23.0 in stage 3.0 (TID 18, 192.168.1.9, executor 1, partition 23, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:33 INFO TaskSetManager: Finished task 21.0 in stage 3.0 (TID 17) in 3357 ms on 192.168.1.9 (executor 1) (14/200)
20/12/04 16:29:34 INFO TaskSetManager: Starting task 24.0 in stage 3.0 (TID 19, 192.168.1.9, executor 2, partition 24, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:34 INFO TaskSetManager: Finished task 19.0 in stage 3.0 (TID 16) in 4471 ms on 192.168.1.9 (executor 2) (15/200)
20/12/04 16:29:35 INFO TaskSetManager: Starting task 27.0 in stage 3.0 (TID 20, 192.168.1.9, executor 1, partition 27, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:35 INFO TaskSetManager: Finished task 23.0 in stage 3.0 (TID 18) in 1887 ms on 192.168.1.9 (executor 1) (16/200)
20/12/04 16:29:36 INFO TaskSetManager: Starting task 30.0 in stage 3.0 (TID 21, 192.168.1.9, executor 2, partition 30, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:36 INFO TaskSetManager: Finished task 24.0 in stage 3.0 (TID 19) in 1949 ms on 192.168.1.9 (executor 2) (17/200)
20/12/04 16:29:36 INFO TaskSetManager: Starting task 31.0 in stage 3.0 (TID 22, 192.168.1.9, executor 1, partition 31, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:36 INFO TaskSetManager: Finished task 27.0 in stage 3.0 (TID 20) in 1038 ms on 192.168.1.9 (executor 1) (18/200)
20/12/04 16:29:38 INFO TaskSetManager: Starting task 32.0 in stage 3.0 (TID 23, 192.168.1.9, executor 1, partition 32, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:38 INFO TaskSetManager: Finished task 31.0 in stage 3.0 (TID 22) in 2478 ms on 192.168.1.9 (executor 1) (19/200)
20/12/04 16:29:38 INFO TaskSetManager: Starting task 34.0 in stage 3.0 (TID 24, 192.168.1.9, executor 2, partition 34, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:38 INFO TaskSetManager: Finished task 30.0 in stage 3.0 (TID 21) in 2614 ms on 192.168.1.9 (executor 2) (20/200)
20/12/04 16:29:40 INFO TaskSetManager: Starting task 35.0 in stage 3.0 (TID 25, 192.168.1.9, executor 2, partition 35, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:40 INFO TaskSetManager: Finished task 34.0 in stage 3.0 (TID 24) in 1465 ms on 192.168.1.9 (executor 2) (21/200)
20/12/04 16:29:41 INFO TaskSetManager: Starting task 36.0 in stage 3.0 (TID 26, 192.168.1.9, executor 1, partition 36, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:41 INFO TaskSetManager: Finished task 32.0 in stage 3.0 (TID 23) in 2442 ms on 192.168.1.9 (executor 1) (22/200)
20/12/04 16:29:42 INFO TaskSetManager: Starting task 37.0 in stage 3.0 (TID 27, 192.168.1.9, executor 2, partition 37, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:42 INFO TaskSetManager: Finished task 35.0 in stage 3.0 (TID 25) in 2085 ms on 192.168.1.9 (executor 2) (23/200)
20/12/04 16:29:42 INFO TaskSetManager: Starting task 41.0 in stage 3.0 (TID 28, 192.168.1.9, executor 1, partition 41, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:42 INFO TaskSetManager: Finished task 36.0 in stage 3.0 (TID 26) in 1399 ms on 192.168.1.9 (executor 1) (24/200)
20/12/04 16:29:43 INFO TaskSetManager: Starting task 43.0 in stage 3.0 (TID 29, 192.168.1.9, executor 1, partition 43, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:43 INFO TaskSetManager: Finished task 41.0 in stage 3.0 (TID 28) in 1120 ms on 192.168.1.9 (executor 1) (25/200)
20/12/04 16:29:43 INFO TaskSetManager: Starting task 44.0 in stage 3.0 (TID 30, 192.168.1.9, executor 2, partition 44, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:43 INFO TaskSetManager: Finished task 37.0 in stage 3.0 (TID 27) in 1450 ms on 192.168.1.9 (executor 2) (26/200)
20/12/04 16:29:45 INFO TaskSetManager: Starting task 48.0 in stage 3.0 (TID 31, 192.168.1.9, executor 2, partition 48, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:45 INFO TaskSetManager: Finished task 44.0 in stage 3.0 (TID 30) in 1272 ms on 192.168.1.9 (executor 2) (27/200)
20/12/04 16:29:46 INFO TaskSetManager: Starting task 49.0 in stage 3.0 (TID 32, 192.168.1.9, executor 1, partition 49, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:46 INFO TaskSetManager: Finished task 43.0 in stage 3.0 (TID 29) in 2144 ms on 192.168.1.9 (executor 1) (28/200)
20/12/04 16:29:46 INFO TaskSetManager: Starting task 51.0 in stage 3.0 (TID 33, 192.168.1.9, executor 2, partition 51, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:46 INFO TaskSetManager: Finished task 48.0 in stage 3.0 (TID 31) in 1323 ms on 192.168.1.9 (executor 2) (29/200)
20/12/04 16:29:48 INFO TaskSetManager: Starting task 53.0 in stage 3.0 (TID 34, 192.168.1.9, executor 1, partition 53, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:48 INFO TaskSetManager: Finished task 49.0 in stage 3.0 (TID 32) in 2512 ms on 192.168.1.9 (executor 1) (30/200)
20/12/04 16:29:49 INFO TaskSetManager: Starting task 55.0 in stage 3.0 (TID 35, 192.168.1.9, executor 2, partition 55, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:49 INFO TaskSetManager: Finished task 51.0 in stage 3.0 (TID 33) in 2691 ms on 192.168.1.9 (executor 2) (31/200)
20/12/04 16:29:50 INFO TaskSetManager: Starting task 57.0 in stage 3.0 (TID 36, 192.168.1.9, executor 2, partition 57, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:50 INFO TaskSetManager: Finished task 55.0 in stage 3.0 (TID 35) in 1059 ms on 192.168.1.9 (executor 2) (32/200)
20/12/04 16:29:51 INFO TaskSetManager: Starting task 58.0 in stage 3.0 (TID 37, 192.168.1.9, executor 1, partition 58, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:51 INFO TaskSetManager: Finished task 53.0 in stage 3.0 (TID 34) in 3310 ms on 192.168.1.9 (executor 1) (33/200)
20/12/04 16:29:52 INFO TaskSetManager: Starting task 61.0 in stage 3.0 (TID 38, 192.168.1.9, executor 2, partition 61, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:52 INFO TaskSetManager: Finished task 57.0 in stage 3.0 (TID 36) in 1965 ms on 192.168.1.9 (executor 2) (34/200)
20/12/04 16:29:53 INFO TaskSetManager: Starting task 63.0 in stage 3.0 (TID 39, 192.168.1.9, executor 1, partition 63, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:53 INFO TaskSetManager: Finished task 58.0 in stage 3.0 (TID 37) in 1207 ms on 192.168.1.9 (executor 1) (35/200)
20/12/04 16:29:54 INFO TaskSetManager: Starting task 65.0 in stage 3.0 (TID 40, 192.168.1.9, executor 1, partition 65, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:54 INFO TaskSetManager: Finished task 63.0 in stage 3.0 (TID 39) in 1065 ms on 192.168.1.9 (executor 1) (36/200)
20/12/04 16:29:55 INFO TaskSetManager: Starting task 66.0 in stage 3.0 (TID 41, 192.168.1.9, executor 2, partition 66, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:55 INFO TaskSetManager: Finished task 61.0 in stage 3.0 (TID 38) in 2724 ms on 192.168.1.9 (executor 2) (37/200)
20/12/04 16:29:55 INFO TaskSetManager: Starting task 69.0 in stage 3.0 (TID 42, 192.168.1.9, executor 1, partition 69, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:55 INFO TaskSetManager: Finished task 65.0 in stage 3.0 (TID 40) in 1336 ms on 192.168.1.9 (executor 1) (38/200)
20/12/04 16:29:56 INFO TaskSetManager: Starting task 70.0 in stage 3.0 (TID 43, 192.168.1.9, executor 2, partition 70, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:56 INFO TaskSetManager: Finished task 66.0 in stage 3.0 (TID 41) in 1149 ms on 192.168.1.9 (executor 2) (39/200)
20/12/04 16:29:56 INFO TaskSetManager: Starting task 73.0 in stage 3.0 (TID 44, 192.168.1.9, executor 1, partition 73, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:56 INFO TaskSetManager: Finished task 69.0 in stage 3.0 (TID 42) in 1436 ms on 192.168.1.9 (executor 1) (40/200)
20/12/04 16:29:57 INFO TaskSetManager: Starting task 75.0 in stage 3.0 (TID 45, 192.168.1.9, executor 2, partition 75, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:57 INFO TaskSetManager: Finished task 70.0 in stage 3.0 (TID 43) in 1365 ms on 192.168.1.9 (executor 2) (41/200)
20/12/04 16:29:58 INFO TaskSetManager: Starting task 77.0 in stage 3.0 (TID 46, 192.168.1.9, executor 1, partition 77, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:58 INFO TaskSetManager: Finished task 73.0 in stage 3.0 (TID 44) in 1709 ms on 192.168.1.9 (executor 1) (42/200)
20/12/04 16:29:59 INFO TaskSetManager: Starting task 79.0 in stage 3.0 (TID 47, 192.168.1.9, executor 2, partition 79, NODE_LOCAL, 7336 bytes)
20/12/04 16:29:59 INFO TaskSetManager: Finished task 75.0 in stage 3.0 (TID 45) in 1855 ms on 192.168.1.9 (executor 2) (43/200)
20/12/04 16:30:01 INFO TaskSetManager: Starting task 84.0 in stage 3.0 (TID 48, 192.168.1.9, executor 2, partition 84, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:01 INFO TaskSetManager: Finished task 79.0 in stage 3.0 (TID 47) in 2032 ms on 192.168.1.9 (executor 2) (44/200)
20/12/04 16:30:01 INFO TaskSetManager: Starting task 85.0 in stage 3.0 (TID 49, 192.168.1.9, executor 1, partition 85, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:01 INFO TaskSetManager: Finished task 77.0 in stage 3.0 (TID 46) in 3035 ms on 192.168.1.9 (executor 1) (45/200)
20/12/04 16:30:02 INFO TaskSetManager: Starting task 86.0 in stage 3.0 (TID 50, 192.168.1.9, executor 2, partition 86, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:02 INFO TaskSetManager: Finished task 84.0 in stage 3.0 (TID 48) in 845 ms on 192.168.1.9 (executor 2) (46/200)
20/12/04 16:30:03 INFO TaskSetManager: Starting task 88.0 in stage 3.0 (TID 51, 192.168.1.9, executor 2, partition 88, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:03 INFO TaskSetManager: Finished task 86.0 in stage 3.0 (TID 50) in 1419 ms on 192.168.1.9 (executor 2) (47/200)
20/12/04 16:30:04 INFO TaskSetManager: Starting task 89.0 in stage 3.0 (TID 52, 192.168.1.9, executor 2, partition 89, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:04 INFO TaskSetManager: Finished task 88.0 in stage 3.0 (TID 51) in 692 ms on 192.168.1.9 (executor 2) (48/200)
20/12/04 16:30:04 INFO TaskSetManager: Starting task 91.0 in stage 3.0 (TID 53, 192.168.1.9, executor 1, partition 91, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:04 INFO TaskSetManager: Finished task 85.0 in stage 3.0 (TID 49) in 3045 ms on 192.168.1.9 (executor 1) (49/200)
20/12/04 16:30:05 INFO TaskSetManager: Starting task 95.0 in stage 3.0 (TID 54, 192.168.1.9, executor 1, partition 95, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:05 INFO TaskSetManager: Finished task 91.0 in stage 3.0 (TID 53) in 1297 ms on 192.168.1.9 (executor 1) (50/200)
20/12/04 16:30:06 INFO TaskSetManager: Starting task 101.0 in stage 3.0 (TID 55, 192.168.1.9, executor 2, partition 101, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:06 INFO TaskSetManager: Finished task 89.0 in stage 3.0 (TID 52) in 2320 ms on 192.168.1.9 (executor 2) (51/200)
20/12/04 16:30:07 INFO TaskSetManager: Starting task 102.0 in stage 3.0 (TID 56, 192.168.1.9, executor 2, partition 102, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:07 INFO TaskSetManager: Finished task 101.0 in stage 3.0 (TID 55) in 1125 ms on 192.168.1.9 (executor 2) (52/200)
20/12/04 16:30:08 INFO TaskSetManager: Starting task 103.0 in stage 3.0 (TID 57, 192.168.1.9, executor 1, partition 103, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:08 INFO TaskSetManager: Finished task 95.0 in stage 3.0 (TID 54) in 2473 ms on 192.168.1.9 (executor 1) (53/200)
20/12/04 16:30:11 INFO TaskSetManager: Starting task 105.0 in stage 3.0 (TID 58, 192.168.1.9, executor 1, partition 105, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:11 INFO TaskSetManager: Finished task 103.0 in stage 3.0 (TID 57) in 2621 ms on 192.168.1.9 (executor 1) (54/200)
20/12/04 16:30:12 INFO TaskSetManager: Starting task 106.0 in stage 3.0 (TID 59, 192.168.1.9, executor 2, partition 106, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:12 INFO TaskSetManager: Finished task 102.0 in stage 3.0 (TID 56) in 4852 ms on 192.168.1.9 (executor 2) (55/200)
20/12/04 16:30:13 INFO TaskSetManager: Starting task 107.0 in stage 3.0 (TID 60, 192.168.1.9, executor 2, partition 107, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:13 INFO TaskSetManager: Finished task 106.0 in stage 3.0 (TID 59) in 602 ms on 192.168.1.9 (executor 2) (56/200)
20/12/04 16:30:14 INFO TaskSetManager: Starting task 108.0 in stage 3.0 (TID 61, 192.168.1.9, executor 2, partition 108, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:14 INFO TaskSetManager: Finished task 107.0 in stage 3.0 (TID 60) in 848 ms on 192.168.1.9 (executor 2) (57/200)
20/12/04 16:30:14 INFO TaskSetManager: Starting task 109.0 in stage 3.0 (TID 62, 192.168.1.9, executor 1, partition 109, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:14 INFO TaskSetManager: Finished task 105.0 in stage 3.0 (TID 58) in 3185 ms on 192.168.1.9 (executor 1) (58/200)
20/12/04 16:30:15 INFO TaskSetManager: Starting task 111.0 in stage 3.0 (TID 63, 192.168.1.9, executor 2, partition 111, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:15 INFO TaskSetManager: Finished task 108.0 in stage 3.0 (TID 61) in 1039 ms on 192.168.1.9 (executor 2) (59/200)
20/12/04 16:30:16 INFO TaskSetManager: Starting task 113.0 in stage 3.0 (TID 64, 192.168.1.9, executor 1, partition 113, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:16 INFO TaskSetManager: Finished task 109.0 in stage 3.0 (TID 62) in 2616 ms on 192.168.1.9 (executor 1) (60/200)
20/12/04 16:30:16 INFO TaskSetManager: Starting task 115.0 in stage 3.0 (TID 65, 192.168.1.9, executor 2, partition 115, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:16 INFO TaskSetManager: Finished task 111.0 in stage 3.0 (TID 63) in 1848 ms on 192.168.1.9 (executor 2) (61/200)
20/12/04 16:30:18 INFO TaskSetManager: Starting task 116.0 in stage 3.0 (TID 66, 192.168.1.9, executor 1, partition 116, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:18 INFO TaskSetManager: Finished task 113.0 in stage 3.0 (TID 64) in 1342 ms on 192.168.1.9 (executor 1) (62/200)
20/12/04 16:30:18 INFO TaskSetManager: Starting task 119.0 in stage 3.0 (TID 67, 192.168.1.9, executor 1, partition 119, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:18 INFO TaskSetManager: Finished task 116.0 in stage 3.0 (TID 66) in 736 ms on 192.168.1.9 (executor 1) (63/200)
20/12/04 16:30:19 INFO TaskSetManager: Starting task 122.0 in stage 3.0 (TID 68, 192.168.1.9, executor 2, partition 122, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:19 INFO TaskSetManager: Finished task 115.0 in stage 3.0 (TID 65) in 2192 ms on 192.168.1.9 (executor 2) (64/200)
20/12/04 16:30:20 INFO TaskSetManager: Finished task 119.0 in stage 3.0 (TID 67) in 1721 ms on 192.168.1.9 (executor 1) (65/200)
20/12/04 16:30:20 INFO TaskSetManager: Starting task 124.0 in stage 3.0 (TID 69, 192.168.1.9, executor 1, partition 124, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:21 INFO TaskSetManager: Finished task 124.0 in stage 3.0 (TID 69) in 1076 ms on 192.168.1.9 (executor 1) (66/200)
20/12/04 16:30:21 INFO TaskSetManager: Starting task 126.0 in stage 3.0 (TID 70, 192.168.1.9, executor 1, partition 126, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:22 INFO TaskSetManager: Starting task 128.0 in stage 3.0 (TID 71, 192.168.1.9, executor 2, partition 128, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:22 INFO TaskSetManager: Finished task 122.0 in stage 3.0 (TID 68) in 3350 ms on 192.168.1.9 (executor 2) (67/200)
20/12/04 16:30:23 INFO TaskSetManager: Starting task 129.0 in stage 3.0 (TID 72, 192.168.1.9, executor 1, partition 129, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:23 INFO TaskSetManager: Finished task 126.0 in stage 3.0 (TID 70) in 1281 ms on 192.168.1.9 (executor 1) (68/200)
20/12/04 16:30:23 INFO TaskSetManager: Finished task 128.0 in stage 3.0 (TID 71) in 1084 ms on 192.168.1.9 (executor 2) (69/200)
20/12/04 16:30:23 INFO TaskSetManager: Starting task 131.0 in stage 3.0 (TID 73, 192.168.1.9, executor 2, partition 131, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:24 INFO TaskSetManager: Finished task 131.0 in stage 3.0 (TID 73) in 1241 ms on 192.168.1.9 (executor 2) (70/200)
20/12/04 16:30:24 INFO TaskSetManager: Starting task 132.0 in stage 3.0 (TID 74, 192.168.1.9, executor 2, partition 132, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:24 INFO TaskSetManager: Finished task 129.0 in stage 3.0 (TID 72) in 1829 ms on 192.168.1.9 (executor 1) (71/200)
20/12/04 16:30:24 INFO TaskSetManager: Starting task 133.0 in stage 3.0 (TID 75, 192.168.1.9, executor 1, partition 133, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:26 INFO TaskSetManager: Starting task 135.0 in stage 3.0 (TID 76, 192.168.1.9, executor 1, partition 135, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:26 INFO TaskSetManager: Finished task 133.0 in stage 3.0 (TID 75) in 1693 ms on 192.168.1.9 (executor 1) (72/200)
20/12/04 16:30:27 INFO TaskSetManager: Starting task 136.0 in stage 3.0 (TID 77, 192.168.1.9, executor 2, partition 136, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:27 INFO TaskSetManager: Finished task 132.0 in stage 3.0 (TID 74) in 2340 ms on 192.168.1.9 (executor 2) (73/200)
20/12/04 16:30:28 INFO TaskSetManager: Finished task 136.0 in stage 3.0 (TID 77) in 938 ms on 192.168.1.9 (executor 2) (74/200)
20/12/04 16:30:28 INFO TaskSetManager: Starting task 137.0 in stage 3.0 (TID 78, 192.168.1.9, executor 2, partition 137, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:29 INFO TaskSetManager: Starting task 139.0 in stage 3.0 (TID 79, 192.168.1.9, executor 1, partition 139, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:29 INFO TaskSetManager: Finished task 135.0 in stage 3.0 (TID 76) in 3149 ms on 192.168.1.9 (executor 1) (75/200)
20/12/04 16:30:30 INFO TaskSetManager: Starting task 140.0 in stage 3.0 (TID 80, 192.168.1.9, executor 1, partition 140, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:30 INFO TaskSetManager: Finished task 139.0 in stage 3.0 (TID 79) in 1073 ms on 192.168.1.9 (executor 1) (76/200)
20/12/04 16:30:31 INFO TaskSetManager: Starting task 141.0 in stage 3.0 (TID 81, 192.168.1.9, executor 2, partition 141, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:31 INFO TaskSetManager: Finished task 137.0 in stage 3.0 (TID 78) in 3809 ms on 192.168.1.9 (executor 2) (77/200)
20/12/04 16:30:32 INFO TaskSetManager: Starting task 143.0 in stage 3.0 (TID 82, 192.168.1.9, executor 2, partition 143, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:32 INFO TaskSetManager: Finished task 141.0 in stage 3.0 (TID 81) in 608 ms on 192.168.1.9 (executor 2) (78/200)
20/12/04 16:30:32 INFO TaskSetManager: Starting task 146.0 in stage 3.0 (TID 83, 192.168.1.9, executor 1, partition 146, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:32 INFO TaskSetManager: Finished task 140.0 in stage 3.0 (TID 80) in 1920 ms on 192.168.1.9 (executor 1) (79/200)
20/12/04 16:30:33 INFO TaskSetManager: Starting task 150.0 in stage 3.0 (TID 84, 192.168.1.9, executor 2, partition 150, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:33 INFO TaskSetManager: Finished task 143.0 in stage 3.0 (TID 82) in 1327 ms on 192.168.1.9 (executor 2) (80/200)
20/12/04 16:30:34 INFO TaskSetManager: Starting task 151.0 in stage 3.0 (TID 85, 192.168.1.9, executor 1, partition 151, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:34 INFO TaskSetManager: Finished task 146.0 in stage 3.0 (TID 83) in 1822 ms on 192.168.1.9 (executor 1) (81/200)
20/12/04 16:30:36 INFO TaskSetManager: Starting task 153.0 in stage 3.0 (TID 86, 192.168.1.9, executor 1, partition 153, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:36 INFO TaskSetManager: Finished task 151.0 in stage 3.0 (TID 85) in 1731 ms on 192.168.1.9 (executor 1) (82/200)
20/12/04 16:30:37 INFO TaskSetManager: Starting task 155.0 in stage 3.0 (TID 87, 192.168.1.9, executor 2, partition 155, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:37 INFO TaskSetManager: Finished task 150.0 in stage 3.0 (TID 84) in 4013 ms on 192.168.1.9 (executor 2) (83/200)
20/12/04 16:30:38 INFO TaskSetManager: Starting task 156.0 in stage 3.0 (TID 88, 192.168.1.9, executor 1, partition 156, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:38 INFO TaskSetManager: Finished task 153.0 in stage 3.0 (TID 86) in 2061 ms on 192.168.1.9 (executor 1) (84/200)
20/12/04 16:30:38 INFO TaskSetManager: Starting task 161.0 in stage 3.0 (TID 89, 192.168.1.9, executor 2, partition 161, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:38 INFO TaskSetManager: Finished task 155.0 in stage 3.0 (TID 87) in 1076 ms on 192.168.1.9 (executor 2) (85/200)
20/12/04 16:30:40 INFO TaskSetManager: Starting task 162.0 in stage 3.0 (TID 90, 192.168.1.9, executor 1, partition 162, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:40 INFO TaskSetManager: Finished task 156.0 in stage 3.0 (TID 88) in 2435 ms on 192.168.1.9 (executor 1) (86/200)
20/12/04 16:30:40 INFO TaskSetManager: Starting task 163.0 in stage 3.0 (TID 91, 192.168.1.9, executor 2, partition 163, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:40 INFO TaskSetManager: Finished task 161.0 in stage 3.0 (TID 89) in 1944 ms on 192.168.1.9 (executor 2) (87/200)
20/12/04 16:30:43 INFO TaskSetManager: Starting task 164.0 in stage 3.0 (TID 92, 192.168.1.9, executor 1, partition 164, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:43 INFO TaskSetManager: Finished task 162.0 in stage 3.0 (TID 90) in 2381 ms on 192.168.1.9 (executor 1) (88/200)
20/12/04 16:30:43 INFO TaskSetManager: Starting task 165.0 in stage 3.0 (TID 93, 192.168.1.9, executor 2, partition 165, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:43 INFO TaskSetManager: Finished task 163.0 in stage 3.0 (TID 91) in 2884 ms on 192.168.1.9 (executor 2) (89/200)
20/12/04 16:30:45 INFO TaskSetManager: Finished task 165.0 in stage 3.0 (TID 93) in 2104 ms on 192.168.1.9 (executor 2) (90/200)
20/12/04 16:30:45 INFO TaskSetManager: Starting task 167.0 in stage 3.0 (TID 94, 192.168.1.9, executor 2, partition 167, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:47 INFO TaskSetManager: Starting task 168.0 in stage 3.0 (TID 95, 192.168.1.9, executor 1, partition 168, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:47 INFO TaskSetManager: Finished task 164.0 in stage 3.0 (TID 92) in 4740 ms on 192.168.1.9 (executor 1) (91/200)
20/12/04 16:30:48 INFO TaskSetManager: Starting task 170.0 in stage 3.0 (TID 96, 192.168.1.9, executor 2, partition 170, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:48 INFO TaskSetManager: Finished task 167.0 in stage 3.0 (TID 94) in 2354 ms on 192.168.1.9 (executor 2) (92/200)
20/12/04 16:30:48 INFO TaskSetManager: Starting task 172.0 in stage 3.0 (TID 97, 192.168.1.9, executor 1, partition 172, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:48 INFO TaskSetManager: Finished task 168.0 in stage 3.0 (TID 95) in 785 ms on 192.168.1.9 (executor 1) (93/200)
20/12/04 16:30:49 INFO TaskSetManager: Starting task 173.0 in stage 3.0 (TID 98, 192.168.1.9, executor 2, partition 173, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:49 INFO TaskSetManager: Finished task 170.0 in stage 3.0 (TID 96) in 1271 ms on 192.168.1.9 (executor 2) (94/200)
20/12/04 16:30:49 INFO TaskSetManager: Finished task 172.0 in stage 3.0 (TID 97) in 1199 ms on 192.168.1.9 (executor 1) (95/200)
20/12/04 16:30:49 INFO TaskSetManager: Starting task 174.0 in stage 3.0 (TID 99, 192.168.1.9, executor 1, partition 174, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:51 INFO TaskSetManager: Starting task 175.0 in stage 3.0 (TID 100, 192.168.1.9, executor 1, partition 175, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:51 INFO TaskSetManager: Finished task 174.0 in stage 3.0 (TID 99) in 2078 ms on 192.168.1.9 (executor 1) (96/200)
20/12/04 16:30:52 INFO TaskSetManager: Starting task 178.0 in stage 3.0 (TID 101, 192.168.1.9, executor 2, partition 178, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:52 INFO TaskSetManager: Finished task 173.0 in stage 3.0 (TID 98) in 2614 ms on 192.168.1.9 (executor 2) (97/200)
20/12/04 16:30:53 INFO TaskSetManager: Starting task 179.0 in stage 3.0 (TID 102, 192.168.1.9, executor 2, partition 179, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:53 INFO TaskSetManager: Finished task 178.0 in stage 3.0 (TID 101) in 890 ms on 192.168.1.9 (executor 2) (98/200)
20/12/04 16:30:54 INFO TaskSetManager: Starting task 181.0 in stage 3.0 (TID 103, 192.168.1.9, executor 1, partition 181, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:54 INFO TaskSetManager: Finished task 175.0 in stage 3.0 (TID 100) in 2252 ms on 192.168.1.9 (executor 1) (99/200)
20/12/04 16:30:54 INFO TaskSetManager: Starting task 183.0 in stage 3.0 (TID 104, 192.168.1.9, executor 2, partition 183, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:54 INFO TaskSetManager: Finished task 179.0 in stage 3.0 (TID 102) in 1506 ms on 192.168.1.9 (executor 2) (100/200)
20/12/04 16:30:55 INFO TaskSetManager: Starting task 184.0 in stage 3.0 (TID 105, 192.168.1.9, executor 2, partition 184, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:55 INFO TaskSetManager: Finished task 183.0 in stage 3.0 (TID 104) in 991 ms on 192.168.1.9 (executor 2) (101/200)
20/12/04 16:30:56 INFO TaskSetManager: Starting task 190.0 in stage 3.0 (TID 106, 192.168.1.9, executor 1, partition 190, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:56 INFO TaskSetManager: Finished task 181.0 in stage 3.0 (TID 103) in 2692 ms on 192.168.1.9 (executor 1) (102/200)
20/12/04 16:30:58 INFO TaskSetManager: Starting task 192.0 in stage 3.0 (TID 107, 192.168.1.9, executor 2, partition 192, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:58 INFO TaskSetManager: Finished task 184.0 in stage 3.0 (TID 105) in 2723 ms on 192.168.1.9 (executor 2) (103/200)
20/12/04 16:30:58 INFO TaskSetManager: Finished task 190.0 in stage 3.0 (TID 106) in 1713 ms on 192.168.1.9 (executor 1) (104/200)
20/12/04 16:30:58 INFO TaskSetManager: Starting task 193.0 in stage 3.0 (TID 108, 192.168.1.9, executor 1, partition 193, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:58 INFO TaskSetManager: Starting task 194.0 in stage 3.0 (TID 109, 192.168.1.9, executor 2, partition 194, NODE_LOCAL, 7336 bytes)
20/12/04 16:30:58 INFO TaskSetManager: Finished task 192.0 in stage 3.0 (TID 107) in 725 ms on 192.168.1.9 (executor 2) (105/200)
20/12/04 16:31:00 INFO TaskSetManager: Starting task 195.0 in stage 3.0 (TID 110, 192.168.1.9, executor 2, partition 195, NODE_LOCAL, 7336 bytes)
20/12/04 16:31:00 INFO TaskSetManager: Finished task 194.0 in stage 3.0 (TID 109) in 1282 ms on 192.168.1.9 (executor 2) (106/200)
20/12/04 16:31:01 INFO TaskSetManager: Starting task 198.0 in stage 3.0 (TID 111, 192.168.1.9, executor 1, partition 198, NODE_LOCAL, 7336 bytes)
20/12/04 16:31:01 INFO TaskSetManager: Finished task 193.0 in stage 3.0 (TID 108) in 2872 ms on 192.168.1.9 (executor 1) (107/200)
20/12/04 16:31:01 INFO TaskSetManager: Starting task 199.0 in stage 3.0 (TID 112, 192.168.1.9, executor 2, partition 199, NODE_LOCAL, 7336 bytes)
20/12/04 16:31:01 INFO TaskSetManager: Finished task 195.0 in stage 3.0 (TID 110) in 1580 ms on 192.168.1.9 (executor 2) (108/200)
20/12/04 16:31:02 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 113, 192.168.1.9, executor 1, partition 1, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:02 INFO TaskSetManager: Finished task 198.0 in stage 3.0 (TID 111) in 894 ms on 192.168.1.9 (executor 1) (109/200)
20/12/04 16:31:02 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 114, 192.168.1.9, executor 1, partition 8, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:02 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 113) in 129 ms on 192.168.1.9 (executor 1) (110/200)
20/12/04 16:31:02 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 115, 192.168.1.9, executor 1, partition 9, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:02 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 114) in 117 ms on 192.168.1.9 (executor 1) (111/200)
20/12/04 16:31:02 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 116, 192.168.1.9, executor 1, partition 15, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:02 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 115) in 194 ms on 192.168.1.9 (executor 1) (112/200)
20/12/04 16:31:02 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 117, 192.168.1.9, executor 1, partition 16, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:02 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 116) in 116 ms on 192.168.1.9 (executor 1) (113/200)
20/12/04 16:31:03 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 118, 192.168.1.9, executor 1, partition 17, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:03 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 117) in 115 ms on 192.168.1.9 (executor 1) (114/200)
20/12/04 16:31:03 INFO TaskSetManager: Starting task 20.0 in stage 3.0 (TID 119, 192.168.1.9, executor 1, partition 20, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:03 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 118) in 171 ms on 192.168.1.9 (executor 1) (115/200)
20/12/04 16:31:03 INFO TaskSetManager: Finished task 20.0 in stage 3.0 (TID 119) in 139 ms on 192.168.1.9 (executor 1) (116/200)
20/12/04 16:31:03 INFO TaskSetManager: Starting task 22.0 in stage 3.0 (TID 120, 192.168.1.9, executor 1, partition 22, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:03 INFO TaskSetManager: Starting task 25.0 in stage 3.0 (TID 121, 192.168.1.9, executor 1, partition 25, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:03 INFO TaskSetManager: Finished task 22.0 in stage 3.0 (TID 120) in 157 ms on 192.168.1.9 (executor 1) (117/200)
20/12/04 16:31:03 INFO TaskSetManager: Starting task 26.0 in stage 3.0 (TID 122, 192.168.1.9, executor 2, partition 26, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:03 INFO TaskSetManager: Finished task 199.0 in stage 3.0 (TID 112) in 1694 ms on 192.168.1.9 (executor 2) (118/200)
20/12/04 16:31:03 INFO TaskSetManager: Starting task 28.0 in stage 3.0 (TID 123, 192.168.1.9, executor 1, partition 28, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:03 INFO TaskSetManager: Finished task 25.0 in stage 3.0 (TID 121) in 199 ms on 192.168.1.9 (executor 1) (119/200)
20/12/04 16:31:03 INFO TaskSetManager: Starting task 29.0 in stage 3.0 (TID 124, 192.168.1.9, executor 2, partition 29, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:03 INFO TaskSetManager: Finished task 26.0 in stage 3.0 (TID 122) in 249 ms on 192.168.1.9 (executor 2) (120/200)
20/12/04 16:31:03 INFO TaskSetManager: Starting task 33.0 in stage 3.0 (TID 125, 192.168.1.9, executor 1, partition 33, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:03 INFO TaskSetManager: Finished task 28.0 in stage 3.0 (TID 123) in 127 ms on 192.168.1.9 (executor 1) (121/200)
20/12/04 16:31:03 INFO TaskSetManager: Starting task 38.0 in stage 3.0 (TID 126, 192.168.1.9, executor 1, partition 38, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:03 INFO TaskSetManager: Finished task 33.0 in stage 3.0 (TID 125) in 169 ms on 192.168.1.9 (executor 1) (122/200)
20/12/04 16:31:04 INFO TaskSetManager: Starting task 39.0 in stage 3.0 (TID 127, 192.168.1.9, executor 2, partition 39, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:04 INFO TaskSetManager: Finished task 29.0 in stage 3.0 (TID 124) in 283 ms on 192.168.1.9 (executor 2) (123/200)
20/12/04 16:31:04 INFO TaskSetManager: Starting task 40.0 in stage 3.0 (TID 128, 192.168.1.9, executor 1, partition 40, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:04 INFO TaskSetManager: Finished task 38.0 in stage 3.0 (TID 126) in 195 ms on 192.168.1.9 (executor 1) (124/200)
20/12/04 16:31:04 INFO TaskSetManager: Starting task 42.0 in stage 3.0 (TID 129, 192.168.1.9, executor 2, partition 42, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:04 INFO TaskSetManager: Finished task 39.0 in stage 3.0 (TID 127) in 202 ms on 192.168.1.9 (executor 2) (125/200)
20/12/04 16:31:04 INFO TaskSetManager: Starting task 45.0 in stage 3.0 (TID 130, 192.168.1.9, executor 1, partition 45, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:04 INFO TaskSetManager: Finished task 40.0 in stage 3.0 (TID 128) in 143 ms on 192.168.1.9 (executor 1) (126/200)
20/12/04 16:31:04 INFO TaskSetManager: Starting task 46.0 in stage 3.0 (TID 131, 192.168.1.9, executor 2, partition 46, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:04 INFO TaskSetManager: Finished task 42.0 in stage 3.0 (TID 129) in 221 ms on 192.168.1.9 (executor 2) (127/200)
20/12/04 16:31:04 INFO TaskSetManager: Starting task 47.0 in stage 3.0 (TID 132, 192.168.1.9, executor 1, partition 47, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:04 INFO TaskSetManager: Finished task 45.0 in stage 3.0 (TID 130) in 207 ms on 192.168.1.9 (executor 1) (128/200)
20/12/04 16:31:04 INFO TaskSetManager: Starting task 50.0 in stage 3.0 (TID 133, 192.168.1.9, executor 2, partition 50, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:04 INFO TaskSetManager: Finished task 46.0 in stage 3.0 (TID 131) in 192 ms on 192.168.1.9 (executor 2) (129/200)
20/12/04 16:31:04 INFO TaskSetManager: Starting task 52.0 in stage 3.0 (TID 134, 192.168.1.9, executor 1, partition 52, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:04 INFO TaskSetManager: Finished task 47.0 in stage 3.0 (TID 132) in 171 ms on 192.168.1.9 (executor 1) (130/200)
20/12/04 16:31:04 INFO TaskSetManager: Starting task 54.0 in stage 3.0 (TID 135, 192.168.1.9, executor 1, partition 54, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:04 INFO TaskSetManager: Finished task 52.0 in stage 3.0 (TID 134) in 125 ms on 192.168.1.9 (executor 1) (131/200)
20/12/04 16:31:04 INFO TaskSetManager: Starting task 56.0 in stage 3.0 (TID 136, 192.168.1.9, executor 2, partition 56, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:04 INFO TaskSetManager: Finished task 50.0 in stage 3.0 (TID 133) in 166 ms on 192.168.1.9 (executor 2) (132/200)
20/12/04 16:31:04 INFO TaskSetManager: Starting task 59.0 in stage 3.0 (TID 137, 192.168.1.9, executor 1, partition 59, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:04 INFO TaskSetManager: Finished task 54.0 in stage 3.0 (TID 135) in 117 ms on 192.168.1.9 (executor 1) (133/200)
20/12/04 16:31:04 INFO TaskSetManager: Starting task 60.0 in stage 3.0 (TID 138, 192.168.1.9, executor 2, partition 60, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:04 INFO TaskSetManager: Finished task 56.0 in stage 3.0 (TID 136) in 161 ms on 192.168.1.9 (executor 2) (134/200)
20/12/04 16:31:05 INFO TaskSetManager: Starting task 62.0 in stage 3.0 (TID 139, 192.168.1.9, executor 1, partition 62, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:05 INFO TaskSetManager: Finished task 59.0 in stage 3.0 (TID 137) in 113 ms on 192.168.1.9 (executor 1) (135/200)
20/12/04 16:31:05 INFO TaskSetManager: Starting task 64.0 in stage 3.0 (TID 140, 192.168.1.9, executor 2, partition 64, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:05 INFO TaskSetManager: Finished task 60.0 in stage 3.0 (TID 138) in 154 ms on 192.168.1.9 (executor 2) (136/200)
20/12/04 16:31:05 INFO TaskSetManager: Starting task 67.0 in stage 3.0 (TID 141, 192.168.1.9, executor 1, partition 67, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:05 INFO TaskSetManager: Finished task 62.0 in stage 3.0 (TID 139) in 122 ms on 192.168.1.9 (executor 1) (137/200)
20/12/04 16:31:05 INFO TaskSetManager: Starting task 68.0 in stage 3.0 (TID 142, 192.168.1.9, executor 1, partition 68, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:05 INFO TaskSetManager: Finished task 67.0 in stage 3.0 (TID 141) in 115 ms on 192.168.1.9 (executor 1) (138/200)
20/12/04 16:31:05 INFO TaskSetManager: Starting task 71.0 in stage 3.0 (TID 143, 192.168.1.9, executor 2, partition 71, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:05 INFO TaskSetManager: Finished task 64.0 in stage 3.0 (TID 140) in 177 ms on 192.168.1.9 (executor 2) (139/200)
20/12/04 16:31:05 INFO TaskSetManager: Starting task 72.0 in stage 3.0 (TID 144, 192.168.1.9, executor 1, partition 72, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:05 INFO TaskSetManager: Finished task 68.0 in stage 3.0 (TID 142) in 201 ms on 192.168.1.9 (executor 1) (140/200)
20/12/04 16:31:05 INFO TaskSetManager: Starting task 74.0 in stage 3.0 (TID 145, 192.168.1.9, executor 2, partition 74, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:05 INFO TaskSetManager: Finished task 71.0 in stage 3.0 (TID 143) in 261 ms on 192.168.1.9 (executor 2) (141/200)
20/12/04 16:31:05 INFO TaskSetManager: Starting task 76.0 in stage 3.0 (TID 146, 192.168.1.9, executor 1, partition 76, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:05 INFO TaskSetManager: Finished task 72.0 in stage 3.0 (TID 144) in 156 ms on 192.168.1.9 (executor 1) (142/200)
20/12/04 16:31:05 INFO TaskSetManager: Starting task 78.0 in stage 3.0 (TID 147, 192.168.1.9, executor 2, partition 78, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:05 INFO TaskSetManager: Finished task 74.0 in stage 3.0 (TID 145) in 239 ms on 192.168.1.9 (executor 2) (143/200)
20/12/04 16:31:05 INFO TaskSetManager: Starting task 80.0 in stage 3.0 (TID 148, 192.168.1.9, executor 1, partition 80, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:05 INFO TaskSetManager: Finished task 76.0 in stage 3.0 (TID 146) in 193 ms on 192.168.1.9 (executor 1) (144/200)
20/12/04 16:31:05 INFO TaskSetManager: Starting task 81.0 in stage 3.0 (TID 149, 192.168.1.9, executor 1, partition 81, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:05 INFO TaskSetManager: Finished task 80.0 in stage 3.0 (TID 148) in 116 ms on 192.168.1.9 (executor 1) (145/200)
20/12/04 16:31:05 INFO TaskSetManager: Starting task 82.0 in stage 3.0 (TID 150, 192.168.1.9, executor 2, partition 82, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:05 INFO TaskSetManager: Finished task 78.0 in stage 3.0 (TID 147) in 162 ms on 192.168.1.9 (executor 2) (146/200)
20/12/04 16:31:06 INFO TaskSetManager: Starting task 83.0 in stage 3.0 (TID 151, 192.168.1.9, executor 1, partition 83, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:06 INFO TaskSetManager: Finished task 81.0 in stage 3.0 (TID 149) in 116 ms on 192.168.1.9 (executor 1) (147/200)
20/12/04 16:31:06 INFO TaskSetManager: Starting task 87.0 in stage 3.0 (TID 152, 192.168.1.9, executor 2, partition 87, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:06 INFO TaskSetManager: Finished task 82.0 in stage 3.0 (TID 150) in 155 ms on 192.168.1.9 (executor 2) (148/200)
20/12/04 16:31:06 INFO TaskSetManager: Starting task 90.0 in stage 3.0 (TID 153, 192.168.1.9, executor 1, partition 90, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:06 INFO TaskSetManager: Finished task 83.0 in stage 3.0 (TID 151) in 118 ms on 192.168.1.9 (executor 1) (149/200)
20/12/04 16:31:06 INFO TaskSetManager: Starting task 92.0 in stage 3.0 (TID 154, 192.168.1.9, executor 2, partition 92, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:06 INFO TaskSetManager: Finished task 87.0 in stage 3.0 (TID 152) in 172 ms on 192.168.1.9 (executor 2) (150/200)
20/12/04 16:31:06 INFO TaskSetManager: Starting task 93.0 in stage 3.0 (TID 155, 192.168.1.9, executor 1, partition 93, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:06 INFO TaskSetManager: Finished task 90.0 in stage 3.0 (TID 153) in 131 ms on 192.168.1.9 (executor 1) (151/200)
20/12/04 16:31:06 INFO TaskSetManager: Starting task 94.0 in stage 3.0 (TID 156, 192.168.1.9, executor 1, partition 94, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:06 INFO TaskSetManager: Finished task 93.0 in stage 3.0 (TID 155) in 131 ms on 192.168.1.9 (executor 1) (152/200)
20/12/04 16:31:06 INFO TaskSetManager: Starting task 96.0 in stage 3.0 (TID 157, 192.168.1.9, executor 2, partition 96, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:06 INFO TaskSetManager: Finished task 92.0 in stage 3.0 (TID 154) in 186 ms on 192.168.1.9 (executor 2) (153/200)
20/12/04 16:31:06 INFO TaskSetManager: Starting task 97.0 in stage 3.0 (TID 158, 192.168.1.9, executor 1, partition 97, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:06 INFO TaskSetManager: Finished task 94.0 in stage 3.0 (TID 156) in 119 ms on 192.168.1.9 (executor 1) (154/200)
20/12/04 16:31:06 INFO TaskSetManager: Starting task 98.0 in stage 3.0 (TID 159, 192.168.1.9, executor 2, partition 98, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:06 INFO TaskSetManager: Finished task 96.0 in stage 3.0 (TID 157) in 163 ms on 192.168.1.9 (executor 2) (155/200)
20/12/04 16:31:06 INFO TaskSetManager: Starting task 99.0 in stage 3.0 (TID 160, 192.168.1.9, executor 1, partition 99, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:06 INFO TaskSetManager: Finished task 97.0 in stage 3.0 (TID 158) in 117 ms on 192.168.1.9 (executor 1) (156/200)
20/12/04 16:31:06 INFO TaskSetManager: Starting task 100.0 in stage 3.0 (TID 161, 192.168.1.9, executor 1, partition 100, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:06 INFO TaskSetManager: Finished task 99.0 in stage 3.0 (TID 160) in 191 ms on 192.168.1.9 (executor 1) (157/200)
20/12/04 16:31:06 INFO TaskSetManager: Starting task 104.0 in stage 3.0 (TID 162, 192.168.1.9, executor 2, partition 104, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:06 INFO TaskSetManager: Finished task 98.0 in stage 3.0 (TID 159) in 236 ms on 192.168.1.9 (executor 2) (158/200)
20/12/04 16:31:06 INFO TaskSetManager: Starting task 110.0 in stage 3.0 (TID 163, 192.168.1.9, executor 1, partition 110, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:06 INFO TaskSetManager: Finished task 100.0 in stage 3.0 (TID 161) in 116 ms on 192.168.1.9 (executor 1) (159/200)
20/12/04 16:31:07 INFO TaskSetManager: Starting task 112.0 in stage 3.0 (TID 164, 192.168.1.9, executor 2, partition 112, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:07 INFO TaskSetManager: Finished task 104.0 in stage 3.0 (TID 162) in 161 ms on 192.168.1.9 (executor 2) (160/200)
20/12/04 16:31:07 INFO TaskSetManager: Starting task 114.0 in stage 3.0 (TID 165, 192.168.1.9, executor 1, partition 114, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:07 INFO TaskSetManager: Finished task 110.0 in stage 3.0 (TID 163) in 154 ms on 192.168.1.9 (executor 1) (161/200)
20/12/04 16:31:07 INFO TaskSetManager: Starting task 117.0 in stage 3.0 (TID 166, 192.168.1.9, executor 2, partition 117, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:07 INFO TaskSetManager: Finished task 112.0 in stage 3.0 (TID 164) in 232 ms on 192.168.1.9 (executor 2) (162/200)
20/12/04 16:31:07 INFO TaskSetManager: Starting task 118.0 in stage 3.0 (TID 167, 192.168.1.9, executor 1, partition 118, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:07 INFO TaskSetManager: Finished task 114.0 in stage 3.0 (TID 165) in 159 ms on 192.168.1.9 (executor 1) (163/200)
20/12/04 16:31:07 INFO TaskSetManager: Starting task 120.0 in stage 3.0 (TID 168, 192.168.1.9, executor 1, partition 120, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:07 INFO TaskSetManager: Finished task 118.0 in stage 3.0 (TID 167) in 112 ms on 192.168.1.9 (executor 1) (164/200)
20/12/04 16:31:07 INFO TaskSetManager: Starting task 121.0 in stage 3.0 (TID 169, 192.168.1.9, executor 2, partition 121, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:07 INFO TaskSetManager: Finished task 117.0 in stage 3.0 (TID 166) in 159 ms on 192.168.1.9 (executor 2) (165/200)
20/12/04 16:31:07 INFO TaskSetManager: Starting task 123.0 in stage 3.0 (TID 170, 192.168.1.9, executor 1, partition 123, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:07 INFO TaskSetManager: Finished task 120.0 in stage 3.0 (TID 168) in 115 ms on 192.168.1.9 (executor 1) (166/200)
20/12/04 16:31:07 INFO TaskSetManager: Starting task 125.0 in stage 3.0 (TID 171, 192.168.1.9, executor 2, partition 125, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:07 INFO TaskSetManager: Finished task 121.0 in stage 3.0 (TID 169) in 177 ms on 192.168.1.9 (executor 2) (167/200)
20/12/04 16:31:07 INFO TaskSetManager: Starting task 127.0 in stage 3.0 (TID 172, 192.168.1.9, executor 1, partition 127, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:07 INFO TaskSetManager: Finished task 123.0 in stage 3.0 (TID 170) in 129 ms on 192.168.1.9 (executor 1) (168/200)
20/12/04 16:31:07 INFO TaskSetManager: Starting task 130.0 in stage 3.0 (TID 173, 192.168.1.9, executor 1, partition 130, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:07 INFO TaskSetManager: Finished task 127.0 in stage 3.0 (TID 172) in 142 ms on 192.168.1.9 (executor 1) (169/200)
20/12/04 16:31:07 INFO TaskSetManager: Starting task 134.0 in stage 3.0 (TID 174, 192.168.1.9, executor 2, partition 134, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:07 INFO TaskSetManager: Finished task 125.0 in stage 3.0 (TID 171) in 202 ms on 192.168.1.9 (executor 2) (170/200)
20/12/04 16:31:07 INFO TaskSetManager: Starting task 138.0 in stage 3.0 (TID 175, 192.168.1.9, executor 1, partition 138, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:07 INFO TaskSetManager: Finished task 130.0 in stage 3.0 (TID 173) in 135 ms on 192.168.1.9 (executor 1) (171/200)
20/12/04 16:31:07 INFO TaskSetManager: Starting task 142.0 in stage 3.0 (TID 176, 192.168.1.9, executor 2, partition 142, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:07 INFO TaskSetManager: Finished task 134.0 in stage 3.0 (TID 174) in 185 ms on 192.168.1.9 (executor 2) (172/200)
20/12/04 16:31:08 INFO TaskSetManager: Starting task 144.0 in stage 3.0 (TID 177, 192.168.1.9, executor 1, partition 144, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:08 INFO TaskSetManager: Finished task 138.0 in stage 3.0 (TID 175) in 134 ms on 192.168.1.9 (executor 1) (173/200)
20/12/04 16:31:08 INFO TaskSetManager: Starting task 145.0 in stage 3.0 (TID 178, 192.168.1.9, executor 1, partition 145, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:08 INFO TaskSetManager: Finished task 144.0 in stage 3.0 (TID 177) in 117 ms on 192.168.1.9 (executor 1) (174/200)
20/12/04 16:31:08 INFO TaskSetManager: Starting task 147.0 in stage 3.0 (TID 179, 192.168.1.9, executor 2, partition 147, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:08 INFO TaskSetManager: Finished task 142.0 in stage 3.0 (TID 176) in 242 ms on 192.168.1.9 (executor 2) (175/200)
20/12/04 16:31:08 INFO TaskSetManager: Starting task 148.0 in stage 3.0 (TID 180, 192.168.1.9, executor 1, partition 148, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:08 INFO TaskSetManager: Finished task 145.0 in stage 3.0 (TID 178) in 144 ms on 192.168.1.9 (executor 1) (176/200)
20/12/04 16:31:08 INFO TaskSetManager: Starting task 149.0 in stage 3.0 (TID 181, 192.168.1.9, executor 1, partition 149, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:08 INFO TaskSetManager: Finished task 148.0 in stage 3.0 (TID 180) in 118 ms on 192.168.1.9 (executor 1) (177/200)
20/12/04 16:31:08 INFO TaskSetManager: Starting task 152.0 in stage 3.0 (TID 182, 192.168.1.9, executor 2, partition 152, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:08 INFO TaskSetManager: Finished task 147.0 in stage 3.0 (TID 179) in 245 ms on 192.168.1.9 (executor 2) (178/200)
20/12/04 16:31:08 INFO TaskSetManager: Starting task 154.0 in stage 3.0 (TID 183, 192.168.1.9, executor 1, partition 154, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:08 INFO TaskSetManager: Finished task 149.0 in stage 3.0 (TID 181) in 125 ms on 192.168.1.9 (executor 1) (179/200)
20/12/04 16:31:08 INFO TaskSetManager: Starting task 157.0 in stage 3.0 (TID 184, 192.168.1.9, executor 2, partition 157, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:08 INFO TaskSetManager: Finished task 152.0 in stage 3.0 (TID 182) in 171 ms on 192.168.1.9 (executor 2) (180/200)
20/12/04 16:31:08 INFO TaskSetManager: Starting task 158.0 in stage 3.0 (TID 185, 192.168.1.9, executor 1, partition 158, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:08 INFO TaskSetManager: Finished task 154.0 in stage 3.0 (TID 183) in 145 ms on 192.168.1.9 (executor 1) (181/200)
20/12/04 16:31:08 INFO TaskSetManager: Starting task 159.0 in stage 3.0 (TID 186, 192.168.1.9, executor 2, partition 159, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:08 INFO TaskSetManager: Finished task 157.0 in stage 3.0 (TID 184) in 174 ms on 192.168.1.9 (executor 2) (182/200)
20/12/04 16:31:08 INFO TaskSetManager: Starting task 160.0 in stage 3.0 (TID 187, 192.168.1.9, executor 1, partition 160, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:08 INFO TaskSetManager: Finished task 158.0 in stage 3.0 (TID 185) in 164 ms on 192.168.1.9 (executor 1) (183/200)
20/12/04 16:31:08 INFO TaskSetManager: Starting task 166.0 in stage 3.0 (TID 188, 192.168.1.9, executor 1, partition 166, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:08 INFO TaskSetManager: Finished task 160.0 in stage 3.0 (TID 187) in 139 ms on 192.168.1.9 (executor 1) (184/200)
20/12/04 16:31:08 INFO TaskSetManager: Starting task 169.0 in stage 3.0 (TID 189, 192.168.1.9, executor 2, partition 169, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:08 INFO TaskSetManager: Finished task 159.0 in stage 3.0 (TID 186) in 178 ms on 192.168.1.9 (executor 2) (185/200)
20/12/04 16:31:09 INFO TaskSetManager: Starting task 171.0 in stage 3.0 (TID 190, 192.168.1.9, executor 1, partition 171, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:09 INFO TaskSetManager: Finished task 166.0 in stage 3.0 (TID 188) in 180 ms on 192.168.1.9 (executor 1) (186/200)
20/12/04 16:31:09 INFO TaskSetManager: Starting task 176.0 in stage 3.0 (TID 191, 192.168.1.9, executor 2, partition 176, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:09 INFO TaskSetManager: Finished task 169.0 in stage 3.0 (TID 189) in 235 ms on 192.168.1.9 (executor 2) (187/200)
20/12/04 16:31:09 INFO TaskSetManager: Starting task 177.0 in stage 3.0 (TID 192, 192.168.1.9, executor 1, partition 177, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:09 INFO TaskSetManager: Finished task 171.0 in stage 3.0 (TID 190) in 147 ms on 192.168.1.9 (executor 1) (188/200)
20/12/04 16:31:09 INFO TaskSetManager: Starting task 180.0 in stage 3.0 (TID 193, 192.168.1.9, executor 2, partition 180, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:09 INFO TaskSetManager: Finished task 176.0 in stage 3.0 (TID 191) in 164 ms on 192.168.1.9 (executor 2) (189/200)
20/12/04 16:31:09 INFO TaskSetManager: Starting task 182.0 in stage 3.0 (TID 194, 192.168.1.9, executor 1, partition 182, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:09 INFO TaskSetManager: Finished task 177.0 in stage 3.0 (TID 192) in 127 ms on 192.168.1.9 (executor 1) (190/200)
20/12/04 16:31:09 INFO TaskSetManager: Starting task 185.0 in stage 3.0 (TID 195, 192.168.1.9, executor 1, partition 185, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:09 INFO TaskSetManager: Finished task 182.0 in stage 3.0 (TID 194) in 117 ms on 192.168.1.9 (executor 1) (191/200)
20/12/04 16:31:09 INFO TaskSetManager: Starting task 186.0 in stage 3.0 (TID 196, 192.168.1.9, executor 2, partition 186, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:09 INFO TaskSetManager: Finished task 180.0 in stage 3.0 (TID 193) in 160 ms on 192.168.1.9 (executor 2) (192/200)
20/12/04 16:31:09 INFO TaskSetManager: Starting task 187.0 in stage 3.0 (TID 197, 192.168.1.9, executor 1, partition 187, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:09 INFO TaskSetManager: Finished task 185.0 in stage 3.0 (TID 195) in 111 ms on 192.168.1.9 (executor 1) (193/200)
20/12/04 16:31:09 INFO TaskSetManager: Starting task 188.0 in stage 3.0 (TID 198, 192.168.1.9, executor 2, partition 188, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:09 INFO TaskSetManager: Finished task 186.0 in stage 3.0 (TID 196) in 156 ms on 192.168.1.9 (executor 2) (194/200)
20/12/04 16:31:09 INFO TaskSetManager: Starting task 189.0 in stage 3.0 (TID 199, 192.168.1.9, executor 1, partition 189, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:09 INFO TaskSetManager: Finished task 187.0 in stage 3.0 (TID 197) in 164 ms on 192.168.1.9 (executor 1) (195/200)
20/12/04 16:31:09 INFO TaskSetManager: Starting task 191.0 in stage 3.0 (TID 200, 192.168.1.9, executor 2, partition 191, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:09 INFO TaskSetManager: Finished task 188.0 in stage 3.0 (TID 198) in 220 ms on 192.168.1.9 (executor 2) (196/200)
20/12/04 16:31:09 INFO TaskSetManager: Starting task 196.0 in stage 3.0 (TID 201, 192.168.1.9, executor 1, partition 196, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:09 INFO TaskSetManager: Finished task 189.0 in stage 3.0 (TID 199) in 129 ms on 192.168.1.9 (executor 1) (197/200)
20/12/04 16:31:10 INFO TaskSetManager: Starting task 197.0 in stage 3.0 (TID 202, 192.168.1.9, executor 1, partition 197, PROCESS_LOCAL, 7336 bytes)
20/12/04 16:31:10 INFO TaskSetManager: Finished task 196.0 in stage 3.0 (TID 201) in 156 ms on 192.168.1.9 (executor 1) (198/200)
20/12/04 16:31:10 INFO TaskSetManager: Finished task 191.0 in stage 3.0 (TID 200) in 207 ms on 192.168.1.9 (executor 2) (199/200)
20/12/04 16:31:10 INFO TaskSetManager: Finished task 197.0 in stage 3.0 (TID 202) in 117 ms on 192.168.1.9 (executor 1) (200/200)
20/12/04 16:31:10 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/04 16:31:10 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/userFiles-d1287690-4709-44d2-a4f0-f0f979a56c90/darima.zip/darima/dlsa.py:22) finished in 123.085 s
20/12/04 16:31:10 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:31:10 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
20/12/04 16:31:10 INFO DAGScheduler: Job 1 finished: toPandas at /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/userFiles-d1287690-4709-44d2-a4f0-f0f979a56c90/darima.zip/darima/dlsa.py:22, took 125.990810 s
20/12/04 16:31:10 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:31:10 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:31:10 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 16:31:10 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:31:10 INFO CodeGenerator: Code generated in 15.463251 ms
20/12/04 16:31:10 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:31:10 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:31:10 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:40463 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:31:10 INFO SparkContext: Created broadcast 6 from toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201
20/12/04 16:31:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:31:10 INFO SparkContext: Starting job: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201
20/12/04 16:31:10 INFO DAGScheduler: Got job 2 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201) with 1 output partitions
20/12/04 16:31:10 INFO DAGScheduler: Final stage: ResultStage 4 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201)
20/12/04 16:31:10 INFO DAGScheduler: Parents of final stage: List()
20/12/04 16:31:10 INFO DAGScheduler: Missing parents: List()
20/12/04 16:31:10 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201), which has no missing parents
20/12/04 16:31:10 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 21.7 KiB, free 5.8 GiB)
20/12/04 16:31:10 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 5.8 GiB)
20/12/04 16:31:10 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:40463 (size: 10.4 KiB, free: 5.8 GiB)
20/12/04 16:31:10 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1223
20/12/04 16:31:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201) (first 15 tasks are for partitions Vector(0))
20/12/04 16:31:10 INFO YarnScheduler: Adding task set 4.0 with 1 tasks
20/12/04 16:31:10 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 203, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7790 bytes)
20/12/04 16:31:10 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:46649 (size: 10.4 KiB, free: 912.1 MiB)
20/12/04 16:31:10 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:46649 (size: 28.7 KiB, free: 912.1 MiB)
20/12/04 16:31:11 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 203) in 593 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 16:31:11 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/12/04 16:31:11 INFO DAGScheduler: ResultStage 4 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201) finished in 0.619 s
20/12/04 16:31:11 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:31:11 INFO YarnScheduler: Killing all running tasks in stage 4: Stage finished
20/12/04 16:31:11 INFO DAGScheduler: Job 2 finished: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:201, took 0.628762 s
20/12/04 16:31:16 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 16:31:16 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 16:31:16 INFO FileSourceStrategy: Post-Scan Filters: 
20/12/04 16:31:16 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 16:31:16 INFO CodeGenerator: Code generated in 12.981993 ms
20/12/04 16:31:16 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 16:31:16 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 16:31:16 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:40463 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 16:31:16 INFO SparkContext: Created broadcast 8 from toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217
20/12/04 16:31:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 16:31:16 INFO SparkContext: Starting job: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217
20/12/04 16:31:16 INFO DAGScheduler: Got job 3 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217) with 1 output partitions
20/12/04 16:31:16 INFO DAGScheduler: Final stage: ResultStage 5 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217)
20/12/04 16:31:16 INFO DAGScheduler: Parents of final stage: List()
20/12/04 16:31:16 INFO DAGScheduler: Missing parents: List()
20/12/04 16:31:16 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217), which has no missing parents
20/12/04 16:31:16 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 12.3 KiB, free 5.8 GiB)
20/12/04 16:31:16 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 5.8 GiB)
20/12/04 16:31:16 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:40463 (size: 6.3 KiB, free: 5.8 GiB)
20/12/04 16:31:16 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1223
20/12/04 16:31:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217) (first 15 tasks are for partitions Vector(0))
20/12/04 16:31:16 INFO YarnScheduler: Adding task set 5.0 with 1 tasks
20/12/04 16:31:16 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 204, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7789 bytes)
20/12/04 16:31:16 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:46649 (size: 6.3 KiB, free: 912.1 MiB)
20/12/04 16:31:16 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:46649 (size: 28.7 KiB, free: 912.1 MiB)
20/12/04 16:31:16 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 204) in 173 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 16:31:16 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/12/04 16:31:16 INFO DAGScheduler: ResultStage 5 (toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217) finished in 0.199 s
20/12/04 16:31:16 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 16:31:16 INFO YarnScheduler: Killing all running tasks in stage 5: Stage finished
20/12/04 16:31:16 INFO DAGScheduler: Job 3 finished: toPandas at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:217, took 0.215879 s
20/12/04 16:31:20 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 16:31:20 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 16:31:20 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 16:31:20 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 16:31:20 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 16:31:21 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 16:31:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 16:31:21 INFO MemoryStore: MemoryStore cleared
20/12/04 16:31:21 INFO BlockManager: BlockManager stopped
20/12/04 16:31:21 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 16:31:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 16:31:21 INFO SparkContext: Successfully stopped SparkContext
20/12/04 16:31:21 INFO ShutdownHookManager: Shutdown hook called
20/12/04 16:31:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-1c1cd901-399c-47d0-bf2a-9dea1ad855aa
20/12/04 16:31:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf/pyspark-ee16ebf6-0da2-4353-9558-9370a8ce70be
20/12/04 16:31:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-99bba085-304e-42b0-a62e-ab7c057938cf
20/12/04 18:43:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 18:43:06 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 18:43:06 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 18:43:06 INFO SecurityManager: Changing view acls groups to: 
20/12/04 18:43:06 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 18:43:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 18:43:07 INFO SparkContext: Running Spark version 3.0.1
20/12/04 18:43:07 INFO ResourceUtils: ==============================================================
20/12/04 18:43:07 INFO ResourceUtils: Resources for spark.driver:

20/12/04 18:43:07 INFO ResourceUtils: ==============================================================
20/12/04 18:43:07 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 18:43:07 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 18:43:07 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 18:43:07 INFO SecurityManager: Changing view acls groups to: 
20/12/04 18:43:07 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 18:43:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 18:43:08 INFO Utils: Successfully started service 'sparkDriver' on port 35813.
20/12/04 18:43:08 INFO SparkEnv: Registering MapOutputTracker
20/12/04 18:43:08 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 18:43:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 18:43:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 18:43:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 18:43:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9fb03024-1553-48c0-a401-2c6ea77098a5
20/12/04 18:43:08 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 18:43:08 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 18:43:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 18:43:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 18:43:09 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 18:43:09 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 18:43:09 INFO Configuration: resource-types.xml not found
20/12/04 18:43:09 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 18:43:10 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 18:43:10 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 18:43:10 INFO Client: Setting up container launch context for our AM
20/12/04 18:43:10 INFO Client: Setting up the launch environment for our AM container
20/12/04 18:43:10 INFO Client: Preparing resources for our AM container
20/12/04 18:43:10 WARN Client: Failed to cleanup staging dir hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0079
org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/bsuconn/.sparkStaging/application_1607015337794_0079. Name node is in safe mode.
The reported blocks 37 has reached the threshold 0.9990 of total blocks 37. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 5 seconds. NamenodeHostName:localhost
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3084)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1114)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:705)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1608)
	at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:952)
	at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:949)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:959)
	at org.apache.spark.deploy.yarn.Client.cleanupStagingDirInternal$1(Client.scala:226)
	at org.apache.spark.deploy.yarn.Client.cleanupStagingDir(Client.scala:235)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:209)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:60)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:201)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:555)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot delete /user/bsuconn/.sparkStaging/application_1607015337794_0079. Name node is in safe mode.
The reported blocks 37 has reached the threshold 0.9990 of total blocks 37. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 5 seconds. NamenodeHostName:localhost
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3084)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1114)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:705)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.delete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:637)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy20.delete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1606)
	... 22 more
20/12/04 18:43:10 ERROR SparkContext: Error initializing SparkContext.
org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /user/bsuconn/.sparkStaging/application_1607015337794_0079. Name node is in safe mode.
The reported blocks 37 has reached the threshold 0.9990 of total blocks 37. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 5 seconds. NamenodeHostName:localhost
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2426)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2400)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1324)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1321)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1338)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1313)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:674)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:441)
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:876)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:196)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:60)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:201)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:555)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /user/bsuconn/.sparkStaging/application_1607015337794_0079. Name node is in safe mode.
The reported blocks 37 has reached the threshold 0.9990 of total blocks 37. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 5 seconds. NamenodeHostName:localhost
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:656)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy20.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2424)
	... 26 more
20/12/04 18:43:10 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 18:43:10 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
20/12/04 18:43:10 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 18:43:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 18:43:10 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 18:43:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 18:43:10 INFO MemoryStore: MemoryStore cleared
20/12/04 18:43:10 INFO BlockManager: BlockManager stopped
20/12/04 18:43:10 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 18:43:10 WARN MetricsSystem: Stopping a MetricsSystem that is not running
20/12/04 18:43:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 18:43:10 INFO SparkContext: Successfully stopped SparkContext
20/12/04 18:43:10 INFO ShutdownHookManager: Shutdown hook called
20/12/04 18:43:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-b4f13ff6-ae7c-49fc-8485-756df5c55175
20/12/04 18:43:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-aa8941e5-3a4c-4087-8add-b6fbeb18128b
20/12/04 18:43:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/04 18:43:13 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 18:43:13 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 18:43:13 INFO SecurityManager: Changing view acls groups to: 
20/12/04 18:43:13 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 18:43:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 18:43:13 INFO SparkContext: Running Spark version 3.0.1
20/12/04 18:43:13 INFO ResourceUtils: ==============================================================
20/12/04 18:43:13 INFO ResourceUtils: Resources for spark.driver:

20/12/04 18:43:13 INFO ResourceUtils: ==============================================================
20/12/04 18:43:13 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/04 18:43:13 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 18:43:13 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 18:43:13 INFO SecurityManager: Changing view acls groups to: 
20/12/04 18:43:13 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 18:43:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 18:43:14 INFO Utils: Successfully started service 'sparkDriver' on port 44145.
20/12/04 18:43:14 INFO SparkEnv: Registering MapOutputTracker
20/12/04 18:43:14 INFO SparkEnv: Registering BlockManagerMaster
20/12/04 18:43:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/04 18:43:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/04 18:43:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/04 18:43:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f91ed6b4-0944-4f45-8c0a-acf26408d555
20/12/04 18:43:14 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/04 18:43:14 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/04 18:43:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/04 18:43:14 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/04 18:43:15 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/04 18:43:15 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/04 18:43:16 INFO Configuration: resource-types.xml not found
20/12/04 18:43:16 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/04 18:43:16 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/04 18:43:16 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/04 18:43:16 INFO Client: Setting up container launch context for our AM
20/12/04 18:43:16 INFO Client: Setting up the launch environment for our AM container
20/12/04 18:43:16 INFO Client: Preparing resources for our AM container
20/12/04 18:43:16 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/04 18:43:18 INFO Client: Uploading resource file:/tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/__spark_libs__16299448609588007531.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0080/__spark_libs__16299448609588007531.zip
20/12/04 18:43:20 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0080/pyspark.zip
20/12/04 18:43:20 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0080/py4j-0.10.9-src.zip
20/12/04 18:43:20 INFO Client: Uploading resource file:/tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/__spark_conf__8336543205274708347.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0080/__spark_conf__.zip
20/12/04 18:43:20 INFO SecurityManager: Changing view acls to: bsuconn
20/12/04 18:43:20 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/04 18:43:20 INFO SecurityManager: Changing view acls groups to: 
20/12/04 18:43:20 INFO SecurityManager: Changing modify acls groups to: 
20/12/04 18:43:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/04 18:43:20 INFO Client: Submitting application application_1607015337794_0080 to ResourceManager
20/12/04 18:43:20 INFO YarnClientImpl: Submitted application application_1607015337794_0080
20/12/04 18:43:21 INFO Client: Application report for application_1607015337794_0080 (state: ACCEPTED)
20/12/04 18:43:21 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607125400589
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0080/
	 user: bsuconn
20/12/04 18:43:22 INFO Client: Application report for application_1607015337794_0080 (state: ACCEPTED)
20/12/04 18:43:23 INFO Client: Application report for application_1607015337794_0080 (state: ACCEPTED)
20/12/04 18:43:24 INFO Client: Application report for application_1607015337794_0080 (state: ACCEPTED)
20/12/04 18:43:25 INFO Client: Application report for application_1607015337794_0080 (state: ACCEPTED)
20/12/04 18:43:26 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0080), /proxy/application_1607015337794_0080
20/12/04 18:43:26 INFO Client: Application report for application_1607015337794_0080 (state: RUNNING)
20/12/04 18:43:26 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607125400589
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0080/
	 user: bsuconn
20/12/04 18:43:26 INFO YarnClientSchedulerBackend: Application application_1607015337794_0080 has started running.
20/12/04 18:43:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40087.
20/12/04 18:43:26 INFO NettyBlockTransferService: Server created on 192.168.1.9:40087
20/12/04 18:43:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/04 18:43:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 40087, None)
20/12/04 18:43:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40087 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 40087, None)
20/12/04 18:43:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 40087, None)
20/12/04 18:43:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 40087, None)
20/12/04 18:43:27 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 18:43:27 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/04 18:43:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/04 18:43:33 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:47140) with ID 1
20/12/04 18:43:33 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:46865 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 46865, None)
20/12/04 18:43:34 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:47148) with ID 2
20/12/04 18:43:34 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/04 18:43:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40235 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 40235, None)
20/12/04 18:43:34 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/04 18:43:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/04 18:43:34 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/04 18:43:34 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 18:43:34 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 18:43:34 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 18:43:34 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 18:43:34 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/04 18:43:35 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:44145/files/darima.zip with timestamp 1607125415569
20/12/04 18:43:35 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/userFiles-fd9d4b08-9195-42cc-bc9f-a0c7bac66e1c/darima.zip
20/12/04 18:43:37 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
20/12/04 18:43:39 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 1 paths.
20/12/04 18:43:39 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 18:43:39 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 18:43:39 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 18:43:39 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 18:43:40 INFO CodeGenerator: Code generated in 323.005653 ms
20/12/04 18:43:40 INFO CodeGenerator: Code generated in 33.399015 ms
20/12/04 18:43:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 18:43:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 18:43:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:40087 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 18:43:40 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/04 18:43:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 18:43:41 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/04 18:43:41 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/04 18:43:41 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/04 18:43:41 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/04 18:43:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/04 18:43:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/04 18:43:41 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 18:43:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/04 18:43:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/04 18:43:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:40087 (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 18:43:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/04 18:43:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 18:43:41 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/04 18:43:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 18:43:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:40235 (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 18:43:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:40235 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 18:43:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3768 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 18:43:45 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/04 18:43:45 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.932 s
20/12/04 18:43:45 INFO DAGScheduler: looking for newly runnable stages
20/12/04 18:43:45 INFO DAGScheduler: running: Set()
20/12/04 18:43:45 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/04 18:43:45 INFO DAGScheduler: failed: Set()
20/12/04 18:43:45 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/04 18:43:45 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/04 18:43:45 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/04 18:43:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:40087 (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 18:43:45 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/04 18:43:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/04 18:43:45 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/04 18:43:45 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 18:43:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:40235 (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 18:43:45 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:47148
20/12/04 18:43:45 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 426 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 18:43:45 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/04 18:43:45 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.445 s
20/12/04 18:43:45 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/04 18:43:45 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/04 18:43:45 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 4.482763 s
20/12/04 18:43:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:40235 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/04 18:43:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:40087 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/04 18:43:49 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:40087 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/04 18:43:49 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:40235 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/04 18:43:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:40235 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 18:43:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:40087 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 18:43:49 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/04 18:43:50 INFO FileSourceStrategy: Pruning directories with: 
20/12/04 18:43:50 INFO FileSourceStrategy: Pushed Filters: 
20/12/04 18:43:50 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/04 18:43:50 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/04 18:43:50 INFO CodeGenerator: Code generated in 36.361168 ms
20/12/04 18:43:50 INFO CodeGenerator: Code generated in 22.483372 ms
20/12/04 18:43:50 INFO CodeGenerator: Code generated in 22.618092 ms
20/12/04 18:43:50 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/04 18:43:50 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/04 18:43:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:40087 (size: 28.7 KiB, free: 5.8 GiB)
20/12/04 18:43:50 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/userFiles-fd9d4b08-9195-42cc-bc9f-a0c7bac66e1c/darima.zip/darima/dlsa.py:22
20/12/04 18:43:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/04 18:43:51 INFO SparkContext: Starting job: toPandas at /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/userFiles-fd9d4b08-9195-42cc-bc9f-a0c7bac66e1c/darima.zip/darima/dlsa.py:22
20/12/04 18:43:51 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/userFiles-fd9d4b08-9195-42cc-bc9f-a0c7bac66e1c/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/04 18:43:51 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/userFiles-fd9d4b08-9195-42cc-bc9f-a0c7bac66e1c/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/04 18:43:51 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/userFiles-fd9d4b08-9195-42cc-bc9f-a0c7bac66e1c/darima.zip/darima/dlsa.py:22)
20/12/04 18:43:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/04 18:43:51 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/04 18:43:51 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/userFiles-fd9d4b08-9195-42cc-bc9f-a0c7bac66e1c/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 18:43:51 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/04 18:43:51 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/04 18:43:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:40087 (size: 11.6 KiB, free: 5.8 GiB)
20/12/04 18:43:51 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/04 18:43:51 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/userFiles-fd9d4b08-9195-42cc-bc9f-a0c7bac66e1c/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/04 18:43:51 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/04 18:43:51 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/04 18:43:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:40235 (size: 11.6 KiB, free: 912.3 MiB)
20/12/04 18:43:51 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:40235 (size: 28.7 KiB, free: 912.3 MiB)
20/12/04 18:43:52 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1542 ms on 192.168.1.9 (executor 2) (1/1)
20/12/04 18:43:52 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/04 18:43:52 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 36307
20/12/04 18:43:52 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/userFiles-fd9d4b08-9195-42cc-bc9f-a0c7bac66e1c/darima.zip/darima/dlsa.py:22) finished in 1.572 s
20/12/04 18:43:52 INFO DAGScheduler: looking for newly runnable stages
20/12/04 18:43:52 INFO DAGScheduler: running: Set()
20/12/04 18:43:52 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/04 18:43:52 INFO DAGScheduler: failed: Set()
20/12/04 18:43:52 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/userFiles-fd9d4b08-9195-42cc-bc9f-a0c7bac66e1c/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/04 18:43:52 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/04 18:43:52 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/04 18:43:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:40087 (size: 131.5 KiB, free: 5.8 GiB)
20/12/04 18:43:52 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/04 18:43:52 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/userFiles-fd9d4b08-9195-42cc-bc9f-a0c7bac66e1c/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/04 18:43:52 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/04 18:43:52 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/04 18:43:52 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/04 18:43:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:40235 (size: 131.5 KiB, free: 912.1 MiB)
20/12/04 18:43:53 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:47148
20/12/04 18:43:53 INFO SparkContext: Invoking stop() from shutdown hook
20/12/04 18:43:53 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/04 18:43:53 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/userFiles-fd9d4b08-9195-42cc-bc9f-a0c7bac66e1c/darima.zip/darima/dlsa.py:22, took 2.336626 s
20/12/04 18:43:53 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/userFiles-fd9d4b08-9195-42cc-bc9f-a0c7bac66e1c/darima.zip/darima/dlsa.py:22) failed in 0.708 s due to Stage cancelled because SparkContext was shut down
20/12/04 18:43:53 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/04 18:43:53 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/04 18:43:53 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/04 18:43:53 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/04 18:43:53 ERROR TransportRequestHandler: Error sending result RpcResponse[requestId=5976615097972622444,body=NioManagedBuffer[buf=java.nio.HeapByteBuffer[pos=0 lim=13 cap=13]]] to /192.168.1.9:50910; closing connection
java.io.IOException: Broken pipe
	at java.base/sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:113)
	at java.base/sun.nio.ch.IOUtil.write(IOUtil.java:79)
	at java.base/sun.nio.ch.IOUtil.write(IOUtil.java:50)
	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466)
	at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:148)
	at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:123)
	at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:362)
	at io.netty.channel.nio.AbstractNioByteChannel.doWriteInternal(AbstractNioByteChannel.java:235)
	at io.netty.channel.nio.AbstractNioByteChannel.doWrite0(AbstractNioByteChannel.java:209)
	at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:400)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:930)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:354)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:897)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1372)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:750)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:742)
	at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:728)
	at io.netty.channel.ChannelDuplexHandler.flush(ChannelDuplexHandler.java:127)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:750)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:765)
	at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:790)
	at io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:758)
	at io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:808)
	at io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1025)
	at io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:294)
	at org.apache.spark.network.server.TransportRequestHandler.respond(TransportRequestHandler.java:267)
	at org.apache.spark.network.server.TransportRequestHandler.access$000(TransportRequestHandler.java:46)
	at org.apache.spark.network.server.TransportRequestHandler$1.onSuccess(TransportRequestHandler.java:162)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:68)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:834)
20/12/04 18:43:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/04 18:43:53 INFO MemoryStore: MemoryStore cleared
20/12/04 18:43:53 INFO BlockManager: BlockManager stopped
20/12/04 18:43:53 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/04 18:43:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/04 18:43:53 INFO SparkContext: Successfully stopped SparkContext
20/12/04 18:43:53 INFO ShutdownHookManager: Shutdown hook called
20/12/04 18:43:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af
20/12/04 18:43:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-87533f5e-7c50-4653-b39c-cfac533504af/pyspark-25bee4c4-f5fc-47a3-8756-28df7faecfe7
20/12/04 18:43:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-f5cf9b24-7f51-4c66-bf64-3c4812198d80
