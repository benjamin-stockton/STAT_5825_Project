20/12/03 13:30:28 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 13:30:28 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 13:30:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 13:30:28 INFO MemoryStore: MemoryStore cleared
20/12/03 13:30:28 INFO BlockManager: BlockManager stopped
20/12/03 13:30:28 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 13:30:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 13:30:29 INFO SparkContext: Successfully stopped SparkContext
20/12/03 13:30:29 INFO ShutdownHookManager: Shutdown hook called
20/12/03 13:30:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-2294cf6a-7438-4d18-b372-36c6352e546d
20/12/03 13:30:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-0468d0d5-b4f7-47e2-96e5-ac8f23a0e8bc/pyspark-6e09b12e-3611-49c1-995c-8c77dc5a8cc0
20/12/03 13:30:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-0468d0d5-b4f7-47e2-96e5-ac8f23a0e8bc
20/12/03 13:34:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 13:34:34 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:34:34 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:34:34 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:34:34 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:34:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:34:35 INFO SparkContext: Running Spark version 3.0.1
20/12/03 13:34:35 INFO ResourceUtils: ==============================================================
20/12/03 13:34:35 INFO ResourceUtils: Resources for spark.driver:

20/12/03 13:34:35 INFO ResourceUtils: ==============================================================
20/12/03 13:34:35 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 13:34:35 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:34:35 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:34:35 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:34:35 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:34:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:34:36 INFO Utils: Successfully started service 'sparkDriver' on port 40379.
20/12/03 13:34:36 INFO SparkEnv: Registering MapOutputTracker
20/12/03 13:34:36 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 13:34:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 13:34:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 13:34:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 13:34:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-60daed91-2852-46d7-b4d1-1384794e640c
20/12/03 13:34:36 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 13:34:36 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 13:34:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 13:34:36 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 13:34:37 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 13:34:37 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 13:34:38 INFO Configuration: resource-types.xml not found
20/12/03 13:34:38 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 13:34:38 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 13:34:38 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 13:34:38 INFO Client: Setting up container launch context for our AM
20/12/03 13:34:38 INFO Client: Setting up the launch environment for our AM container
20/12/03 13:34:38 INFO Client: Preparing resources for our AM container
20/12/03 13:34:38 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 13:34:40 INFO Client: Uploading resource file:/tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/__spark_libs__13100485411767732217.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0017/__spark_libs__13100485411767732217.zip
20/12/03 13:34:42 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0017/pyspark.zip
20/12/03 13:34:42 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0017/py4j-0.10.9-src.zip
20/12/03 13:34:42 INFO Client: Uploading resource file:/tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/__spark_conf__8400844331802616460.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0017/__spark_conf__.zip
20/12/03 13:34:42 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:34:42 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:34:42 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:34:42 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:34:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:34:42 INFO Client: Submitting application application_1607015337794_0017 to ResourceManager
20/12/03 13:34:42 INFO YarnClientImpl: Submitted application application_1607015337794_0017
20/12/03 13:34:43 INFO Client: Application report for application_1607015337794_0017 (state: ACCEPTED)
20/12/03 13:34:43 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607020482811
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0017/
	 user: bsuconn
20/12/03 13:34:44 INFO Client: Application report for application_1607015337794_0017 (state: ACCEPTED)
20/12/03 13:34:45 INFO Client: Application report for application_1607015337794_0017 (state: ACCEPTED)
20/12/03 13:34:46 INFO Client: Application report for application_1607015337794_0017 (state: ACCEPTED)
20/12/03 13:34:47 INFO Client: Application report for application_1607015337794_0017 (state: ACCEPTED)
20/12/03 13:34:48 INFO Client: Application report for application_1607015337794_0017 (state: ACCEPTED)
20/12/03 13:34:49 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0017), /proxy/application_1607015337794_0017
20/12/03 13:34:49 INFO Client: Application report for application_1607015337794_0017 (state: RUNNING)
20/12/03 13:34:49 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607020482811
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0017/
	 user: bsuconn
20/12/03 13:34:49 INFO YarnClientSchedulerBackend: Application application_1607015337794_0017 has started running.
20/12/03 13:34:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45721.
20/12/03 13:34:49 INFO NettyBlockTransferService: Server created on 192.168.1.9:45721
20/12/03 13:34:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 13:34:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 45721, None)
20/12/03 13:34:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:45721 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 45721, None)
20/12/03 13:34:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 45721, None)
20/12/03 13:34:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 45721, None)
20/12/03 13:34:50 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:34:51 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 13:34:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 13:34:55 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:57452) with ID 1
20/12/03 13:34:55 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/03 13:34:56 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:46473 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 46473, None)
20/12/03 13:34:56 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 13:34:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 13:34:56 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 13:34:56 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:34:56 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:34:56 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:34:56 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:34:56 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:34:57 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:40379/files/darima.zip with timestamp 1607020497406
20/12/03 13:34:57 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/userFiles-e239d264-7896-47b2-a692-560e64fc4c80/darima.zip
20/12/03 13:34:59 INFO InMemoryFileIndex: It took 87 ms to list leaf files for 1 paths.
20/12/03 13:35:02 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:35:02 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:35:02 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:35:02 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:35:03 INFO CodeGenerator: Code generated in 338.45295 ms
20/12/03 13:35:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.3 KiB, free 5.8 GiB)
20/12/03 13:35:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 13:35:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:45721 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:35:03 INFO SparkContext: Created broadcast 0 from head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108
20/12/03 13:35:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:35:03 INFO SparkContext: Starting job: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108
20/12/03 13:35:03 INFO DAGScheduler: Got job 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) with 1 output partitions
20/12/03 13:35:03 INFO DAGScheduler: Final stage: ResultStage 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108)
20/12/03 13:35:03 INFO DAGScheduler: Parents of final stage: List()
20/12/03 13:35:03 INFO DAGScheduler: Missing parents: List()
20/12/03 13:35:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108), which has no missing parents
20/12/03 13:35:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.6 KiB, free 5.8 GiB)
20/12/03 13:35:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 5.8 GiB)
20/12/03 13:35:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:45721 (size: 6.3 KiB, free: 5.8 GiB)
20/12/03 13:35:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 13:35:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) (first 15 tasks are for partitions Vector(0))
20/12/03 13:35:03 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 13:35:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7790 bytes)
20/12/03 13:35:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:46473 (size: 6.3 KiB, free: 912.3 MiB)
20/12/03 13:35:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:46473 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 13:35:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3805 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:35:07 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/03 13:35:07 INFO DAGScheduler: ResultStage 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) finished in 4.031 s
20/12/03 13:35:07 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:35:07 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
20/12/03 13:35:07 INFO DAGScheduler: Job 0 finished: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108, took 4.113071 s
20/12/03 13:35:07 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
20/12/03 13:35:07 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:35:07 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:35:07 INFO FileSourceStrategy: Post-Scan Filters: 
20/12/03 13:35:07 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:35:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.3 KiB, free 5.8 GiB)
20/12/03 13:35:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 13:35:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:45721 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:35:07 INFO SparkContext: Created broadcast 2 from head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112
20/12/03 13:35:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:35:07 INFO SparkContext: Starting job: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112
20/12/03 13:35:07 INFO DAGScheduler: Got job 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) with 1 output partitions
20/12/03 13:35:07 INFO DAGScheduler: Final stage: ResultStage 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112)
20/12/03 13:35:07 INFO DAGScheduler: Parents of final stage: List()
20/12/03 13:35:07 INFO DAGScheduler: Missing parents: List()
20/12/03 13:35:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112), which has no missing parents
20/12/03 13:35:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.9 KiB, free 5.8 GiB)
20/12/03 13:35:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 5.8 GiB)
20/12/03 13:35:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:45721 (size: 4.9 KiB, free: 5.8 GiB)
20/12/03 13:35:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223
20/12/03 13:35:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) (first 15 tasks are for partitions Vector(0))
20/12/03 13:35:07 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/03 13:35:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7789 bytes)
20/12/03 13:35:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:46473 (size: 4.9 KiB, free: 912.3 MiB)
20/12/03 13:35:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:46473 (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 13:35:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 225 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:35:08 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/03 13:35:08 INFO DAGScheduler: ResultStage 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) finished in 0.270 s
20/12/03 13:35:08 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:35:08 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/03 13:35:08 INFO DAGScheduler: Job 1 finished: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112, took 0.287489 s
20/12/03 13:35:08 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:35:08 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:35:08 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:35:08 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:35:08 INFO CodeGenerator: Code generated in 39.924141 ms
20/12/03 13:35:08 INFO CodeGenerator: Code generated in 28.42435 ms
20/12/03 13:35:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.3 KiB, free 5.8 GiB)
20/12/03 13:35:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 13:35:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:45721 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:35:08 INFO SparkContext: Created broadcast 4 from count at NativeMethodAccessorImpl.java:0
20/12/03 13:35:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:35:08 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 13:35:08 INFO DAGScheduler: Registering RDD 10 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 13:35:08 INFO DAGScheduler: Got job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 13:35:08 INFO DAGScheduler: Final stage: ResultStage 3 (count at NativeMethodAccessorImpl.java:0)
20/12/03 13:35:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/03 13:35:08 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/03 13:35:08 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 13:35:08 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 13:35:08 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 13:35:08 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:45721 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 13:35:08 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/03 13:35:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 13:35:08 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/03 13:35:08 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 13:35:08 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:46473 (size: 8.0 KiB, free: 912.2 MiB)
20/12/03 13:35:09 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:46473 (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 13:35:10 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1334 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:35:10 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/03 13:35:10 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 1.376 s
20/12/03 13:35:10 INFO DAGScheduler: looking for newly runnable stages
20/12/03 13:35:10 INFO DAGScheduler: running: Set()
20/12/03 13:35:10 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/03 13:35:10 INFO DAGScheduler: failed: Set()
20/12/03 13:35:10 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 13:35:10 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/03 13:35:10 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/03 13:35:10 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:45721 (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 13:35:10 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1223
20/12/03 13:35:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 13:35:10 INFO YarnScheduler: Adding task set 3.0 with 1 tasks
20/12/03 13:35:10 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:35:10 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:46473 (size: 5.0 KiB, free: 912.2 MiB)
20/12/03 13:35:10 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:57452
20/12/03 13:35:10 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 327 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:35:10 INFO DAGScheduler: ResultStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.351 s
20/12/03 13:35:10 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:35:10 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/03 13:35:10 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
20/12/03 13:35:10 INFO DAGScheduler: Job 2 finished: count at NativeMethodAccessorImpl.java:0, took 1.785713 s
20/12/03 13:35:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:45721 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:35:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:46473 in memory (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 13:35:12 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.9:46473 in memory (size: 4.9 KiB, free: 912.2 MiB)
20/12/03 13:35:12 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.9:45721 in memory (size: 4.9 KiB, free: 5.8 GiB)
20/12/03 13:35:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:46473 in memory (size: 6.3 KiB, free: 912.2 MiB)
20/12/03 13:35:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:45721 in memory (size: 6.3 KiB, free: 5.8 GiB)
20/12/03 13:35:12 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.9:46473 in memory (size: 5.0 KiB, free: 912.2 MiB)
20/12/03 13:35:12 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.9:45721 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 13:35:12 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:45721 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:35:12 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:46473 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 13:35:12 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.9:46473 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 13:35:12 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.9:45721 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 13:35:12 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:45721 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:35:12 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:46473 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 13:35:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/03 13:35:14 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:35:14 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:35:14 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:35:14 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:35:15 INFO CodeGenerator: Code generated in 36.481952 ms
20/12/03 13:35:15 INFO CodeGenerator: Code generated in 16.664725 ms
20/12/03 13:35:15 INFO CodeGenerator: Code generated in 17.793511 ms
20/12/03 13:35:15 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 177.3 KiB, free 5.8 GiB)
20/12/03 13:35:15 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 13:35:15 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:45721 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:35:15 INFO SparkContext: Created broadcast 7 from toPandas at /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/userFiles-e239d264-7896-47b2-a692-560e64fc4c80/darima.zip/darima/dlsa.py:22
20/12/03 13:35:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:35:15 INFO SparkContext: Starting job: toPandas at /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/userFiles-e239d264-7896-47b2-a692-560e64fc4c80/darima.zip/darima/dlsa.py:22
20/12/03 13:35:15 INFO DAGScheduler: Registering RDD 20 (toPandas at /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/userFiles-e239d264-7896-47b2-a692-560e64fc4c80/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/03 13:35:15 INFO DAGScheduler: Got job 3 (toPandas at /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/userFiles-e239d264-7896-47b2-a692-560e64fc4c80/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/03 13:35:15 INFO DAGScheduler: Final stage: ResultStage 5 (toPandas at /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/userFiles-e239d264-7896-47b2-a692-560e64fc4c80/darima.zip/darima/dlsa.py:22)
20/12/03 13:35:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
20/12/03 13:35:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)
20/12/03 13:35:15 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at toPandas at /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/userFiles-e239d264-7896-47b2-a692-560e64fc4c80/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 13:35:15 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/03 13:35:15 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/03 13:35:15 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:45721 (size: 11.6 KiB, free: 5.8 GiB)
20/12/03 13:35:15 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1223
20/12/03 13:35:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at toPandas at /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/userFiles-e239d264-7896-47b2-a692-560e64fc4c80/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/03 13:35:15 INFO YarnScheduler: Adding task set 4.0 with 1 tasks
20/12/03 13:35:15 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 13:35:15 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:46473 (size: 11.6 KiB, free: 912.3 MiB)
20/12/03 13:35:16 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:46473 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 13:35:17 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1989 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:35:17 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/12/03 13:35:17 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 51591
20/12/03 13:35:17 INFO DAGScheduler: ShuffleMapStage 4 (toPandas at /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/userFiles-e239d264-7896-47b2-a692-560e64fc4c80/darima.zip/darima/dlsa.py:22) finished in 2.023 s
20/12/03 13:35:17 INFO DAGScheduler: looking for newly runnable stages
20/12/03 13:35:17 INFO DAGScheduler: running: Set()
20/12/03 13:35:17 INFO DAGScheduler: waiting: Set(ResultStage 5)
20/12/03 13:35:17 INFO DAGScheduler: failed: Set()
20/12/03 13:35:17 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at toPandas at /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/userFiles-e239d264-7896-47b2-a692-560e64fc4c80/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 13:35:17 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/03 13:35:17 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 131.4 KiB, free 5.8 GiB)
20/12/03 13:35:17 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:45721 (size: 131.4 KiB, free: 5.8 GiB)
20/12/03 13:35:17 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1223
20/12/03 13:35:17 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at toPandas at /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/userFiles-e239d264-7896-47b2-a692-560e64fc4c80/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/03 13:35:17 INFO YarnScheduler: Adding task set 5.0 with 200 tasks
20/12/03 13:35:17 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:35:17 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:46473 (size: 131.4 KiB, free: 912.1 MiB)
20/12/03 13:35:18 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:57452
20/12/03 13:35:22 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:35:22 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/03 13:35:23 INFO TaskSetManager: Starting task 0.1 in stage 5.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:35:23 INFO TaskSetManager: Lost task 2.0 in stage 5.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 1]
20/12/03 13:35:23 INFO TaskSetManager: Lost task 0.1 in stage 5.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 2]
20/12/03 13:35:23 INFO TaskSetManager: Starting task 0.2 in stage 5.0 (TID 8, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:35:24 INFO TaskSetManager: Starting task 2.1 in stage 5.0 (TID 9, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:35:24 INFO TaskSetManager: Lost task 0.2 in stage 5.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 3]
20/12/03 13:35:24 INFO TaskSetManager: Starting task 0.3 in stage 5.0 (TID 10, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:35:24 INFO TaskSetManager: Lost task 2.1 in stage 5.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 4]
20/12/03 13:35:25 INFO TaskSetManager: Starting task 2.2 in stage 5.0 (TID 11, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:35:25 INFO TaskSetManager: Lost task 0.3 in stage 5.0 (TID 10) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 5]
20/12/03 13:35:25 ERROR TaskSetManager: Task 0 in stage 5.0 failed 4 times; aborting job
20/12/03 13:35:25 INFO YarnScheduler: Cancelling stage 5
20/12/03 13:35:25 INFO YarnScheduler: Killing all running tasks in stage 5: Stage cancelled
20/12/03 13:35:25 INFO YarnScheduler: Stage 5 was cancelled
20/12/03 13:35:25 INFO DAGScheduler: ResultStage 5 (toPandas at /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/userFiles-e239d264-7896-47b2-a692-560e64fc4c80/darima.zip/darima/dlsa.py:22) failed in 8.158 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 10, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/03 13:35:25 INFO DAGScheduler: Job 3 failed: toPandas at /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/userFiles-e239d264-7896-47b2-a692-560e64fc4c80/darima.zip/darima/dlsa.py:22, took 10.262449 s
20/12/03 13:35:25 WARN TaskSetManager: Lost task 2.2 in stage 5.0 (TID 11, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/03 13:35:25 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/12/03 13:35:26 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 13:35:26 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 13:35:26 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 13:35:26 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 13:35:26 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 13:35:26 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 13:35:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 13:35:26 INFO MemoryStore: MemoryStore cleared
20/12/03 13:35:26 INFO BlockManager: BlockManager stopped
20/12/03 13:35:26 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 13:35:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 13:35:26 INFO SparkContext: Successfully stopped SparkContext
20/12/03 13:35:26 INFO ShutdownHookManager: Shutdown hook called
20/12/03 13:35:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405
20/12/03 13:35:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-748fba27-77b9-478e-a766-26e02aa069d3
20/12/03 13:35:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-3dc2cf07-a680-45cc-bb9e-bf858716b405/pyspark-874bf89d-68df-4b75-a607-b47d02d5007c
20/12/03 13:35:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 13:35:30 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:35:30 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:35:30 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:35:30 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:35:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:35:31 INFO SparkContext: Running Spark version 3.0.1
20/12/03 13:35:31 INFO ResourceUtils: ==============================================================
20/12/03 13:35:31 INFO ResourceUtils: Resources for spark.driver:

20/12/03 13:35:31 INFO ResourceUtils: ==============================================================
20/12/03 13:35:31 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 13:35:31 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:35:31 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:35:31 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:35:31 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:35:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:35:31 INFO Utils: Successfully started service 'sparkDriver' on port 45873.
20/12/03 13:35:31 INFO SparkEnv: Registering MapOutputTracker
20/12/03 13:35:32 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 13:35:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 13:35:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 13:35:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 13:35:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8139d415-d1b7-4ac6-a4e1-29e22827b6ee
20/12/03 13:35:32 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 13:35:32 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 13:35:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 13:35:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 13:35:33 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 13:35:33 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 13:35:34 INFO Configuration: resource-types.xml not found
20/12/03 13:35:34 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 13:35:34 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 13:35:34 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 13:35:34 INFO Client: Setting up container launch context for our AM
20/12/03 13:35:34 INFO Client: Setting up the launch environment for our AM container
20/12/03 13:35:34 INFO Client: Preparing resources for our AM container
20/12/03 13:35:34 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 13:35:36 INFO Client: Uploading resource file:/tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/__spark_libs__7985702336530298802.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0018/__spark_libs__7985702336530298802.zip
20/12/03 13:35:37 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0018/pyspark.zip
20/12/03 13:35:37 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0018/py4j-0.10.9-src.zip
20/12/03 13:35:38 INFO Client: Uploading resource file:/tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/__spark_conf__12374126706047901975.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0018/__spark_conf__.zip
20/12/03 13:35:38 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:35:38 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:35:38 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:35:38 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:35:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:35:38 INFO Client: Submitting application application_1607015337794_0018 to ResourceManager
20/12/03 13:35:38 INFO YarnClientImpl: Submitted application application_1607015337794_0018
20/12/03 13:35:39 INFO Client: Application report for application_1607015337794_0018 (state: ACCEPTED)
20/12/03 13:35:39 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607020538927
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0018/
	 user: bsuconn
20/12/03 13:35:40 INFO Client: Application report for application_1607015337794_0018 (state: ACCEPTED)
20/12/03 13:35:41 INFO Client: Application report for application_1607015337794_0018 (state: ACCEPTED)
20/12/03 13:35:42 INFO Client: Application report for application_1607015337794_0018 (state: ACCEPTED)
20/12/03 13:35:43 INFO Client: Application report for application_1607015337794_0018 (state: ACCEPTED)
20/12/03 13:35:44 INFO Client: Application report for application_1607015337794_0018 (state: ACCEPTED)
20/12/03 13:35:45 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0018), /proxy/application_1607015337794_0018
20/12/03 13:35:45 INFO Client: Application report for application_1607015337794_0018 (state: RUNNING)
20/12/03 13:35:45 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607020538927
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0018/
	 user: bsuconn
20/12/03 13:35:45 INFO YarnClientSchedulerBackend: Application application_1607015337794_0018 has started running.
20/12/03 13:35:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38147.
20/12/03 13:35:46 INFO NettyBlockTransferService: Server created on 192.168.1.9:38147
20/12/03 13:35:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 13:35:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 38147, None)
20/12/03 13:35:46 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:38147 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 38147, None)
20/12/03 13:35:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 38147, None)
20/12/03 13:35:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 38147, None)
20/12/03 13:35:46 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:35:47 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 13:35:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 13:35:55 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:37954) with ID 1
20/12/03 13:35:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:36287 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 36287, None)
20/12/03 13:35:56 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:37958) with ID 2
20/12/03 13:35:56 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/03 13:35:56 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:43481 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 43481, None)
20/12/03 13:35:56 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 13:35:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 13:35:57 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 13:35:57 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:35:57 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:35:57 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:35:57 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:35:57 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:35:57 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:45873/files/darima.zip with timestamp 1607020557979
20/12/03 13:35:57 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/userFiles-5a0157cb-1d18-44b3-b5ab-33109af319be/darima.zip
20/12/03 13:36:00 INFO InMemoryFileIndex: It took 121 ms to list leaf files for 1 paths.
20/12/03 13:36:03 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:36:03 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:36:03 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:36:03 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:36:04 INFO CodeGenerator: Code generated in 521.024664 ms
20/12/03 13:36:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.3 KiB, free 5.8 GiB)
20/12/03 13:36:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.6 KiB, free 5.8 GiB)
20/12/03 13:36:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:38147 (size: 28.6 KiB, free: 5.8 GiB)
20/12/03 13:36:04 INFO SparkContext: Created broadcast 0 from head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108
20/12/03 13:36:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:36:05 INFO SparkContext: Starting job: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108
20/12/03 13:36:05 INFO DAGScheduler: Got job 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) with 1 output partitions
20/12/03 13:36:05 INFO DAGScheduler: Final stage: ResultStage 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108)
20/12/03 13:36:05 INFO DAGScheduler: Parents of final stage: List()
20/12/03 13:36:05 INFO DAGScheduler: Missing parents: List()
20/12/03 13:36:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108), which has no missing parents
20/12/03 13:36:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.6 KiB, free 5.8 GiB)
20/12/03 13:36:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 5.8 GiB)
20/12/03 13:36:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:38147 (size: 6.3 KiB, free: 5.8 GiB)
20/12/03 13:36:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 13:36:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) (first 15 tasks are for partitions Vector(0))
20/12/03 13:36:05 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 13:36:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7790 bytes)
20/12/03 13:36:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:43481 (size: 6.3 KiB, free: 912.3 MiB)
20/12/03 13:36:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:43481 (size: 28.6 KiB, free: 912.3 MiB)
20/12/03 13:36:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4456 ms on 192.168.1.9 (executor 2) (1/1)
20/12/03 13:36:10 INFO DAGScheduler: ResultStage 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) finished in 4.674 s
20/12/03 13:36:10 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/03 13:36:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:36:10 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
20/12/03 13:36:10 INFO DAGScheduler: Job 0 finished: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108, took 4.874157 s
20/12/03 13:36:10 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
20/12/03 13:36:10 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:36:10 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:36:10 INFO FileSourceStrategy: Post-Scan Filters: 
20/12/03 13:36:10 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:36:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.3 KiB, free 5.8 GiB)
20/12/03 13:36:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.6 KiB, free 5.8 GiB)
20/12/03 13:36:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:38147 (size: 28.6 KiB, free: 5.8 GiB)
20/12/03 13:36:10 INFO SparkContext: Created broadcast 2 from head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112
20/12/03 13:36:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:36:10 INFO SparkContext: Starting job: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112
20/12/03 13:36:10 INFO DAGScheduler: Got job 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) with 1 output partitions
20/12/03 13:36:10 INFO DAGScheduler: Final stage: ResultStage 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112)
20/12/03 13:36:10 INFO DAGScheduler: Parents of final stage: List()
20/12/03 13:36:10 INFO DAGScheduler: Missing parents: List()
20/12/03 13:36:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112), which has no missing parents
20/12/03 13:36:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.9 KiB, free 5.8 GiB)
20/12/03 13:36:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 5.8 GiB)
20/12/03 13:36:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:38147 (size: 4.9 KiB, free: 5.8 GiB)
20/12/03 13:36:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223
20/12/03 13:36:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) (first 15 tasks are for partitions Vector(0))
20/12/03 13:36:10 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/03 13:36:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7789 bytes)
20/12/03 13:36:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:36287 (size: 4.9 KiB, free: 912.3 MiB)
20/12/03 13:36:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:36287 (size: 28.6 KiB, free: 912.3 MiB)
20/12/03 13:36:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4286 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:36:14 INFO DAGScheduler: ResultStage 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) finished in 4.310 s
20/12/03 13:36:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:36:14 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/03 13:36:14 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/03 13:36:14 INFO DAGScheduler: Job 1 finished: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112, took 4.318351 s
20/12/03 13:36:14 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:36:14 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:36:14 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:36:14 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:36:14 INFO CodeGenerator: Code generated in 30.989214 ms
20/12/03 13:36:14 INFO CodeGenerator: Code generated in 33.856802 ms
20/12/03 13:36:14 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.3 KiB, free 5.8 GiB)
20/12/03 13:36:14 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.6 KiB, free 5.8 GiB)
20/12/03 13:36:14 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:38147 (size: 28.6 KiB, free: 5.8 GiB)
20/12/03 13:36:14 INFO SparkContext: Created broadcast 4 from count at NativeMethodAccessorImpl.java:0
20/12/03 13:36:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:36:15 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 13:36:15 INFO DAGScheduler: Registering RDD 10 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 13:36:15 INFO DAGScheduler: Got job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 13:36:15 INFO DAGScheduler: Final stage: ResultStage 3 (count at NativeMethodAccessorImpl.java:0)
20/12/03 13:36:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/03 13:36:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/03 13:36:15 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 13:36:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 13:36:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 13:36:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:38147 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 13:36:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/03 13:36:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 13:36:15 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/03 13:36:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 13:36:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:36287 (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 13:36:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:36287 (size: 28.6 KiB, free: 912.2 MiB)
20/12/03 13:36:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1004 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:36:16 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 1.051 s
20/12/03 13:36:16 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/03 13:36:16 INFO DAGScheduler: looking for newly runnable stages
20/12/03 13:36:16 INFO DAGScheduler: running: Set()
20/12/03 13:36:16 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/03 13:36:16 INFO DAGScheduler: failed: Set()
20/12/03 13:36:16 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 13:36:16 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/03 13:36:16 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/03 13:36:16 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:38147 (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 13:36:16 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1223
20/12/03 13:36:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 13:36:16 INFO YarnScheduler: Adding task set 3.0 with 1 tasks
20/12/03 13:36:16 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:36:16 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:36287 (size: 5.0 KiB, free: 912.2 MiB)
20/12/03 13:36:16 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:37954
20/12/03 13:36:16 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 219 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:36:16 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/03 13:36:16 INFO DAGScheduler: ResultStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.239 s
20/12/03 13:36:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:36:16 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
20/12/03 13:36:16 INFO DAGScheduler: Job 2 finished: count at NativeMethodAccessorImpl.java:0, took 1.353193 s
20/12/03 13:36:18 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.9:38147 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 13:36:18 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.9:36287 in memory (size: 8.0 KiB, free: 912.2 MiB)
20/12/03 13:36:18 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.9:36287 in memory (size: 4.9 KiB, free: 912.2 MiB)
20/12/03 13:36:18 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.9:38147 in memory (size: 4.9 KiB, free: 5.8 GiB)
20/12/03 13:36:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:38147 in memory (size: 6.3 KiB, free: 5.8 GiB)
20/12/03 13:36:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:43481 in memory (size: 6.3 KiB, free: 912.3 MiB)
20/12/03 13:36:18 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.9:36287 in memory (size: 5.0 KiB, free: 912.2 MiB)
20/12/03 13:36:18 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.9:38147 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 13:36:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:38147 in memory (size: 28.6 KiB, free: 5.8 GiB)
20/12/03 13:36:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:43481 in memory (size: 28.6 KiB, free: 912.3 MiB)
20/12/03 13:36:18 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:38147 in memory (size: 28.6 KiB, free: 5.8 GiB)
20/12/03 13:36:18 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:36287 in memory (size: 28.6 KiB, free: 912.3 MiB)
20/12/03 13:36:19 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/03 13:36:20 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:36:20 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:36:20 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:36:20 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:36:21 INFO CodeGenerator: Code generated in 32.506251 ms
20/12/03 13:36:21 INFO CodeGenerator: Code generated in 26.228248 ms
20/12/03 13:36:21 INFO CodeGenerator: Code generated in 27.406528 ms
20/12/03 13:36:21 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 177.3 KiB, free 5.8 GiB)
20/12/03 13:36:21 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 28.6 KiB, free 5.8 GiB)
20/12/03 13:36:21 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:38147 (size: 28.6 KiB, free: 5.8 GiB)
20/12/03 13:36:21 INFO SparkContext: Created broadcast 7 from toPandas at /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/userFiles-5a0157cb-1d18-44b3-b5ab-33109af319be/darima.zip/darima/dlsa.py:22
20/12/03 13:36:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:36:21 INFO SparkContext: Starting job: toPandas at /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/userFiles-5a0157cb-1d18-44b3-b5ab-33109af319be/darima.zip/darima/dlsa.py:22
20/12/03 13:36:21 INFO DAGScheduler: Registering RDD 20 (toPandas at /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/userFiles-5a0157cb-1d18-44b3-b5ab-33109af319be/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/03 13:36:21 INFO DAGScheduler: Got job 3 (toPandas at /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/userFiles-5a0157cb-1d18-44b3-b5ab-33109af319be/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/03 13:36:21 INFO DAGScheduler: Final stage: ResultStage 5 (toPandas at /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/userFiles-5a0157cb-1d18-44b3-b5ab-33109af319be/darima.zip/darima/dlsa.py:22)
20/12/03 13:36:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
20/12/03 13:36:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)
20/12/03 13:36:21 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at toPandas at /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/userFiles-5a0157cb-1d18-44b3-b5ab-33109af319be/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 13:36:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.2 KiB, free 5.8 GiB)
20/12/03 13:36:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/03 13:36:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:38147 (size: 11.6 KiB, free: 5.8 GiB)
20/12/03 13:36:21 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1223
20/12/03 13:36:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at toPandas at /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/userFiles-5a0157cb-1d18-44b3-b5ab-33109af319be/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/03 13:36:21 INFO YarnScheduler: Adding task set 4.0 with 1 tasks
20/12/03 13:36:21 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 13:36:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:43481 (size: 11.6 KiB, free: 912.3 MiB)
20/12/03 13:36:22 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:43481 (size: 28.6 KiB, free: 912.3 MiB)
20/12/03 13:36:23 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 2202 ms on 192.168.1.9 (executor 2) (1/1)
20/12/03 13:36:23 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/12/03 13:36:23 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 35405
20/12/03 13:36:23 INFO DAGScheduler: ShuffleMapStage 4 (toPandas at /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/userFiles-5a0157cb-1d18-44b3-b5ab-33109af319be/darima.zip/darima/dlsa.py:22) finished in 2.232 s
20/12/03 13:36:23 INFO DAGScheduler: looking for newly runnable stages
20/12/03 13:36:23 INFO DAGScheduler: running: Set()
20/12/03 13:36:23 INFO DAGScheduler: waiting: Set(ResultStage 5)
20/12/03 13:36:23 INFO DAGScheduler: failed: Set()
20/12/03 13:36:23 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at toPandas at /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/userFiles-5a0157cb-1d18-44b3-b5ab-33109af319be/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 13:36:23 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 428.0 KiB, free 5.8 GiB)
20/12/03 13:36:23 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 131.4 KiB, free 5.8 GiB)
20/12/03 13:36:23 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:38147 (size: 131.4 KiB, free: 5.8 GiB)
20/12/03 13:36:23 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1223
20/12/03 13:36:23 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at toPandas at /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/userFiles-5a0157cb-1d18-44b3-b5ab-33109af319be/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/03 13:36:23 INFO YarnScheduler: Adding task set 5.0 with 200 tasks
20/12/03 13:36:23 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:36:23 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 6, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:36:23 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:43481 (size: 131.4 KiB, free: 912.1 MiB)
20/12/03 13:36:23 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:36287 (size: 131.4 KiB, free: 912.1 MiB)
20/12/03 13:36:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:37958
20/12/03 13:36:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:37954
20/12/03 13:36:31 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 7, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 13:36:31 WARN TaskSetManager: Lost task 2.0 in stage 5.0 (TID 6, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/03 13:36:33 INFO TaskSetManager: Starting task 2.1 in stage 5.0 (TID 8, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:36:34 INFO TaskSetManager: Lost task 3.0 in stage 5.0 (TID 7) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 1]
20/12/03 13:36:34 INFO TaskSetManager: Starting task 3.1 in stage 5.0 (TID 9, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 13:36:34 INFO TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 2]
20/12/03 13:36:34 INFO TaskSetManager: Starting task 0.1 in stage 5.0 (TID 10, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:36:34 INFO TaskSetManager: Lost task 2.1 in stage 5.0 (TID 8) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 3]
20/12/03 13:36:36 INFO TaskSetManager: Starting task 2.2 in stage 5.0 (TID 11, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:36:36 INFO TaskSetManager: Lost task 0.1 in stage 5.0 (TID 10) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 4]
20/12/03 13:36:36 INFO TaskSetManager: Starting task 0.2 in stage 5.0 (TID 12, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:36:36 INFO TaskSetManager: Lost task 3.1 in stage 5.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 5]
20/12/03 13:36:37 INFO TaskSetManager: Starting task 3.2 in stage 5.0 (TID 13, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 13:36:37 INFO TaskSetManager: Lost task 2.2 in stage 5.0 (TID 11) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 6]
20/12/03 13:36:37 INFO TaskSetManager: Starting task 2.3 in stage 5.0 (TID 14, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:36:37 INFO TaskSetManager: Lost task 0.2 in stage 5.0 (TID 12) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 7]
20/12/03 13:36:38 INFO TaskSetManager: Starting task 0.3 in stage 5.0 (TID 15, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:36:38 INFO TaskSetManager: Lost task 3.2 in stage 5.0 (TID 13) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 8]
20/12/03 13:36:38 INFO TaskSetManager: Starting task 3.3 in stage 5.0 (TID 16, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 13:36:38 INFO TaskSetManager: Lost task 2.3 in stage 5.0 (TID 14) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 9]
20/12/03 13:36:38 ERROR TaskSetManager: Task 2 in stage 5.0 failed 4 times; aborting job
20/12/03 13:36:38 INFO TaskSetManager: Lost task 0.3 in stage 5.0 (TID 15) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 10]
20/12/03 13:36:38 INFO YarnScheduler: Cancelling stage 5
20/12/03 13:36:38 INFO YarnScheduler: Killing all running tasks in stage 5: Stage cancelled
20/12/03 13:36:38 INFO YarnScheduler: Stage 5 was cancelled
20/12/03 13:36:38 INFO DAGScheduler: ResultStage 5 (toPandas at /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/userFiles-5a0157cb-1d18-44b3-b5ab-33109af319be/darima.zip/darima/dlsa.py:22) failed in 15.398 s due to Job aborted due to stage failure: Task 2 in stage 5.0 failed 4 times, most recent failure: Lost task 2.3 in stage 5.0 (TID 14, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/03 13:36:38 INFO DAGScheduler: Job 3 failed: toPandas at /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/userFiles-5a0157cb-1d18-44b3-b5ab-33109af319be/darima.zip/darima/dlsa.py:22, took 17.695965 s
20/12/03 13:36:39 WARN TaskSetManager: Lost task 3.3 in stage 5.0 (TID 16, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/03 13:36:39 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/12/03 13:36:39 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 13:36:39 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 13:36:39 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 13:36:39 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 13:36:39 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 13:36:39 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 13:36:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 13:36:39 INFO MemoryStore: MemoryStore cleared
20/12/03 13:36:39 INFO BlockManager: BlockManager stopped
20/12/03 13:36:39 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 13:36:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 13:36:39 INFO SparkContext: Successfully stopped SparkContext
20/12/03 13:36:39 INFO ShutdownHookManager: Shutdown hook called
20/12/03 13:36:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb
20/12/03 13:36:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-46ce8830-9f8c-41f8-8811-0aeeb05f40cb/pyspark-9ab41f63-954d-4a44-8454-49b3009fc0a8
20/12/03 13:36:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-f19967eb-8ad3-4b73-87ca-30faaa54d71d
20/12/03 13:36:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 13:36:43 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:36:43 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:36:43 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:36:43 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:36:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:36:45 INFO SparkContext: Running Spark version 3.0.1
20/12/03 13:36:45 INFO ResourceUtils: ==============================================================
20/12/03 13:36:45 INFO ResourceUtils: Resources for spark.driver:

20/12/03 13:36:45 INFO ResourceUtils: ==============================================================
20/12/03 13:36:45 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 13:36:45 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:36:45 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:36:45 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:36:45 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:36:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:36:45 INFO Utils: Successfully started service 'sparkDriver' on port 44791.
20/12/03 13:36:46 INFO SparkEnv: Registering MapOutputTracker
20/12/03 13:36:46 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 13:36:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 13:36:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 13:36:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 13:36:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b73e9008-c4da-403a-b0e9-2e8a43be7748
20/12/03 13:36:46 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 13:36:46 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 13:36:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 13:36:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 13:36:47 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 13:36:47 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 13:36:48 INFO Configuration: resource-types.xml not found
20/12/03 13:36:48 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 13:36:48 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 13:36:48 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 13:36:48 INFO Client: Setting up container launch context for our AM
20/12/03 13:36:48 INFO Client: Setting up the launch environment for our AM container
20/12/03 13:36:48 INFO Client: Preparing resources for our AM container
20/12/03 13:36:48 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 13:36:50 INFO Client: Uploading resource file:/tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/__spark_libs__11416470657685886353.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0019/__spark_libs__11416470657685886353.zip
20/12/03 13:36:51 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0019/pyspark.zip
20/12/03 13:36:51 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0019/py4j-0.10.9-src.zip
20/12/03 13:36:52 INFO Client: Uploading resource file:/tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/__spark_conf__3850301426468076110.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0019/__spark_conf__.zip
20/12/03 13:36:52 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:36:52 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:36:52 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:36:52 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:36:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:36:52 INFO Client: Submitting application application_1607015337794_0019 to ResourceManager
20/12/03 13:36:52 INFO YarnClientImpl: Submitted application application_1607015337794_0019
20/12/03 13:36:53 INFO Client: Application report for application_1607015337794_0019 (state: ACCEPTED)
20/12/03 13:36:53 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607020612577
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0019/
	 user: bsuconn
20/12/03 13:36:54 INFO Client: Application report for application_1607015337794_0019 (state: ACCEPTED)
20/12/03 13:36:55 INFO Client: Application report for application_1607015337794_0019 (state: ACCEPTED)
20/12/03 13:36:56 INFO Client: Application report for application_1607015337794_0019 (state: ACCEPTED)
20/12/03 13:36:57 INFO Client: Application report for application_1607015337794_0019 (state: ACCEPTED)
20/12/03 13:36:58 INFO Client: Application report for application_1607015337794_0019 (state: ACCEPTED)
20/12/03 13:36:59 INFO Client: Application report for application_1607015337794_0019 (state: RUNNING)
20/12/03 13:36:59 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607020612577
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0019/
	 user: bsuconn
20/12/03 13:36:59 INFO YarnClientSchedulerBackend: Application application_1607015337794_0019 has started running.
20/12/03 13:36:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46179.
20/12/03 13:36:59 INFO NettyBlockTransferService: Server created on 192.168.1.9:46179
20/12/03 13:36:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 13:36:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 46179, None)
20/12/03 13:36:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:46179 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 46179, None)
20/12/03 13:36:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 46179, None)
20/12/03 13:36:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 46179, None)
20/12/03 13:37:00 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0019), /proxy/application_1607015337794_0019
20/12/03 13:37:00 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:37:01 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 13:37:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 13:37:08 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:57216) with ID 1
20/12/03 13:37:09 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:32939 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 32939, None)
20/12/03 13:37:09 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:57220) with ID 2
20/12/03 13:37:10 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:37875 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 37875, None)
20/12/03 13:37:17 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
20/12/03 13:37:17 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 13:37:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 13:37:17 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 13:37:17 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:37:17 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:37:17 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:37:17 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:37:17 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:37:18 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:44791/files/darima.zip with timestamp 1607020638378
20/12/03 13:37:18 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/userFiles-f71f9c8f-7569-4614-9571-580cc7e374f7/darima.zip
20/12/03 13:37:20 INFO InMemoryFileIndex: It took 97 ms to list leaf files for 1 paths.
20/12/03 13:37:23 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:37:23 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:37:23 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:37:23 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:37:24 INFO CodeGenerator: Code generated in 425.015268 ms
20/12/03 13:37:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.3 KiB, free 5.8 GiB)
20/12/03 13:37:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.6 KiB, free 5.8 GiB)
20/12/03 13:37:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:46179 (size: 28.6 KiB, free: 5.8 GiB)
20/12/03 13:37:24 INFO SparkContext: Created broadcast 0 from head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108
20/12/03 13:37:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:37:24 INFO SparkContext: Starting job: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108
20/12/03 13:37:24 INFO DAGScheduler: Got job 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) with 1 output partitions
20/12/03 13:37:24 INFO DAGScheduler: Final stage: ResultStage 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108)
20/12/03 13:37:24 INFO DAGScheduler: Parents of final stage: List()
20/12/03 13:37:24 INFO DAGScheduler: Missing parents: List()
20/12/03 13:37:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108), which has no missing parents
20/12/03 13:37:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.6 KiB, free 5.8 GiB)
20/12/03 13:37:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 5.8 GiB)
20/12/03 13:37:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:46179 (size: 6.3 KiB, free: 5.8 GiB)
20/12/03 13:37:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 13:37:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) (first 15 tasks are for partitions Vector(0))
20/12/03 13:37:24 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 13:37:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7790 bytes)
20/12/03 13:37:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:32939 (size: 6.3 KiB, free: 912.3 MiB)
20/12/03 13:37:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:32939 (size: 28.6 KiB, free: 912.3 MiB)
20/12/03 13:37:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3843 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:37:28 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/03 13:37:28 INFO DAGScheduler: ResultStage 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) finished in 4.043 s
20/12/03 13:37:28 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:37:28 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
20/12/03 13:37:28 INFO DAGScheduler: Job 0 finished: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108, took 4.131203 s
20/12/03 13:37:28 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
20/12/03 13:37:28 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:37:28 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:37:28 INFO FileSourceStrategy: Post-Scan Filters: 
20/12/03 13:37:28 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:37:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.3 KiB, free 5.8 GiB)
20/12/03 13:37:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.6 KiB, free 5.8 GiB)
20/12/03 13:37:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:46179 (size: 28.6 KiB, free: 5.8 GiB)
20/12/03 13:37:28 INFO SparkContext: Created broadcast 2 from head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112
20/12/03 13:37:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:37:28 INFO SparkContext: Starting job: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112
20/12/03 13:37:28 INFO DAGScheduler: Got job 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) with 1 output partitions
20/12/03 13:37:28 INFO DAGScheduler: Final stage: ResultStage 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112)
20/12/03 13:37:28 INFO DAGScheduler: Parents of final stage: List()
20/12/03 13:37:28 INFO DAGScheduler: Missing parents: List()
20/12/03 13:37:28 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112), which has no missing parents
20/12/03 13:37:28 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.9 KiB, free 5.8 GiB)
20/12/03 13:37:28 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 5.8 GiB)
20/12/03 13:37:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:46179 (size: 4.9 KiB, free: 5.8 GiB)
20/12/03 13:37:28 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223
20/12/03 13:37:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) (first 15 tasks are for partitions Vector(0))
20/12/03 13:37:28 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/03 13:37:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7789 bytes)
20/12/03 13:37:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:37875 (size: 4.9 KiB, free: 912.3 MiB)
20/12/03 13:37:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:37875 (size: 28.6 KiB, free: 912.3 MiB)
20/12/03 13:37:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3721 ms on 192.168.1.9 (executor 2) (1/1)
20/12/03 13:37:32 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/03 13:37:32 INFO DAGScheduler: ResultStage 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) finished in 3.758 s
20/12/03 13:37:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:37:32 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/03 13:37:32 INFO DAGScheduler: Job 1 finished: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112, took 3.769070 s
20/12/03 13:37:32 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:37:32 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:37:32 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:37:32 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:37:32 INFO CodeGenerator: Code generated in 31.843921 ms
20/12/03 13:37:33 INFO CodeGenerator: Code generated in 31.551592 ms
20/12/03 13:37:33 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.3 KiB, free 5.8 GiB)
20/12/03 13:37:33 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.6 KiB, free 5.8 GiB)
20/12/03 13:37:33 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:46179 (size: 28.6 KiB, free: 5.8 GiB)
20/12/03 13:37:33 INFO SparkContext: Created broadcast 4 from count at NativeMethodAccessorImpl.java:0
20/12/03 13:37:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:37:33 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 13:37:33 INFO DAGScheduler: Registering RDD 10 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 13:37:33 INFO DAGScheduler: Got job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 13:37:33 INFO DAGScheduler: Final stage: ResultStage 3 (count at NativeMethodAccessorImpl.java:0)
20/12/03 13:37:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/03 13:37:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/03 13:37:33 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 13:37:33 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 13:37:33 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 13:37:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:46179 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 13:37:33 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/03 13:37:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 13:37:33 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/03 13:37:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 13:37:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:37875 (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 13:37:33 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:37875 (size: 28.6 KiB, free: 912.2 MiB)
20/12/03 13:37:34 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 936 ms on 192.168.1.9 (executor 2) (1/1)
20/12/03 13:37:34 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/03 13:37:34 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.975 s
20/12/03 13:37:34 INFO DAGScheduler: looking for newly runnable stages
20/12/03 13:37:34 INFO DAGScheduler: running: Set()
20/12/03 13:37:34 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/03 13:37:34 INFO DAGScheduler: failed: Set()
20/12/03 13:37:34 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 13:37:34 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/03 13:37:34 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/03 13:37:34 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:46179 (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 13:37:34 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1223
20/12/03 13:37:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 13:37:34 INFO YarnScheduler: Adding task set 3.0 with 1 tasks
20/12/03 13:37:34 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:37:34 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:37875 (size: 5.0 KiB, free: 912.2 MiB)
20/12/03 13:37:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:57220
20/12/03 13:37:34 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 255 ms on 192.168.1.9 (executor 2) (1/1)
20/12/03 13:37:34 INFO DAGScheduler: ResultStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.275 s
20/12/03 13:37:34 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:37:34 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/03 13:37:34 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
20/12/03 13:37:34 INFO DAGScheduler: Job 2 finished: count at NativeMethodAccessorImpl.java:0, took 1.287957 s
20/12/03 13:37:36 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.9:37875 in memory (size: 8.0 KiB, free: 912.2 MiB)
20/12/03 13:37:36 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.9:46179 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 13:37:36 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.9:46179 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 13:37:36 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.9:37875 in memory (size: 5.0 KiB, free: 912.2 MiB)
20/12/03 13:37:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:46179 in memory (size: 6.3 KiB, free: 5.8 GiB)
20/12/03 13:37:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:32939 in memory (size: 6.3 KiB, free: 912.3 MiB)
20/12/03 13:37:36 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.9:46179 in memory (size: 4.9 KiB, free: 5.8 GiB)
20/12/03 13:37:36 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.9:37875 in memory (size: 4.9 KiB, free: 912.2 MiB)
20/12/03 13:37:37 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:37875 in memory (size: 28.6 KiB, free: 912.3 MiB)
20/12/03 13:37:37 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:46179 in memory (size: 28.6 KiB, free: 5.8 GiB)
20/12/03 13:37:37 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:37875 in memory (size: 28.6 KiB, free: 912.3 MiB)
20/12/03 13:37:37 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.9:46179 in memory (size: 28.6 KiB, free: 5.8 GiB)
20/12/03 13:37:37 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:32939 in memory (size: 28.6 KiB, free: 912.3 MiB)
20/12/03 13:37:37 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:46179 in memory (size: 28.6 KiB, free: 5.8 GiB)
20/12/03 13:37:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/03 13:37:39 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:37:39 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:37:39 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:37:39 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:37:39 INFO CodeGenerator: Code generated in 58.454654 ms
20/12/03 13:37:39 INFO CodeGenerator: Code generated in 31.945302 ms
20/12/03 13:37:39 INFO CodeGenerator: Code generated in 28.528646 ms
20/12/03 13:37:39 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 177.3 KiB, free 5.8 GiB)
20/12/03 13:37:39 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 28.6 KiB, free 5.8 GiB)
20/12/03 13:37:39 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:46179 (size: 28.6 KiB, free: 5.8 GiB)
20/12/03 13:37:39 INFO SparkContext: Created broadcast 7 from toPandas at /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/userFiles-f71f9c8f-7569-4614-9571-580cc7e374f7/darima.zip/darima/dlsa.py:22
20/12/03 13:37:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:37:39 INFO SparkContext: Starting job: toPandas at /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/userFiles-f71f9c8f-7569-4614-9571-580cc7e374f7/darima.zip/darima/dlsa.py:22
20/12/03 13:37:39 INFO DAGScheduler: Registering RDD 20 (toPandas at /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/userFiles-f71f9c8f-7569-4614-9571-580cc7e374f7/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/03 13:37:39 INFO DAGScheduler: Got job 3 (toPandas at /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/userFiles-f71f9c8f-7569-4614-9571-580cc7e374f7/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/03 13:37:39 INFO DAGScheduler: Final stage: ResultStage 5 (toPandas at /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/userFiles-f71f9c8f-7569-4614-9571-580cc7e374f7/darima.zip/darima/dlsa.py:22)
20/12/03 13:37:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
20/12/03 13:37:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)
20/12/03 13:37:39 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at toPandas at /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/userFiles-f71f9c8f-7569-4614-9571-580cc7e374f7/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 13:37:39 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/03 13:37:39 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/03 13:37:39 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:46179 (size: 11.6 KiB, free: 5.8 GiB)
20/12/03 13:37:39 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1223
20/12/03 13:37:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at toPandas at /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/userFiles-f71f9c8f-7569-4614-9571-580cc7e374f7/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/03 13:37:39 INFO YarnScheduler: Adding task set 4.0 with 1 tasks
20/12/03 13:37:39 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 13:37:39 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:32939 (size: 11.6 KiB, free: 912.3 MiB)
20/12/03 13:37:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:32939 (size: 28.6 KiB, free: 912.3 MiB)
20/12/03 13:37:41 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1906 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:37:41 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/12/03 13:37:41 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 45129
20/12/03 13:37:41 INFO DAGScheduler: ShuffleMapStage 4 (toPandas at /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/userFiles-f71f9c8f-7569-4614-9571-580cc7e374f7/darima.zip/darima/dlsa.py:22) finished in 1.953 s
20/12/03 13:37:41 INFO DAGScheduler: looking for newly runnable stages
20/12/03 13:37:41 INFO DAGScheduler: running: Set()
20/12/03 13:37:41 INFO DAGScheduler: waiting: Set(ResultStage 5)
20/12/03 13:37:41 INFO DAGScheduler: failed: Set()
20/12/03 13:37:41 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at toPandas at /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/userFiles-f71f9c8f-7569-4614-9571-580cc7e374f7/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 13:37:41 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/03 13:37:41 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 131.4 KiB, free 5.8 GiB)
20/12/03 13:37:41 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:46179 (size: 131.4 KiB, free: 5.8 GiB)
20/12/03 13:37:41 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1223
20/12/03 13:37:41 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at toPandas at /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/userFiles-f71f9c8f-7569-4614-9571-580cc7e374f7/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/03 13:37:41 INFO YarnScheduler: Adding task set 5.0 with 200 tasks
20/12/03 13:37:41 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:37:41 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 6, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:37:41 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:32939 (size: 131.4 KiB, free: 912.1 MiB)
20/12/03 13:37:41 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:37875 (size: 131.4 KiB, free: 912.2 MiB)
20/12/03 13:37:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:57220
20/12/03 13:37:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:57216
20/12/03 13:37:50 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 7, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 13:37:50 WARN TaskSetManager: Lost task 2.0 in stage 5.0 (TID 6, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/03 13:37:50 INFO TaskSetManager: Starting task 2.1 in stage 5.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:37:50 INFO TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 1]
20/12/03 13:37:51 INFO TaskSetManager: Starting task 0.1 in stage 5.0 (TID 9, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:37:51 INFO TaskSetManager: Lost task 3.0 in stage 5.0 (TID 7) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 2]
20/12/03 13:37:51 INFO TaskSetManager: Starting task 3.1 in stage 5.0 (TID 10, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 13:37:51 INFO TaskSetManager: Lost task 2.1 in stage 5.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 3]
20/12/03 13:37:52 INFO TaskSetManager: Starting task 2.2 in stage 5.0 (TID 11, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:37:52 INFO TaskSetManager: Lost task 0.1 in stage 5.0 (TID 9) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 4]
20/12/03 13:37:52 INFO TaskSetManager: Starting task 0.2 in stage 5.0 (TID 12, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:37:52 INFO TaskSetManager: Lost task 3.1 in stage 5.0 (TID 10) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 5]
20/12/03 13:37:53 INFO TaskSetManager: Starting task 3.2 in stage 5.0 (TID 13, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 13:37:53 INFO TaskSetManager: Lost task 2.2 in stage 5.0 (TID 11) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 6]
20/12/03 13:37:53 INFO TaskSetManager: Starting task 2.3 in stage 5.0 (TID 14, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:37:53 INFO TaskSetManager: Lost task 0.2 in stage 5.0 (TID 12) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 7]
20/12/03 13:37:54 INFO TaskSetManager: Starting task 0.3 in stage 5.0 (TID 15, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:37:54 INFO TaskSetManager: Lost task 2.3 in stage 5.0 (TID 14) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 8]
20/12/03 13:37:54 ERROR TaskSetManager: Task 2 in stage 5.0 failed 4 times; aborting job
20/12/03 13:37:54 INFO YarnScheduler: Cancelling stage 5
20/12/03 13:37:54 INFO YarnScheduler: Killing all running tasks in stage 5: Stage cancelled
20/12/03 13:37:54 INFO YarnScheduler: Stage 5 was cancelled
20/12/03 13:37:54 INFO DAGScheduler: ResultStage 5 (toPandas at /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/userFiles-f71f9c8f-7569-4614-9571-580cc7e374f7/darima.zip/darima/dlsa.py:22) failed in 12.886 s due to Job aborted due to stage failure: Task 2 in stage 5.0 failed 4 times, most recent failure: Lost task 2.3 in stage 5.0 (TID 14, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/03 13:37:54 INFO TaskSetManager: Lost task 3.2 in stage 5.0 (TID 13) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 589, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 421, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 254, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py", line 74, in read_command
    command = serializer._read_with_length(file)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py", line 458, in loads
    return pickle.loads(obj, encoding=encoding)
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/__init__.py", line 1, in <module>
    from .model import sarima2ar_model, darima_model
  File "<frozen zipimport>", line 259, in load_module
  File "./darima.zip/darima/model.py", line 9, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
) [duplicate 9]
20/12/03 13:37:54 INFO DAGScheduler: Job 3 failed: toPandas at /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/userFiles-f71f9c8f-7569-4614-9571-580cc7e374f7/darima.zip/darima/dlsa.py:22, took 14.919768 s
20/12/03 13:37:54 WARN TaskSetManager: Lost task 0.3 in stage 5.0 (TID 15, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/03 13:37:54 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/12/03 13:37:55 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 13:37:55 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 13:37:55 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 13:37:55 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 13:37:55 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 13:37:55 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 13:37:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 13:37:55 INFO MemoryStore: MemoryStore cleared
20/12/03 13:37:55 INFO BlockManager: BlockManager stopped
20/12/03 13:37:55 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 13:37:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 13:37:55 INFO SparkContext: Successfully stopped SparkContext
20/12/03 13:37:55 INFO ShutdownHookManager: Shutdown hook called
20/12/03 13:37:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf
20/12/03 13:37:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-3627e944-6f0c-4b55-a897-86736baf5c06
20/12/03 13:37:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-2e155669-5edb-4cf6-8680-2978b09947bf/pyspark-947b3846-5f4c-4b42-a7fc-0f45a3f2afba
20/12/03 13:57:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 13:57:40 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:57:40 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:57:40 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:57:40 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:57:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:57:41 INFO SparkContext: Running Spark version 3.0.1
20/12/03 13:57:41 INFO ResourceUtils: ==============================================================
20/12/03 13:57:41 INFO ResourceUtils: Resources for spark.driver:

20/12/03 13:57:41 INFO ResourceUtils: ==============================================================
20/12/03 13:57:41 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 13:57:41 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:57:41 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:57:41 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:57:41 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:57:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:57:42 INFO Utils: Successfully started service 'sparkDriver' on port 34205.
20/12/03 13:57:42 INFO SparkEnv: Registering MapOutputTracker
20/12/03 13:57:42 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 13:57:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 13:57:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 13:57:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 13:57:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a93a59e7-aa46-4c01-9b9a-50011b82c066
20/12/03 13:57:42 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 13:57:42 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 13:57:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 13:57:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 13:57:43 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 13:57:43 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 13:57:44 INFO Configuration: resource-types.xml not found
20/12/03 13:57:44 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 13:57:44 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 13:57:44 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 13:57:44 INFO Client: Setting up container launch context for our AM
20/12/03 13:57:44 INFO Client: Setting up the launch environment for our AM container
20/12/03 13:57:44 INFO Client: Preparing resources for our AM container
20/12/03 13:57:44 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 13:57:46 INFO Client: Uploading resource file:/tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/__spark_libs__12933055739038140203.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0020/__spark_libs__12933055739038140203.zip
20/12/03 13:57:47 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0020/pyspark.zip
20/12/03 13:57:47 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0020/py4j-0.10.9-src.zip
20/12/03 13:57:47 INFO Client: Uploading resource file:/tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/__spark_conf__9343493834162024766.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0020/__spark_conf__.zip
20/12/03 13:57:47 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:57:47 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:57:47 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:57:47 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:57:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:57:47 INFO Client: Submitting application application_1607015337794_0020 to ResourceManager
20/12/03 13:57:47 INFO YarnClientImpl: Submitted application application_1607015337794_0020
20/12/03 13:57:48 INFO Client: Application report for application_1607015337794_0020 (state: ACCEPTED)
20/12/03 13:57:48 INFO Client: 
	 client token: N/A
	 diagnostics: [Thu Dec 03 13:57:48 -0500 2020] Scheduler has assigned a container for AM, waiting for AM container to be launched
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607021867467
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0020/
	 user: bsuconn
20/12/03 13:57:49 INFO Client: Application report for application_1607015337794_0020 (state: ACCEPTED)
20/12/03 13:57:50 INFO Client: Application report for application_1607015337794_0020 (state: ACCEPTED)
20/12/03 13:57:51 INFO Client: Application report for application_1607015337794_0020 (state: ACCEPTED)
20/12/03 13:57:52 INFO Client: Application report for application_1607015337794_0020 (state: ACCEPTED)
20/12/03 13:57:53 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0020), /proxy/application_1607015337794_0020
20/12/03 13:57:53 INFO Client: Application report for application_1607015337794_0020 (state: RUNNING)
20/12/03 13:57:53 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607021867467
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0020/
	 user: bsuconn
20/12/03 13:57:53 INFO YarnClientSchedulerBackend: Application application_1607015337794_0020 has started running.
20/12/03 13:57:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34679.
20/12/03 13:57:53 INFO NettyBlockTransferService: Server created on 192.168.1.9:34679
20/12/03 13:57:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 13:57:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 34679, None)
20/12/03 13:57:53 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:34679 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 34679, None)
20/12/03 13:57:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 34679, None)
20/12/03 13:57:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 34679, None)
20/12/03 13:57:53 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:57:54 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 13:57:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 13:57:58 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:60300) with ID 1
20/12/03 13:57:58 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/03 13:57:58 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:41419 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 41419, None)
20/12/03 13:57:58 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 13:57:58 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 13:57:58 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 13:57:58 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:57:58 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:57:58 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:57:58 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:57:58 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:57:59 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:34205/files/darima.zip with timestamp 1607021879101
20/12/03 13:57:59 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/userFiles-f70638d6-159f-4ba0-b1ef-d9eae17b2f5b/darima.zip
20/12/03 13:58:00 INFO InMemoryFileIndex: It took 51 ms to list leaf files for 1 paths.
20/12/03 13:58:02 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:58:02 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:58:02 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:58:02 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:58:03 INFO CodeGenerator: Code generated in 321.953271 ms
20/12/03 13:58:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 13:58:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 13:58:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:34679 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:58:04 INFO SparkContext: Created broadcast 0 from head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108
20/12/03 13:58:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:58:04 INFO SparkContext: Starting job: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108
20/12/03 13:58:04 INFO DAGScheduler: Got job 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) with 1 output partitions
20/12/03 13:58:04 INFO DAGScheduler: Final stage: ResultStage 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108)
20/12/03 13:58:04 INFO DAGScheduler: Parents of final stage: List()
20/12/03 13:58:04 INFO DAGScheduler: Missing parents: List()
20/12/03 13:58:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108), which has no missing parents
20/12/03 13:58:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.6 KiB, free 5.8 GiB)
20/12/03 13:58:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 5.8 GiB)
20/12/03 13:58:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:34679 (size: 6.3 KiB, free: 5.8 GiB)
20/12/03 13:58:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 13:58:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) (first 15 tasks are for partitions Vector(0))
20/12/03 13:58:04 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 13:58:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7790 bytes)
20/12/03 13:58:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:41419 (size: 6.3 KiB, free: 912.3 MiB)
20/12/03 13:58:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:41419 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 13:58:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3386 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:58:07 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/03 13:58:07 INFO DAGScheduler: ResultStage 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) finished in 3.553 s
20/12/03 13:58:07 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:58:07 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
20/12/03 13:58:07 INFO DAGScheduler: Job 0 finished: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108, took 3.632838 s
20/12/03 13:58:07 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/12/03 13:58:07 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:58:07 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:58:07 INFO FileSourceStrategy: Post-Scan Filters: 
20/12/03 13:58:07 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:58:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 13:58:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 13:58:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:34679 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:58:08 INFO SparkContext: Created broadcast 2 from head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112
20/12/03 13:58:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:58:08 INFO SparkContext: Starting job: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112
20/12/03 13:58:08 INFO DAGScheduler: Got job 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) with 1 output partitions
20/12/03 13:58:08 INFO DAGScheduler: Final stage: ResultStage 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112)
20/12/03 13:58:08 INFO DAGScheduler: Parents of final stage: List()
20/12/03 13:58:08 INFO DAGScheduler: Missing parents: List()
20/12/03 13:58:08 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112), which has no missing parents
20/12/03 13:58:08 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.9 KiB, free 5.8 GiB)
20/12/03 13:58:08 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 5.8 GiB)
20/12/03 13:58:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:34679 (size: 4.9 KiB, free: 5.8 GiB)
20/12/03 13:58:08 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223
20/12/03 13:58:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) (first 15 tasks are for partitions Vector(0))
20/12/03 13:58:08 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/03 13:58:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7789 bytes)
20/12/03 13:58:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:41419 (size: 4.9 KiB, free: 912.3 MiB)
20/12/03 13:58:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:41419 (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 13:58:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 182 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:58:08 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/03 13:58:08 INFO DAGScheduler: ResultStage 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) finished in 0.213 s
20/12/03 13:58:08 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:58:08 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/03 13:58:08 INFO DAGScheduler: Job 1 finished: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112, took 0.218753 s
20/12/03 13:58:08 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:58:08 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:58:08 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:58:08 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:58:08 INFO CodeGenerator: Code generated in 28.739476 ms
20/12/03 13:58:08 INFO CodeGenerator: Code generated in 27.253547 ms
20/12/03 13:58:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 13:58:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 13:58:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:34679 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:58:08 INFO SparkContext: Created broadcast 4 from count at NativeMethodAccessorImpl.java:0
20/12/03 13:58:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:58:08 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 13:58:08 INFO DAGScheduler: Registering RDD 10 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 13:58:08 INFO DAGScheduler: Got job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 13:58:08 INFO DAGScheduler: Final stage: ResultStage 3 (count at NativeMethodAccessorImpl.java:0)
20/12/03 13:58:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/03 13:58:08 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/03 13:58:08 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 13:58:08 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 13:58:08 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 13:58:08 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:34679 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 13:58:08 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/03 13:58:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 13:58:08 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/03 13:58:08 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 13:58:08 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:41419 (size: 8.0 KiB, free: 912.2 MiB)
20/12/03 13:58:09 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:41419 (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 13:58:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 622 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:58:09 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/03 13:58:09 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.659 s
20/12/03 13:58:09 INFO DAGScheduler: looking for newly runnable stages
20/12/03 13:58:09 INFO DAGScheduler: running: Set()
20/12/03 13:58:09 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/03 13:58:09 INFO DAGScheduler: failed: Set()
20/12/03 13:58:09 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 13:58:09 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/03 13:58:09 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/03 13:58:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:34679 (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 13:58:09 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1223
20/12/03 13:58:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 13:58:09 INFO YarnScheduler: Adding task set 3.0 with 1 tasks
20/12/03 13:58:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:58:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:41419 (size: 5.0 KiB, free: 912.2 MiB)
20/12/03 13:58:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:60300
20/12/03 13:58:09 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 223 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:58:09 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/03 13:58:09 INFO DAGScheduler: ResultStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.242 s
20/12/03 13:58:09 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:58:09 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
20/12/03 13:58:09 INFO DAGScheduler: Job 2 finished: count at NativeMethodAccessorImpl.java:0, took 0.941198 s
20/12/03 13:58:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:34679 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:58:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:41419 in memory (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 13:58:11 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.9:34679 in memory (size: 4.9 KiB, free: 5.8 GiB)
20/12/03 13:58:11 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.9:41419 in memory (size: 4.9 KiB, free: 912.2 MiB)
20/12/03 13:58:12 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:41419 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 13:58:12 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:34679 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:58:12 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.9:41419 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 13:58:12 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.9:34679 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 13:58:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:41419 in memory (size: 6.3 KiB, free: 912.3 MiB)
20/12/03 13:58:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:34679 in memory (size: 6.3 KiB, free: 5.8 GiB)
20/12/03 13:58:12 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.9:41419 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 13:58:12 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.9:34679 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 13:58:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/03 13:58:12 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:58:12 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:58:12 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:58:12 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:58:13 INFO CodeGenerator: Code generated in 26.402103 ms
20/12/03 13:58:13 INFO CodeGenerator: Code generated in 17.005978 ms
20/12/03 13:58:13 INFO CodeGenerator: Code generated in 27.998623 ms
20/12/03 13:58:13 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 13:58:13 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 13:58:13 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:34679 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:58:13 INFO SparkContext: Created broadcast 7 from toPandas at /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/userFiles-f70638d6-159f-4ba0-b1ef-d9eae17b2f5b/darima.zip/darima/dlsa.py:22
20/12/03 13:58:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:58:13 INFO SparkContext: Starting job: toPandas at /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/userFiles-f70638d6-159f-4ba0-b1ef-d9eae17b2f5b/darima.zip/darima/dlsa.py:22
20/12/03 13:58:13 INFO DAGScheduler: Registering RDD 20 (toPandas at /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/userFiles-f70638d6-159f-4ba0-b1ef-d9eae17b2f5b/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/03 13:58:13 INFO DAGScheduler: Got job 3 (toPandas at /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/userFiles-f70638d6-159f-4ba0-b1ef-d9eae17b2f5b/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/03 13:58:13 INFO DAGScheduler: Final stage: ResultStage 5 (toPandas at /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/userFiles-f70638d6-159f-4ba0-b1ef-d9eae17b2f5b/darima.zip/darima/dlsa.py:22)
20/12/03 13:58:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
20/12/03 13:58:13 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)
20/12/03 13:58:13 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at toPandas at /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/userFiles-f70638d6-159f-4ba0-b1ef-d9eae17b2f5b/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 13:58:13 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.2 KiB, free 5.8 GiB)
20/12/03 13:58:13 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/03 13:58:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:34679 (size: 11.6 KiB, free: 5.8 GiB)
20/12/03 13:58:13 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1223
20/12/03 13:58:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at toPandas at /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/userFiles-f70638d6-159f-4ba0-b1ef-d9eae17b2f5b/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/03 13:58:13 INFO YarnScheduler: Adding task set 4.0 with 1 tasks
20/12/03 13:58:13 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 13:58:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:41419 (size: 11.6 KiB, free: 912.3 MiB)
20/12/03 13:58:14 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:41419 (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 13:58:14 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1400 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:58:14 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/12/03 13:58:14 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 35529
20/12/03 13:58:14 INFO DAGScheduler: ShuffleMapStage 4 (toPandas at /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/userFiles-f70638d6-159f-4ba0-b1ef-d9eae17b2f5b/darima.zip/darima/dlsa.py:22) finished in 1.431 s
20/12/03 13:58:14 INFO DAGScheduler: looking for newly runnable stages
20/12/03 13:58:14 INFO DAGScheduler: running: Set()
20/12/03 13:58:14 INFO DAGScheduler: waiting: Set(ResultStage 5)
20/12/03 13:58:14 INFO DAGScheduler: failed: Set()
20/12/03 13:58:14 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at toPandas at /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/userFiles-f70638d6-159f-4ba0-b1ef-d9eae17b2f5b/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 13:58:15 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 428.0 KiB, free 5.8 GiB)
20/12/03 13:58:15 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 131.4 KiB, free 5.8 GiB)
20/12/03 13:58:15 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:34679 (size: 131.4 KiB, free: 5.8 GiB)
20/12/03 13:58:15 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1223
20/12/03 13:58:15 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at toPandas at /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/userFiles-f70638d6-159f-4ba0-b1ef-d9eae17b2f5b/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/03 13:58:15 INFO YarnScheduler: Adding task set 5.0 with 200 tasks
20/12/03 13:58:15 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:58:15 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:41419 (size: 131.4 KiB, free: 912.1 MiB)
20/12/03 13:58:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:60300
20/12/03 13:58:18 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:58:18 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/03 13:58:19 INFO TaskSetManager: Starting task 0.1 in stage 5.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:58:19 INFO TaskSetManager: Lost task 2.0 in stage 5.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/03 13:58:20 INFO TaskSetManager: Lost task 0.1 in stage 5.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/03 13:58:20 INFO TaskSetManager: Starting task 0.2 in stage 5.0 (TID 8, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:58:21 INFO TaskSetManager: Starting task 2.1 in stage 5.0 (TID 9, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:58:21 INFO TaskSetManager: Lost task 0.2 in stage 5.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/03 13:58:22 INFO TaskSetManager: Lost task 2.1 in stage 5.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/03 13:58:22 INFO TaskSetManager: Starting task 2.2 in stage 5.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:58:23 INFO TaskSetManager: Starting task 0.3 in stage 5.0 (TID 11, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:58:23 INFO TaskSetManager: Lost task 2.2 in stage 5.0 (TID 10) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/03 13:58:24 INFO TaskSetManager: Starting task 2.3 in stage 5.0 (TID 12, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:58:24 INFO TaskSetManager: Lost task 0.3 in stage 5.0 (TID 11) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/03 13:58:24 ERROR TaskSetManager: Task 0 in stage 5.0 failed 4 times; aborting job
20/12/03 13:58:24 INFO YarnScheduler: Cancelling stage 5
20/12/03 13:58:24 INFO YarnScheduler: Killing all running tasks in stage 5: Stage cancelled
20/12/03 13:58:24 INFO YarnScheduler: Stage 5 was cancelled
20/12/03 13:58:24 INFO DAGScheduler: ResultStage 5 (toPandas at /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/userFiles-f70638d6-159f-4ba0-b1ef-d9eae17b2f5b/darima.zip/darima/dlsa.py:22) failed in 9.118 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 11, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/03 13:58:24 INFO DAGScheduler: Job 3 failed: toPandas at /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/userFiles-f70638d6-159f-4ba0-b1ef-d9eae17b2f5b/darima.zip/darima/dlsa.py:22, took 10.610637 s
20/12/03 13:58:24 WARN TaskSetManager: Lost task 2.3 in stage 5.0 (TID 12, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/03 13:58:24 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/12/03 13:58:24 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 13:58:24 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 13:58:24 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 13:58:24 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 13:58:24 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 13:58:24 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 13:58:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 13:58:24 INFO MemoryStore: MemoryStore cleared
20/12/03 13:58:24 INFO BlockManager: BlockManager stopped
20/12/03 13:58:24 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 13:58:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 13:58:24 INFO SparkContext: Successfully stopped SparkContext
20/12/03 13:58:24 INFO ShutdownHookManager: Shutdown hook called
20/12/03 13:58:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0/pyspark-df4c1100-fc84-4e71-918e-3ee140e66fcb
20/12/03 13:58:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-7e04dd59-af1d-4f42-a987-959418b3e374
20/12/03 13:58:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-7a9d0ea7-5f79-443b-a1a0-37b0bf64f7d0
20/12/03 13:58:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 13:58:27 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:58:27 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:58:27 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:58:27 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:58:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:58:28 INFO SparkContext: Running Spark version 3.0.1
20/12/03 13:58:28 INFO ResourceUtils: ==============================================================
20/12/03 13:58:28 INFO ResourceUtils: Resources for spark.driver:

20/12/03 13:58:28 INFO ResourceUtils: ==============================================================
20/12/03 13:58:28 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 13:58:28 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:58:28 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:58:28 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:58:28 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:58:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:58:28 INFO Utils: Successfully started service 'sparkDriver' on port 40151.
20/12/03 13:58:28 INFO SparkEnv: Registering MapOutputTracker
20/12/03 13:58:28 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 13:58:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 13:58:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 13:58:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 13:58:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d5abc5dd-ba0f-4367-a1f6-40a88a98fcd3
20/12/03 13:58:28 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 13:58:28 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 13:58:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 13:58:29 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 13:58:29 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 13:58:30 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 13:58:30 INFO Configuration: resource-types.xml not found
20/12/03 13:58:30 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 13:58:30 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 13:58:30 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 13:58:30 INFO Client: Setting up container launch context for our AM
20/12/03 13:58:30 INFO Client: Setting up the launch environment for our AM container
20/12/03 13:58:30 INFO Client: Preparing resources for our AM container
20/12/03 13:58:30 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 13:58:32 INFO Client: Uploading resource file:/tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/__spark_libs__1547751570720720686.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0021/__spark_libs__1547751570720720686.zip
20/12/03 13:58:33 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0021/pyspark.zip
20/12/03 13:58:33 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0021/py4j-0.10.9-src.zip
20/12/03 13:58:34 INFO Client: Uploading resource file:/tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/__spark_conf__651445449753544240.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0021/__spark_conf__.zip
20/12/03 13:58:34 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:58:34 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:58:34 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:58:34 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:58:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:58:34 INFO Client: Submitting application application_1607015337794_0021 to ResourceManager
20/12/03 13:58:34 INFO YarnClientImpl: Submitted application application_1607015337794_0021
20/12/03 13:58:35 INFO Client: Application report for application_1607015337794_0021 (state: ACCEPTED)
20/12/03 13:58:35 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607021914640
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0021/
	 user: bsuconn
20/12/03 13:58:36 INFO Client: Application report for application_1607015337794_0021 (state: ACCEPTED)
20/12/03 13:58:37 INFO Client: Application report for application_1607015337794_0021 (state: ACCEPTED)
20/12/03 13:58:38 INFO Client: Application report for application_1607015337794_0021 (state: ACCEPTED)
20/12/03 13:58:39 INFO Client: Application report for application_1607015337794_0021 (state: ACCEPTED)
20/12/03 13:58:40 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0021), /proxy/application_1607015337794_0021
20/12/03 13:58:40 INFO Client: Application report for application_1607015337794_0021 (state: RUNNING)
20/12/03 13:58:40 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607021914640
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0021/
	 user: bsuconn
20/12/03 13:58:40 INFO YarnClientSchedulerBackend: Application application_1607015337794_0021 has started running.
20/12/03 13:58:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37489.
20/12/03 13:58:40 INFO NettyBlockTransferService: Server created on 192.168.1.9:37489
20/12/03 13:58:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 13:58:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 37489, None)
20/12/03 13:58:40 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:37489 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 37489, None)
20/12/03 13:58:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 37489, None)
20/12/03 13:58:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 37489, None)
20/12/03 13:58:41 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:58:41 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 13:58:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 13:58:47 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:51242) with ID 1
20/12/03 13:58:47 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:46669 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 46669, None)
20/12/03 13:58:49 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:51246) with ID 2
20/12/03 13:58:49 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/03 13:58:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:41283 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 41283, None)
20/12/03 13:58:49 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 13:58:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 13:58:49 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 13:58:49 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:58:49 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:58:49 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:58:49 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:58:49 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:58:50 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:40151/files/darima.zip with timestamp 1607021930200
20/12/03 13:58:50 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/userFiles-edefac6d-cbff-488a-a7b4-06c65f7ee080/darima.zip
20/12/03 13:58:51 INFO InMemoryFileIndex: It took 51 ms to list leaf files for 1 paths.
20/12/03 13:58:53 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:58:53 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:58:53 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:58:53 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:58:54 INFO CodeGenerator: Code generated in 306.185525 ms
20/12/03 13:58:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 13:58:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 13:58:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:37489 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:58:55 INFO SparkContext: Created broadcast 0 from head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108
20/12/03 13:58:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:58:55 INFO SparkContext: Starting job: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108
20/12/03 13:58:55 INFO DAGScheduler: Got job 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) with 1 output partitions
20/12/03 13:58:55 INFO DAGScheduler: Final stage: ResultStage 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108)
20/12/03 13:58:55 INFO DAGScheduler: Parents of final stage: List()
20/12/03 13:58:55 INFO DAGScheduler: Missing parents: List()
20/12/03 13:58:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108), which has no missing parents
20/12/03 13:58:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.6 KiB, free 5.8 GiB)
20/12/03 13:58:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 5.8 GiB)
20/12/03 13:58:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:37489 (size: 6.3 KiB, free: 5.8 GiB)
20/12/03 13:58:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 13:58:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) (first 15 tasks are for partitions Vector(0))
20/12/03 13:58:55 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 13:58:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7790 bytes)
20/12/03 13:58:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:41283 (size: 6.3 KiB, free: 912.3 MiB)
20/12/03 13:58:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:41283 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 13:58:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3481 ms on 192.168.1.9 (executor 2) (1/1)
20/12/03 13:58:58 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/03 13:58:58 INFO DAGScheduler: ResultStage 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) finished in 3.713 s
20/12/03 13:58:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:58:58 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
20/12/03 13:58:58 INFO DAGScheduler: Job 0 finished: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108, took 3.783742 s
20/12/03 13:58:59 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/12/03 13:58:59 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:58:59 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:58:59 INFO FileSourceStrategy: Post-Scan Filters: 
20/12/03 13:58:59 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:58:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 13:58:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 13:58:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:37489 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:58:59 INFO SparkContext: Created broadcast 2 from head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112
20/12/03 13:58:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:58:59 INFO SparkContext: Starting job: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112
20/12/03 13:58:59 INFO DAGScheduler: Got job 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) with 1 output partitions
20/12/03 13:58:59 INFO DAGScheduler: Final stage: ResultStage 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112)
20/12/03 13:58:59 INFO DAGScheduler: Parents of final stage: List()
20/12/03 13:58:59 INFO DAGScheduler: Missing parents: List()
20/12/03 13:58:59 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112), which has no missing parents
20/12/03 13:58:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.9 KiB, free 5.8 GiB)
20/12/03 13:58:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 5.8 GiB)
20/12/03 13:58:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:37489 (size: 4.9 KiB, free: 5.8 GiB)
20/12/03 13:58:59 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223
20/12/03 13:58:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) (first 15 tasks are for partitions Vector(0))
20/12/03 13:58:59 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/03 13:58:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7789 bytes)
20/12/03 13:58:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:41283 (size: 4.9 KiB, free: 912.3 MiB)
20/12/03 13:58:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:41283 (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 13:58:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 249 ms on 192.168.1.9 (executor 2) (1/1)
20/12/03 13:58:59 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/03 13:58:59 INFO DAGScheduler: ResultStage 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) finished in 0.271 s
20/12/03 13:58:59 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:58:59 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/03 13:58:59 INFO DAGScheduler: Job 1 finished: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112, took 0.283080 s
20/12/03 13:58:59 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:58:59 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:58:59 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:58:59 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:58:59 INFO CodeGenerator: Code generated in 29.407042 ms
20/12/03 13:58:59 INFO CodeGenerator: Code generated in 25.979712 ms
20/12/03 13:58:59 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 13:58:59 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 13:58:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:37489 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:58:59 INFO SparkContext: Created broadcast 4 from count at NativeMethodAccessorImpl.java:0
20/12/03 13:58:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:58:59 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 13:58:59 INFO DAGScheduler: Registering RDD 10 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 13:58:59 INFO DAGScheduler: Got job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 13:58:59 INFO DAGScheduler: Final stage: ResultStage 3 (count at NativeMethodAccessorImpl.java:0)
20/12/03 13:58:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/03 13:58:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/03 13:58:59 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 13:58:59 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 13:58:59 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 13:58:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:37489 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 13:58:59 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/03 13:58:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 13:58:59 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/03 13:58:59 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 13:58:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:41283 (size: 8.0 KiB, free: 912.2 MiB)
20/12/03 13:59:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:41283 (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 13:59:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 673 ms on 192.168.1.9 (executor 2) (1/1)
20/12/03 13:59:00 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/03 13:59:00 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.702 s
20/12/03 13:59:00 INFO DAGScheduler: looking for newly runnable stages
20/12/03 13:59:00 INFO DAGScheduler: running: Set()
20/12/03 13:59:00 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/03 13:59:00 INFO DAGScheduler: failed: Set()
20/12/03 13:59:00 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 13:59:00 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/03 13:59:00 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/03 13:59:00 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:37489 (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 13:59:00 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1223
20/12/03 13:59:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 13:59:00 INFO YarnScheduler: Adding task set 3.0 with 1 tasks
20/12/03 13:59:00 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:59:01 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:46669 (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 13:59:01 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:51242
20/12/03 13:59:02 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1972 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:59:02 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/03 13:59:02 INFO DAGScheduler: ResultStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 1.995 s
20/12/03 13:59:02 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 13:59:02 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
20/12/03 13:59:02 INFO DAGScheduler: Job 2 finished: count at NativeMethodAccessorImpl.java:0, took 2.733031 s
20/12/03 13:59:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:37489 in memory (size: 6.3 KiB, free: 5.8 GiB)
20/12/03 13:59:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:41283 in memory (size: 6.3 KiB, free: 912.2 MiB)
20/12/03 13:59:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:41283 in memory (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 13:59:06 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:37489 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:59:06 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/03 13:59:06 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.9:37489 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 13:59:06 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.9:46669 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 13:59:06 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:41283 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 13:59:06 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:37489 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:59:06 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.9:41283 in memory (size: 4.9 KiB, free: 912.3 MiB)
20/12/03 13:59:06 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.9:37489 in memory (size: 4.9 KiB, free: 5.8 GiB)
20/12/03 13:59:06 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.9:37489 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 13:59:06 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.9:41283 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 13:59:06 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 13:59:06 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 13:59:06 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 13:59:06 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 13:59:07 INFO CodeGenerator: Code generated in 44.388546 ms
20/12/03 13:59:07 INFO CodeGenerator: Code generated in 17.140142 ms
20/12/03 13:59:07 INFO CodeGenerator: Code generated in 20.809085 ms
20/12/03 13:59:07 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 13:59:07 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 13:59:07 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:37489 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 13:59:07 INFO SparkContext: Created broadcast 7 from toPandas at /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/userFiles-edefac6d-cbff-488a-a7b4-06c65f7ee080/darima.zip/darima/dlsa.py:22
20/12/03 13:59:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 13:59:07 INFO SparkContext: Starting job: toPandas at /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/userFiles-edefac6d-cbff-488a-a7b4-06c65f7ee080/darima.zip/darima/dlsa.py:22
20/12/03 13:59:07 INFO DAGScheduler: Registering RDD 20 (toPandas at /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/userFiles-edefac6d-cbff-488a-a7b4-06c65f7ee080/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/03 13:59:07 INFO DAGScheduler: Got job 3 (toPandas at /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/userFiles-edefac6d-cbff-488a-a7b4-06c65f7ee080/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/03 13:59:07 INFO DAGScheduler: Final stage: ResultStage 5 (toPandas at /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/userFiles-edefac6d-cbff-488a-a7b4-06c65f7ee080/darima.zip/darima/dlsa.py:22)
20/12/03 13:59:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
20/12/03 13:59:07 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)
20/12/03 13:59:07 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at toPandas at /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/userFiles-edefac6d-cbff-488a-a7b4-06c65f7ee080/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 13:59:07 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/03 13:59:07 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/03 13:59:07 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:37489 (size: 11.6 KiB, free: 5.8 GiB)
20/12/03 13:59:07 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1223
20/12/03 13:59:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at toPandas at /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/userFiles-edefac6d-cbff-488a-a7b4-06c65f7ee080/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/03 13:59:07 INFO YarnScheduler: Adding task set 4.0 with 1 tasks
20/12/03 13:59:07 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 13:59:07 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:46669 (size: 11.6 KiB, free: 912.3 MiB)
20/12/03 13:59:08 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:46669 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 13:59:10 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 3152 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 13:59:10 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/12/03 13:59:10 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 56307
20/12/03 13:59:10 INFO DAGScheduler: ShuffleMapStage 4 (toPandas at /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/userFiles-edefac6d-cbff-488a-a7b4-06c65f7ee080/darima.zip/darima/dlsa.py:22) finished in 3.194 s
20/12/03 13:59:10 INFO DAGScheduler: looking for newly runnable stages
20/12/03 13:59:10 INFO DAGScheduler: running: Set()
20/12/03 13:59:10 INFO DAGScheduler: waiting: Set(ResultStage 5)
20/12/03 13:59:10 INFO DAGScheduler: failed: Set()
20/12/03 13:59:10 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at toPandas at /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/userFiles-edefac6d-cbff-488a-a7b4-06c65f7ee080/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 13:59:10 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/03 13:59:10 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 131.4 KiB, free 5.8 GiB)
20/12/03 13:59:10 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:37489 (size: 131.4 KiB, free: 5.8 GiB)
20/12/03 13:59:10 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1223
20/12/03 13:59:10 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at toPandas at /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/userFiles-edefac6d-cbff-488a-a7b4-06c65f7ee080/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/03 13:59:10 INFO YarnScheduler: Adding task set 5.0 with 200 tasks
20/12/03 13:59:10 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:59:10 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 6, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:59:10 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:41283 (size: 131.4 KiB, free: 912.1 MiB)
20/12/03 13:59:10 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:46669 (size: 131.4 KiB, free: 912.1 MiB)
20/12/03 13:59:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:51242
20/12/03 13:59:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:51246
20/12/03 13:59:17 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 7, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 13:59:17 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/03 13:59:18 INFO TaskSetManager: Starting task 0.1 in stage 5.0 (TID 8, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:59:18 INFO TaskSetManager: Lost task 2.0 in stage 5.0 (TID 6) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/03 13:59:19 INFO TaskSetManager: Starting task 2.1 in stage 5.0 (TID 9, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:59:19 INFO TaskSetManager: Lost task 3.0 in stage 5.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/03 13:59:19 INFO TaskSetManager: Starting task 3.1 in stage 5.0 (TID 10, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 13:59:19 INFO TaskSetManager: Lost task 0.1 in stage 5.0 (TID 8) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/03 13:59:20 INFO TaskSetManager: Starting task 0.2 in stage 5.0 (TID 11, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:59:20 INFO TaskSetManager: Lost task 2.1 in stage 5.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/03 13:59:21 INFO TaskSetManager: Starting task 2.2 in stage 5.0 (TID 12, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:59:21 INFO TaskSetManager: Lost task 3.1 in stage 5.0 (TID 10) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/03 13:59:22 INFO TaskSetManager: Starting task 3.2 in stage 5.0 (TID 13, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 13:59:22 INFO TaskSetManager: Lost task 0.2 in stage 5.0 (TID 11) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/03 13:59:22 INFO TaskSetManager: Starting task 0.3 in stage 5.0 (TID 14, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 13:59:22 INFO TaskSetManager: Lost task 2.2 in stage 5.0 (TID 12) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/03 13:59:23 INFO TaskSetManager: Starting task 2.3 in stage 5.0 (TID 15, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 13:59:23 INFO TaskSetManager: Lost task 3.2 in stage 5.0 (TID 13) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/03 13:59:24 INFO TaskSetManager: Starting task 3.3 in stage 5.0 (TID 16, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 13:59:24 INFO TaskSetManager: Lost task 0.3 in stage 5.0 (TID 14) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 9]
20/12/03 13:59:24 ERROR TaskSetManager: Task 0 in stage 5.0 failed 4 times; aborting job
20/12/03 13:59:24 INFO YarnScheduler: Cancelling stage 5
20/12/03 13:59:24 INFO YarnScheduler: Killing all running tasks in stage 5: Stage cancelled
20/12/03 13:59:24 INFO YarnScheduler: Stage 5 was cancelled
20/12/03 13:59:24 INFO DAGScheduler: ResultStage 5 (toPandas at /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/userFiles-edefac6d-cbff-488a-a7b4-06c65f7ee080/darima.zip/darima/dlsa.py:22) failed in 13.793 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 14, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/03 13:59:24 INFO DAGScheduler: Job 3 failed: toPandas at /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/userFiles-edefac6d-cbff-488a-a7b4-06c65f7ee080/darima.zip/darima/dlsa.py:22, took 17.063610 s
20/12/03 13:59:24 WARN TaskSetManager: Lost task 3.3 in stage 5.0 (TID 16, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/03 13:59:24 WARN TaskSetManager: Lost task 2.3 in stage 5.0 (TID 15, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/03 13:59:24 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/12/03 13:59:25 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 13:59:25 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 13:59:25 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 13:59:25 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 13:59:25 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 13:59:25 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 13:59:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 13:59:25 INFO MemoryStore: MemoryStore cleared
20/12/03 13:59:25 INFO BlockManager: BlockManager stopped
20/12/03 13:59:25 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 13:59:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 13:59:25 INFO SparkContext: Successfully stopped SparkContext
20/12/03 13:59:25 INFO ShutdownHookManager: Shutdown hook called
20/12/03 13:59:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8
20/12/03 13:59:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-182a9bed-a43e-42b5-aa75-a2c85343a5c8/pyspark-1d08cd8f-86ff-4721-a7c0-2da302c01e3f
20/12/03 13:59:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-2edd9660-5d91-4db4-a707-42144f92127c
20/12/03 13:59:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 13:59:28 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:59:28 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:59:28 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:59:28 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:59:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:59:28 INFO SparkContext: Running Spark version 3.0.1
20/12/03 13:59:28 INFO ResourceUtils: ==============================================================
20/12/03 13:59:28 INFO ResourceUtils: Resources for spark.driver:

20/12/03 13:59:28 INFO ResourceUtils: ==============================================================
20/12/03 13:59:28 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 13:59:29 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:59:29 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:59:29 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:59:29 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:59:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:59:29 INFO Utils: Successfully started service 'sparkDriver' on port 38769.
20/12/03 13:59:29 INFO SparkEnv: Registering MapOutputTracker
20/12/03 13:59:29 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 13:59:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 13:59:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 13:59:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 13:59:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-41de46de-b0f2-4f35-be67-e5cbf486d08f
20/12/03 13:59:29 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 13:59:29 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 13:59:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 13:59:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 13:59:30 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 13:59:30 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 13:59:31 INFO Configuration: resource-types.xml not found
20/12/03 13:59:31 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 13:59:31 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 13:59:31 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 13:59:31 INFO Client: Setting up container launch context for our AM
20/12/03 13:59:31 INFO Client: Setting up the launch environment for our AM container
20/12/03 13:59:31 INFO Client: Preparing resources for our AM container
20/12/03 13:59:31 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 13:59:33 INFO Client: Uploading resource file:/tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/__spark_libs__12711093140310518692.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0022/__spark_libs__12711093140310518692.zip
20/12/03 13:59:34 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0022/pyspark.zip
20/12/03 13:59:34 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0022/py4j-0.10.9-src.zip
20/12/03 13:59:35 INFO Client: Uploading resource file:/tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/__spark_conf__8647653168473758325.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0022/__spark_conf__.zip
20/12/03 13:59:36 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 13:59:36 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 13:59:36 INFO SecurityManager: Changing view acls groups to: 
20/12/03 13:59:36 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 13:59:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 13:59:36 INFO Client: Submitting application application_1607015337794_0022 to ResourceManager
20/12/03 13:59:36 INFO YarnClientImpl: Submitted application application_1607015337794_0022
20/12/03 13:59:37 INFO Client: Application report for application_1607015337794_0022 (state: ACCEPTED)
20/12/03 13:59:37 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607021976097
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0022/
	 user: bsuconn
20/12/03 13:59:38 INFO Client: Application report for application_1607015337794_0022 (state: ACCEPTED)
20/12/03 13:59:39 INFO Client: Application report for application_1607015337794_0022 (state: ACCEPTED)
20/12/03 13:59:40 INFO Client: Application report for application_1607015337794_0022 (state: ACCEPTED)
20/12/03 13:59:41 INFO Client: Application report for application_1607015337794_0022 (state: ACCEPTED)
20/12/03 13:59:41 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0022), /proxy/application_1607015337794_0022
20/12/03 13:59:42 INFO Client: Application report for application_1607015337794_0022 (state: RUNNING)
20/12/03 13:59:42 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607021976097
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0022/
	 user: bsuconn
20/12/03 13:59:42 INFO YarnClientSchedulerBackend: Application application_1607015337794_0022 has started running.
20/12/03 13:59:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35935.
20/12/03 13:59:42 INFO NettyBlockTransferService: Server created on 192.168.1.9:35935
20/12/03 13:59:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 13:59:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 35935, None)
20/12/03 13:59:42 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:35935 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 35935, None)
20/12/03 13:59:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 35935, None)
20/12/03 13:59:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 35935, None)
20/12/03 13:59:42 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 13:59:42 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 13:59:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 13:59:48 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:52650) with ID 1
20/12/03 13:59:48 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:45947 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 45947, None)
20/12/03 13:59:49 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:52654) with ID 2
20/12/03 13:59:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:42275 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 42275, None)
20/12/03 14:00:00 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
20/12/03 14:00:00 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 14:00:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 14:00:00 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 14:00:00 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:00:00 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:00:00 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:00:00 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:00:00 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:00:01 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:38769/files/darima.zip with timestamp 1607022001359
20/12/03 14:00:01 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/userFiles-b70ca20e-d40b-45d9-999c-44739f062f34/darima.zip
20/12/03 14:00:03 INFO InMemoryFileIndex: It took 78 ms to list leaf files for 1 paths.
20/12/03 14:00:05 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:00:05 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:00:05 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:00:05 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:00:06 INFO CodeGenerator: Code generated in 321.268568 ms
20/12/03 14:00:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:00:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:00:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:35935 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:00:06 INFO SparkContext: Created broadcast 0 from head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108
20/12/03 14:00:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:00:06 INFO SparkContext: Starting job: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108
20/12/03 14:00:06 INFO DAGScheduler: Got job 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) with 1 output partitions
20/12/03 14:00:06 INFO DAGScheduler: Final stage: ResultStage 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108)
20/12/03 14:00:06 INFO DAGScheduler: Parents of final stage: List()
20/12/03 14:00:06 INFO DAGScheduler: Missing parents: List()
20/12/03 14:00:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108), which has no missing parents
20/12/03 14:00:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.6 KiB, free 5.8 GiB)
20/12/03 14:00:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 5.8 GiB)
20/12/03 14:00:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:35935 (size: 6.3 KiB, free: 5.8 GiB)
20/12/03 14:00:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 14:00:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) (first 15 tasks are for partitions Vector(0))
20/12/03 14:00:06 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 14:00:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7790 bytes)
20/12/03 14:00:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:45947 (size: 6.3 KiB, free: 912.3 MiB)
20/12/03 14:00:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:45947 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:00:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2900 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:00:09 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/03 14:00:09 INFO DAGScheduler: ResultStage 0 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108) finished in 3.053 s
20/12/03 14:00:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 14:00:09 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
20/12/03 14:00:09 INFO DAGScheduler: Job 0 finished: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:108, took 3.126014 s
20/12/03 14:00:09 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
20/12/03 14:00:09 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:00:09 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:00:09 INFO FileSourceStrategy: Post-Scan Filters: 
20/12/03 14:00:09 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:00:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:00:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:00:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:35935 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:00:09 INFO SparkContext: Created broadcast 2 from head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112
20/12/03 14:00:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:00:09 INFO SparkContext: Starting job: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112
20/12/03 14:00:09 INFO DAGScheduler: Got job 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) with 1 output partitions
20/12/03 14:00:09 INFO DAGScheduler: Final stage: ResultStage 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112)
20/12/03 14:00:09 INFO DAGScheduler: Parents of final stage: List()
20/12/03 14:00:09 INFO DAGScheduler: Missing parents: List()
20/12/03 14:00:09 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112), which has no missing parents
20/12/03 14:00:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.9 KiB, free 5.8 GiB)
20/12/03 14:00:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 5.8 GiB)
20/12/03 14:00:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:35935 (size: 4.9 KiB, free: 5.8 GiB)
20/12/03 14:00:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223
20/12/03 14:00:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) (first 15 tasks are for partitions Vector(0))
20/12/03 14:00:10 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/03 14:00:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7789 bytes)
20/12/03 14:00:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:45947 (size: 4.9 KiB, free: 912.3 MiB)
20/12/03 14:00:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:45947 (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 14:00:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 191 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:00:10 INFO DAGScheduler: ResultStage 1 (head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112) finished in 0.209 s
20/12/03 14:00:10 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 14:00:10 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/03 14:00:10 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/03 14:00:10 INFO DAGScheduler: Job 1 finished: head at /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py:112, took 0.216128 s
20/12/03 14:00:10 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:00:10 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:00:10 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:00:10 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:00:10 INFO CodeGenerator: Code generated in 30.416911 ms
20/12/03 14:00:10 INFO CodeGenerator: Code generated in 24.263144 ms
20/12/03 14:00:10 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:00:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:00:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:35935 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:00:10 INFO SparkContext: Created broadcast 4 from count at NativeMethodAccessorImpl.java:0
20/12/03 14:00:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:00:10 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 14:00:10 INFO DAGScheduler: Registering RDD 10 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 14:00:10 INFO DAGScheduler: Got job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 14:00:10 INFO DAGScheduler: Final stage: ResultStage 3 (count at NativeMethodAccessorImpl.java:0)
20/12/03 14:00:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/03 14:00:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/03 14:00:10 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:00:10 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 14:00:10 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 14:00:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:35935 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:00:10 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/03 14:00:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:00:10 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/03 14:00:10 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:00:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:45947 (size: 8.0 KiB, free: 912.2 MiB)
20/12/03 14:00:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:45947 (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 14:00:11 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 590 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:00:11 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/03 14:00:11 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.622 s
20/12/03 14:00:11 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:00:11 INFO DAGScheduler: running: Set()
20/12/03 14:00:11 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/03 14:00:11 INFO DAGScheduler: failed: Set()
20/12/03 14:00:11 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:00:11 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/03 14:00:11 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/03 14:00:11 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:35935 (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:00:11 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1223
20/12/03 14:00:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:00:11 INFO YarnScheduler: Adding task set 3.0 with 1 tasks
20/12/03 14:00:11 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:00:11 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.9:42275 (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:00:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:52654
20/12/03 14:00:13 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1821 ms on 192.168.1.9 (executor 2) (1/1)
20/12/03 14:00:13 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/03 14:00:13 INFO DAGScheduler: ResultStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 1.836 s
20/12/03 14:00:13 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 14:00:13 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
20/12/03 14:00:13 INFO DAGScheduler: Job 2 finished: count at NativeMethodAccessorImpl.java:0, took 2.497505 s
20/12/03 14:00:14 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.9:35935 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:00:14 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.9:45947 in memory (size: 8.0 KiB, free: 912.2 MiB)
20/12/03 14:00:14 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.9:45947 in memory (size: 4.9 KiB, free: 912.2 MiB)
20/12/03 14:00:14 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.9:35935 in memory (size: 4.9 KiB, free: 5.8 GiB)
20/12/03 14:00:14 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:45947 in memory (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 14:00:15 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:35935 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:00:15 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.9:42275 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:00:15 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.9:35935 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:00:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:45947 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:00:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:35935 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:00:15 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:35935 in memory (size: 6.3 KiB, free: 5.8 GiB)
20/12/03 14:00:15 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:45947 in memory (size: 6.3 KiB, free: 912.3 MiB)
20/12/03 14:00:15 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/03 14:00:16 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:00:16 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:00:16 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:00:16 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:00:16 INFO CodeGenerator: Code generated in 28.87765 ms
20/12/03 14:00:16 INFO CodeGenerator: Code generated in 19.997042 ms
20/12/03 14:00:16 INFO CodeGenerator: Code generated in 20.481548 ms
20/12/03 14:00:17 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:00:17 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:00:17 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:35935 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:00:17 INFO SparkContext: Created broadcast 7 from toPandas at /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/userFiles-b70ca20e-d40b-45d9-999c-44739f062f34/darima.zip/darima/dlsa.py:22
20/12/03 14:00:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:00:17 INFO SparkContext: Starting job: toPandas at /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/userFiles-b70ca20e-d40b-45d9-999c-44739f062f34/darima.zip/darima/dlsa.py:22
20/12/03 14:00:17 INFO DAGScheduler: Registering RDD 20 (toPandas at /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/userFiles-b70ca20e-d40b-45d9-999c-44739f062f34/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/03 14:00:17 INFO DAGScheduler: Got job 3 (toPandas at /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/userFiles-b70ca20e-d40b-45d9-999c-44739f062f34/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/03 14:00:17 INFO DAGScheduler: Final stage: ResultStage 5 (toPandas at /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/userFiles-b70ca20e-d40b-45d9-999c-44739f062f34/darima.zip/darima/dlsa.py:22)
20/12/03 14:00:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
20/12/03 14:00:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)
20/12/03 14:00:17 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at toPandas at /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/userFiles-b70ca20e-d40b-45d9-999c-44739f062f34/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:00:17 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.2 KiB, free 5.8 GiB)
20/12/03 14:00:17 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/03 14:00:17 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:35935 (size: 11.6 KiB, free: 5.8 GiB)
20/12/03 14:00:17 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1223
20/12/03 14:00:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at toPandas at /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/userFiles-b70ca20e-d40b-45d9-999c-44739f062f34/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/03 14:00:17 INFO YarnScheduler: Adding task set 4.0 with 1 tasks
20/12/03 14:00:17 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:00:17 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.9:42275 (size: 11.6 KiB, free: 912.3 MiB)
20/12/03 14:00:18 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.9:42275 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:00:19 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 2650 ms on 192.168.1.9 (executor 2) (1/1)
20/12/03 14:00:19 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/12/03 14:00:19 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 49453
20/12/03 14:00:19 INFO DAGScheduler: ShuffleMapStage 4 (toPandas at /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/userFiles-b70ca20e-d40b-45d9-999c-44739f062f34/darima.zip/darima/dlsa.py:22) finished in 2.681 s
20/12/03 14:00:19 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:00:19 INFO DAGScheduler: running: Set()
20/12/03 14:00:19 INFO DAGScheduler: waiting: Set(ResultStage 5)
20/12/03 14:00:19 INFO DAGScheduler: failed: Set()
20/12/03 14:00:19 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at toPandas at /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/userFiles-b70ca20e-d40b-45d9-999c-44739f062f34/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:00:19 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 428.0 KiB, free 5.8 GiB)
20/12/03 14:00:19 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 131.4 KiB, free 5.8 GiB)
20/12/03 14:00:19 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:35935 (size: 131.4 KiB, free: 5.8 GiB)
20/12/03 14:00:19 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1223
20/12/03 14:00:19 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at toPandas at /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/userFiles-b70ca20e-d40b-45d9-999c-44739f062f34/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/03 14:00:19 INFO YarnScheduler: Adding task set 5.0 with 200 tasks
20/12/03 14:00:19 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:00:19 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:00:19 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:45947 (size: 131.4 KiB, free: 912.1 MiB)
20/12/03 14:00:20 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.9:42275 (size: 131.4 KiB, free: 912.1 MiB)
20/12/03 14:00:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:52650
20/12/03 14:00:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:52654
20/12/03 14:00:26 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 7, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:00:26 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/03 14:00:27 INFO TaskSetManager: Starting task 0.1 in stage 5.0 (TID 8, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:00:27 INFO TaskSetManager: Lost task 2.0 in stage 5.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/03 14:00:28 INFO TaskSetManager: Starting task 2.1 in stage 5.0 (TID 9, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:00:28 INFO TaskSetManager: Lost task 3.0 in stage 5.0 (TID 7) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/03 14:00:28 INFO TaskSetManager: Starting task 3.1 in stage 5.0 (TID 10, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:00:28 INFO TaskSetManager: Lost task 0.1 in stage 5.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/03 14:00:29 INFO TaskSetManager: Starting task 0.2 in stage 5.0 (TID 11, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:00:29 INFO TaskSetManager: Lost task 2.1 in stage 5.0 (TID 9) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/03 14:00:30 INFO TaskSetManager: Starting task 2.2 in stage 5.0 (TID 12, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:00:30 INFO TaskSetManager: Lost task 3.1 in stage 5.0 (TID 10) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/03 14:00:31 INFO TaskSetManager: Starting task 3.2 in stage 5.0 (TID 13, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:00:31 INFO TaskSetManager: Lost task 0.2 in stage 5.0 (TID 11) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/03 14:00:31 INFO TaskSetManager: Starting task 0.3 in stage 5.0 (TID 14, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:00:31 INFO TaskSetManager: Lost task 2.2 in stage 5.0 (TID 12) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/03 14:00:32 INFO TaskSetManager: Starting task 2.3 in stage 5.0 (TID 15, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:00:32 INFO TaskSetManager: Lost task 3.2 in stage 5.0 (TID 13) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/03 14:00:32 INFO TaskSetManager: Starting task 3.3 in stage 5.0 (TID 16, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:00:32 INFO TaskSetManager: Lost task 0.3 in stage 5.0 (TID 14) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 9]
20/12/03 14:00:32 ERROR TaskSetManager: Task 0 in stage 5.0 failed 4 times; aborting job
20/12/03 14:00:32 INFO YarnScheduler: Cancelling stage 5
20/12/03 14:00:32 INFO YarnScheduler: Killing all running tasks in stage 5: Stage cancelled
20/12/03 14:00:32 INFO YarnScheduler: Stage 5 was cancelled
20/12/03 14:00:32 INFO DAGScheduler: ResultStage 5 (toPandas at /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/userFiles-b70ca20e-d40b-45d9-999c-44739f062f34/darima.zip/darima/dlsa.py:22) failed in 12.945 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 14, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/03 14:00:32 INFO DAGScheduler: Job 3 failed: toPandas at /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/userFiles-b70ca20e-d40b-45d9-999c-44739f062f34/darima.zip/darima/dlsa.py:22, took 15.687552 s
20/12/03 14:00:32 WARN TaskSetManager: Lost task 3.3 in stage 5.0 (TID 16, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/03 14:00:33 WARN TaskSetManager: Lost task 2.3 in stage 5.0 (TID 15, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/03 14:00:33 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/12/03 14:00:33 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 14:00:33 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 14:00:33 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 14:00:33 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 14:00:33 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 14:00:33 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 14:00:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 14:00:33 INFO MemoryStore: MemoryStore cleared
20/12/03 14:00:33 INFO BlockManager: BlockManager stopped
20/12/03 14:00:33 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 14:00:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 14:00:33 INFO SparkContext: Successfully stopped SparkContext
20/12/03 14:00:33 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:00:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0
20/12/03 14:00:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-43a20386-4af1-4f14-9eaa-51b1c7fbdcf0/pyspark-419bf80d-b0f5-424d-910f-c6e2c7ff7203
20/12/03 14:00:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-c76a72c9-904a-463e-9136-3e575018b6fd
20/12/03 14:14:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:14:00 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:14:00 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:14:00 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:14:00 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:14:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:14:01 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:14:01 INFO ResourceUtils: ==============================================================
20/12/03 14:14:01 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:14:01 INFO ResourceUtils: ==============================================================
20/12/03 14:14:01 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:14:01 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:14:01 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:14:01 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:14:01 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:14:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:14:01 INFO Utils: Successfully started service 'sparkDriver' on port 35583.
20/12/03 14:14:01 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:14:01 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:14:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:14:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:14:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:14:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b548c85b-44a6-465d-9116-6e638f49ef47
20/12/03 14:14:02 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:14:02 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:14:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:14:02 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:14:03 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:14:03 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:14:04 INFO Configuration: resource-types.xml not found
20/12/03 14:14:04 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:14:04 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:14:04 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:14:04 INFO Client: Setting up container launch context for our AM
20/12/03 14:14:04 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:14:04 INFO Client: Preparing resources for our AM container
20/12/03 14:14:04 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:14:06 INFO Client: Uploading resource file:/tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/__spark_libs__11283731967665413024.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0023/__spark_libs__11283731967665413024.zip
20/12/03 14:14:06 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0023/pyspark.zip
20/12/03 14:14:06 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0023/py4j-0.10.9-src.zip
20/12/03 14:14:07 INFO Client: Uploading resource file:/tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/__spark_conf__14561670779345394980.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0023/__spark_conf__.zip
20/12/03 14:14:07 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:14:07 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:14:07 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:14:07 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:14:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:14:07 INFO Client: Submitting application application_1607015337794_0023 to ResourceManager
20/12/03 14:14:07 INFO YarnClientImpl: Submitted application application_1607015337794_0023
20/12/03 14:14:08 INFO Client: Application report for application_1607015337794_0023 (state: ACCEPTED)
20/12/03 14:14:08 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607022847271
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0023/
	 user: bsuconn
20/12/03 14:14:09 INFO Client: Application report for application_1607015337794_0023 (state: ACCEPTED)
20/12/03 14:14:10 INFO Client: Application report for application_1607015337794_0023 (state: ACCEPTED)
20/12/03 14:14:11 INFO Client: Application report for application_1607015337794_0023 (state: ACCEPTED)
20/12/03 14:14:12 INFO Client: Application report for application_1607015337794_0023 (state: RUNNING)
20/12/03 14:14:12 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607022847271
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0023/
	 user: bsuconn
20/12/03 14:14:12 INFO YarnClientSchedulerBackend: Application application_1607015337794_0023 has started running.
20/12/03 14:14:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38235.
20/12/03 14:14:12 INFO NettyBlockTransferService: Server created on 192.168.1.9:38235
20/12/03 14:14:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 14:14:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 38235, None)
20/12/03 14:14:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:38235 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 38235, None)
20/12/03 14:14:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 38235, None)
20/12/03 14:14:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 38235, None)
20/12/03 14:14:12 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0023), /proxy/application_1607015337794_0023
20/12/03 14:14:12 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:14:13 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 14:14:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 14:14:17 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:39386) with ID 1
20/12/03 14:14:17 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/03 14:14:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:36391 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 36391, None)
20/12/03 14:14:17 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 14:14:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 14:14:17 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 14:14:17 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:14:17 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:14:17 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:14:17 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:14:17 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:14:18 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:35583/files/darima.zip with timestamp 1607022858399
20/12/03 14:14:18 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/userFiles-48473071-8b42-435c-aab8-363696af1f72/darima.zip
20/12/03 14:14:20 INFO InMemoryFileIndex: It took 68 ms to list leaf files for 1 paths.
20/12/03 14:14:22 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/12/03 14:14:22 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:14:22 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:14:22 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:14:22 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:14:23 INFO CodeGenerator: Code generated in 258.650715 ms
20/12/03 14:14:23 INFO CodeGenerator: Code generated in 32.924839 ms
20/12/03 14:14:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:14:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:14:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:38235 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:14:24 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/03 14:14:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:14:24 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 14:14:24 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 14:14:24 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 14:14:24 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/03 14:14:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/03 14:14:24 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/03 14:14:24 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:14:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 14:14:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 14:14:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:38235 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:14:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 14:14:24 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:14:24 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 14:14:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:14:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:36391 (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:14:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:36391 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:14:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3437 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:14:27 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/03 14:14:28 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.622 s
20/12/03 14:14:28 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:14:28 INFO DAGScheduler: running: Set()
20/12/03 14:14:28 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/03 14:14:28 INFO DAGScheduler: failed: Set()
20/12/03 14:14:28 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:14:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/03 14:14:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/03 14:14:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:38235 (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:14:28 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/03 14:14:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:14:28 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/03 14:14:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:14:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:36391 (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:14:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:39386
20/12/03 14:14:28 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 324 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:14:28 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/03 14:14:28 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.340 s
20/12/03 14:14:28 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 14:14:28 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/03 14:14:28 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 4.068915 s
20/12/03 14:14:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:38235 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:14:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:36391 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:14:30 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:38235 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:14:30 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:36391 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:14:30 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:36391 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:14:30 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:38235 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:14:30 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/03 14:14:31 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:14:31 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:14:31 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:14:31 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:14:31 INFO CodeGenerator: Code generated in 31.173602 ms
20/12/03 14:14:32 INFO CodeGenerator: Code generated in 15.529811 ms
20/12/03 14:14:32 INFO CodeGenerator: Code generated in 23.204094 ms
20/12/03 14:14:32 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:14:32 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:14:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:38235 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:14:32 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/userFiles-48473071-8b42-435c-aab8-363696af1f72/darima.zip/darima/dlsa.py:22
20/12/03 14:14:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:14:32 INFO SparkContext: Starting job: toPandas at /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/userFiles-48473071-8b42-435c-aab8-363696af1f72/darima.zip/darima/dlsa.py:22
20/12/03 14:14:32 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/userFiles-48473071-8b42-435c-aab8-363696af1f72/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/03 14:14:32 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/userFiles-48473071-8b42-435c-aab8-363696af1f72/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/03 14:14:32 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/userFiles-48473071-8b42-435c-aab8-363696af1f72/darima.zip/darima/dlsa.py:22)
20/12/03 14:14:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/03 14:14:32 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/03 14:14:32 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/userFiles-48473071-8b42-435c-aab8-363696af1f72/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:14:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/03 14:14:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/03 14:14:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:38235 (size: 11.6 KiB, free: 5.8 GiB)
20/12/03 14:14:32 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/03 14:14:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/userFiles-48473071-8b42-435c-aab8-363696af1f72/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/03 14:14:32 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/03 14:14:32 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:14:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:36391 (size: 11.6 KiB, free: 912.3 MiB)
20/12/03 14:14:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:36391 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:14:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1238 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:14:33 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/03 14:14:33 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 54711
20/12/03 14:14:33 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/userFiles-48473071-8b42-435c-aab8-363696af1f72/darima.zip/darima/dlsa.py:22) finished in 1.262 s
20/12/03 14:14:33 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:14:33 INFO DAGScheduler: running: Set()
20/12/03 14:14:33 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/03 14:14:33 INFO DAGScheduler: failed: Set()
20/12/03 14:14:33 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/userFiles-48473071-8b42-435c-aab8-363696af1f72/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:14:33 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/03 14:14:33 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/03 14:14:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:38235 (size: 131.5 KiB, free: 5.8 GiB)
20/12/03 14:14:33 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/03 14:14:33 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/userFiles-48473071-8b42-435c-aab8-363696af1f72/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/03 14:14:33 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/03 14:14:33 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:14:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:36391 (size: 131.5 KiB, free: 912.1 MiB)
20/12/03 14:14:33 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:39386
20/12/03 14:14:36 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:14:36 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/03 14:14:37 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:14:37 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/03 14:14:38 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:14:38 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/03 14:14:39 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:14:39 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/03 14:14:40 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:14:40 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/03 14:14:40 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:14:40 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/03 14:14:41 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:14:41 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/03 14:14:41 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/03 14:14:41 INFO YarnScheduler: Cancelling stage 3
20/12/03 14:14:41 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/03 14:14:41 INFO YarnScheduler: Stage 3 was cancelled
20/12/03 14:14:41 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/userFiles-48473071-8b42-435c-aab8-363696af1f72/darima.zip/darima/dlsa.py:22) failed in 7.985 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/03 14:14:41 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/userFiles-48473071-8b42-435c-aab8-363696af1f72/darima.zip/darima/dlsa.py:22, took 9.302627 s
20/12/03 14:14:41 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/03 14:14:41 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/03 14:14:41 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 14:14:41 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 14:14:41 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 14:14:42 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 14:14:42 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 14:14:42 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 14:14:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 14:14:42 INFO MemoryStore: MemoryStore cleared
20/12/03 14:14:42 INFO BlockManager: BlockManager stopped
20/12/03 14:14:42 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 14:14:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 14:14:42 INFO SparkContext: Successfully stopped SparkContext
20/12/03 14:14:42 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:14:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7
20/12/03 14:14:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-987bf55c-4bc2-4b7c-bb3e-e4be8dcc5bc7/pyspark-bd67a91f-1899-41e3-87f5-976b32375297
20/12/03 14:14:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-6fe171a9-2cfd-4b89-8643-23fc0a831cb1
20/12/03 14:14:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:14:44 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:14:44 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:14:44 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:14:44 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:14:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:14:45 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:14:45 INFO ResourceUtils: ==============================================================
20/12/03 14:14:45 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:14:45 INFO ResourceUtils: ==============================================================
20/12/03 14:14:45 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:14:45 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:14:45 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:14:45 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:14:45 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:14:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:14:45 INFO Utils: Successfully started service 'sparkDriver' on port 41997.
20/12/03 14:14:45 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:14:45 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:14:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:14:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:14:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:14:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-474f83ab-c900-451d-9194-74525dfa8b60
20/12/03 14:14:45 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:14:46 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:14:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:14:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:14:46 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:14:47 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:14:47 INFO Configuration: resource-types.xml not found
20/12/03 14:14:47 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:14:47 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:14:47 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:14:47 INFO Client: Setting up container launch context for our AM
20/12/03 14:14:47 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:14:47 INFO Client: Preparing resources for our AM container
20/12/03 14:14:47 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:14:49 INFO Client: Uploading resource file:/tmp/spark-3b34e64c-1622-44c2-8047-c20af5681a2c/__spark_libs__11710359735550475305.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0024/__spark_libs__11710359735550475305.zip
20/12/03 14:14:50 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0024/pyspark.zip
20/12/03 14:14:50 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0024/py4j-0.10.9-src.zip
20/12/03 14:14:51 INFO Client: Uploading resource file:/tmp/spark-3b34e64c-1622-44c2-8047-c20af5681a2c/__spark_conf__9710231660346208010.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0024/__spark_conf__.zip
20/12/03 14:14:51 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:14:51 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:14:51 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:14:51 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:14:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:14:51 INFO Client: Submitting application application_1607015337794_0024 to ResourceManager
20/12/03 14:14:51 INFO YarnClientImpl: Submitted application application_1607015337794_0024
20/12/03 14:14:52 INFO Client: Application report for application_1607015337794_0024 (state: ACCEPTED)
20/12/03 14:14:52 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607022891152
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0024/
	 user: bsuconn
20/12/03 14:14:53 INFO Client: Application report for application_1607015337794_0024 (state: ACCEPTED)
20/12/03 14:14:54 INFO Client: Application report for application_1607015337794_0024 (state: ACCEPTED)
20/12/03 14:14:55 INFO Client: Application report for application_1607015337794_0024 (state: ACCEPTED)
20/12/03 14:14:56 INFO Client: Application report for application_1607015337794_0024 (state: RUNNING)
20/12/03 14:14:56 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607022891152
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0024/
	 user: bsuconn
20/12/03 14:14:56 INFO YarnClientSchedulerBackend: Application application_1607015337794_0024 has started running.
20/12/03 14:14:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43921.
20/12/03 14:14:56 INFO NettyBlockTransferService: Server created on 192.168.1.9:43921
20/12/03 14:14:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 14:14:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 43921, None)
20/12/03 14:14:56 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:43921 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 43921, None)
20/12/03 14:14:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 43921, None)
20/12/03 14:14:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 43921, None)
20/12/03 14:14:56 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0024), /proxy/application_1607015337794_0024
20/12/03 14:14:56 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:14:57 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 14:15:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 14:15:02 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:46674) with ID 1
20/12/03 14:15:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:34139 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 34139, None)
20/12/03 14:15:03 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:46678) with ID 2
20/12/03 14:15:03 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/03 14:15:03 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:45373 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 45373, None)
20/12/03 14:15:03 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 14:15:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 14:15:03 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 14:15:03 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:15:03 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:15:03 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:15:03 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:15:03 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:15:04 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:41997/files/darima.zip with timestamp 1607022904472
20/12/03 14:15:04 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-3b34e64c-1622-44c2-8047-c20af5681a2c/userFiles-b5123094-5c5c-41d5-a686-bf8909e9c52a/darima.zip
20/12/03 14:15:06 INFO InMemoryFileIndex: It took 68 ms to list leaf files for 1 paths.
20/12/03 14:15:07 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/12/03 14:15:08 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:15:08 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:15:08 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:15:08 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:15:09 INFO CodeGenerator: Code generated in 278.163361 ms
20/12/03 14:15:09 INFO CodeGenerator: Code generated in 33.391194 ms
20/12/03 14:15:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:15:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:15:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:43921 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:15:09 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/03 14:15:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:15:09 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 14:15:09 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 14:15:09 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 14:15:09 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/03 14:15:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/03 14:15:09 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/03 14:15:09 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:15:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 14:15:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 14:15:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:43921 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:15:09 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 14:15:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:15:09 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 14:15:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:15:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:45373 (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:15:12 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 14:15:12 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 14:15:12 INFO DAGScheduler: Job 0 failed: count at NativeMethodAccessorImpl.java:0, took 2.380895 s
20/12/03 14:15:12 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) failed in 2.314 s due to Stage cancelled because SparkContext was shut down
20/12/03 14:15:12 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 14:15:12 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 14:15:12 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 14:15:12 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 14:15:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 14:15:12 INFO MemoryStore: MemoryStore cleared
20/12/03 14:15:12 INFO BlockManager: BlockManager stopped
20/12/03 14:15:12 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 14:15:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 14:15:12 INFO SparkContext: Successfully stopped SparkContext
20/12/03 14:15:12 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:15:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-3b34e64c-1622-44c2-8047-c20af5681a2c
20/12/03 14:15:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-3b34e64c-1622-44c2-8047-c20af5681a2c/pyspark-3d421369-7f08-4274-b0bd-d4da055ce642
20/12/03 14:15:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-e810f79e-12c3-47ad-a39e-9a62cec5c994
20/12/03 14:35:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:35:46 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:35:46 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:35:46 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:35:46 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:35:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:35:47 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:35:47 INFO ResourceUtils: ==============================================================
20/12/03 14:35:47 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:35:47 INFO ResourceUtils: ==============================================================
20/12/03 14:35:47 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:35:47 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:35:47 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:35:47 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:35:47 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:35:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:35:47 INFO Utils: Successfully started service 'sparkDriver' on port 40121.
20/12/03 14:35:48 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:35:48 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:35:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:35:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:35:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:35:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-58b9117a-1bab-4a48-8094-fecfe5172867
20/12/03 14:35:48 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:35:48 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:35:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:35:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:35:48 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:35:49 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:35:49 INFO Configuration: resource-types.xml not found
20/12/03 14:35:49 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:35:49 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:35:49 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:35:49 INFO Client: Setting up container launch context for our AM
20/12/03 14:35:49 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:35:49 INFO Client: Preparing resources for our AM container
20/12/03 14:35:49 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:35:51 INFO Client: Uploading resource file:/tmp/spark-2cceac0b-afa0-487d-8d06-65f20dadedf4/__spark_libs__7021012643882355303.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0025/__spark_libs__7021012643882355303.zip
20/12/03 14:35:52 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0025/pyspark.zip
20/12/03 14:35:53 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0025/py4j-0.10.9-src.zip
20/12/03 14:35:53 INFO Client: Uploading resource file:/tmp/spark-2cceac0b-afa0-487d-8d06-65f20dadedf4/__spark_conf__9044860768961994106.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0025/__spark_conf__.zip
20/12/03 14:35:53 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:35:53 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:35:53 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:35:53 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:35:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:35:53 INFO Client: Submitting application application_1607015337794_0025 to ResourceManager
20/12/03 14:35:53 INFO YarnClientImpl: Submitted application application_1607015337794_0025
20/12/03 14:35:54 INFO Client: Application report for application_1607015337794_0025 (state: ACCEPTED)
20/12/03 14:35:54 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024153858
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0025/
	 user: bsuconn
20/12/03 14:35:55 INFO Client: Application report for application_1607015337794_0025 (state: ACCEPTED)
20/12/03 14:35:56 INFO Client: Application report for application_1607015337794_0025 (state: ACCEPTED)
20/12/03 14:35:57 INFO Client: Application report for application_1607015337794_0025 (state: ACCEPTED)
20/12/03 14:35:58 INFO Client: Application report for application_1607015337794_0025 (state: RUNNING)
20/12/03 14:35:58 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024153858
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0025/
	 user: bsuconn
20/12/03 14:35:58 INFO YarnClientSchedulerBackend: Application application_1607015337794_0025 has started running.
20/12/03 14:35:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45475.
20/12/03 14:35:58 INFO NettyBlockTransferService: Server created on 192.168.1.9:45475
20/12/03 14:35:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 14:35:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 45475, None)
20/12/03 14:35:58 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:45475 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 45475, None)
20/12/03 14:35:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 45475, None)
20/12/03 14:35:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 45475, None)
20/12/03 14:35:59 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0025), /proxy/application_1607015337794_0025
20/12/03 14:35:59 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:36:00 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 14:36:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 14:36:04 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:34896) with ID 1
20/12/03 14:36:04 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/03 14:36:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:44035 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 44035, None)
20/12/03 14:36:04 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 14:36:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 14:36:04 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 14:36:04 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:36:04 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:36:04 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:36:04 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:36:04 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:36:05 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:40121/files/darima.zip with timestamp 1607024165474
20/12/03 14:36:05 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-2cceac0b-afa0-487d-8d06-65f20dadedf4/userFiles-2f989acf-8bc3-41fe-bf9b-76e9c83315e1/darima.zip
20/12/03 14:36:07 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 14:36:07 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 14:36:07 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 14:36:07 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 14:36:07 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 14:36:07 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 14:36:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 14:36:07 INFO MemoryStore: MemoryStore cleared
20/12/03 14:36:07 INFO BlockManager: BlockManager stopped
20/12/03 14:36:07 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 14:36:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 14:36:07 INFO SparkContext: Successfully stopped SparkContext
20/12/03 14:36:07 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:36:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-610ad96c-07ce-402c-8547-dbad52ce6774
20/12/03 14:36:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-2cceac0b-afa0-487d-8d06-65f20dadedf4
20/12/03 14:36:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-2cceac0b-afa0-487d-8d06-65f20dadedf4/pyspark-e0dfcb00-f7ac-4174-923e-806014b33977
20/12/03 14:36:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:36:10 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:36:10 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:36:10 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:36:10 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:36:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:36:10 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:36:10 INFO ResourceUtils: ==============================================================
20/12/03 14:36:10 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:36:10 INFO ResourceUtils: ==============================================================
20/12/03 14:36:10 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:36:10 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:36:10 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:36:10 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:36:10 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:36:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:36:11 INFO Utils: Successfully started service 'sparkDriver' on port 40797.
20/12/03 14:36:11 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:36:11 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:36:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:36:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:36:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:36:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-646a912c-fde4-4a00-9b93-24000f9b419e
20/12/03 14:36:11 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:36:11 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:36:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:36:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:36:12 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:36:12 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:36:12 INFO Configuration: resource-types.xml not found
20/12/03 14:36:12 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:36:12 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:36:12 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:36:12 INFO Client: Setting up container launch context for our AM
20/12/03 14:36:12 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:36:12 INFO Client: Preparing resources for our AM container
20/12/03 14:36:13 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:36:14 INFO Client: Uploading resource file:/tmp/spark-5c6a22a8-e63e-4e91-a616-b84846c2b4e6/__spark_libs__3431064052182009608.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0026/__spark_libs__3431064052182009608.zip
20/12/03 14:36:16 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0026/pyspark.zip
20/12/03 14:36:16 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0026/py4j-0.10.9-src.zip
20/12/03 14:36:16 INFO Client: Uploading resource file:/tmp/spark-5c6a22a8-e63e-4e91-a616-b84846c2b4e6/__spark_conf__14633844346469496627.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0026/__spark_conf__.zip
20/12/03 14:36:17 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:36:17 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:36:17 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:36:17 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:36:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:36:17 INFO Client: Submitting application application_1607015337794_0026 to ResourceManager
20/12/03 14:36:17 INFO YarnClientImpl: Submitted application application_1607015337794_0026
20/12/03 14:36:18 INFO Client: Application report for application_1607015337794_0026 (state: ACCEPTED)
20/12/03 14:36:18 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024177160
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0026/
	 user: bsuconn
20/12/03 14:36:19 INFO Client: Application report for application_1607015337794_0026 (state: ACCEPTED)
20/12/03 14:36:20 INFO Client: Application report for application_1607015337794_0026 (state: ACCEPTED)
20/12/03 14:36:21 INFO Client: Application report for application_1607015337794_0026 (state: ACCEPTED)
20/12/03 14:36:22 INFO Client: Application report for application_1607015337794_0026 (state: ACCEPTED)
20/12/03 14:36:22 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0026), /proxy/application_1607015337794_0026
20/12/03 14:36:23 INFO Client: Application report for application_1607015337794_0026 (state: RUNNING)
20/12/03 14:36:23 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024177160
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0026/
	 user: bsuconn
20/12/03 14:36:23 INFO YarnClientSchedulerBackend: Application application_1607015337794_0026 has started running.
20/12/03 14:36:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40199.
20/12/03 14:36:23 INFO NettyBlockTransferService: Server created on 192.168.1.9:40199
20/12/03 14:36:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 14:36:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 40199, None)
20/12/03 14:36:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40199 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 40199, None)
20/12/03 14:36:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 40199, None)
20/12/03 14:36:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 40199, None)
20/12/03 14:36:23 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:36:23 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 14:36:27 INFO DiskBlockManager: Shutdown hook called
20/12/03 14:36:27 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:36:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-4d80b603-d13a-4fd4-84fe-039127a0bf46
20/12/03 14:36:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-5c6a22a8-e63e-4e91-a616-b84846c2b4e6
20/12/03 14:36:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-5c6a22a8-e63e-4e91-a616-b84846c2b4e6/userFiles-0ed72163-7908-407f-b577-473bba36e358
20/12/03 14:36:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:36:33 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:36:33 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:36:33 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:36:33 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:36:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:36:34 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:36:34 INFO ResourceUtils: ==============================================================
20/12/03 14:36:34 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:36:34 INFO ResourceUtils: ==============================================================
20/12/03 14:36:34 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:36:34 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:36:34 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:36:34 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:36:34 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:36:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:36:35 INFO Utils: Successfully started service 'sparkDriver' on port 42473.
20/12/03 14:36:35 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:36:35 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:36:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:36:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:36:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:36:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f9983202-b56e-482c-a517-65dbb6b9f7f4
20/12/03 14:36:35 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:36:35 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:36:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:36:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:36:36 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:36:36 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:36:37 INFO Configuration: resource-types.xml not found
20/12/03 14:36:37 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:36:37 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:36:37 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:36:37 INFO Client: Setting up container launch context for our AM
20/12/03 14:36:37 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:36:37 INFO Client: Preparing resources for our AM container
20/12/03 14:36:37 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:36:38 INFO DiskBlockManager: Shutdown hook called
20/12/03 14:36:38 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:36:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-16758cd7-2660-46e7-9860-d0bd50bf1d01
20/12/03 14:36:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-79ada54f-0540-4b22-a26a-c67f85c7e986
20/12/03 14:36:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-79ada54f-0540-4b22-a26a-c67f85c7e986/userFiles-32285a31-a33b-4a9f-b671-28fa02e1d1b7
20/12/03 14:37:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:37:16 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:37:16 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:37:16 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:37:16 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:37:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:37:17 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:37:17 INFO ResourceUtils: ==============================================================
20/12/03 14:37:17 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:37:17 INFO ResourceUtils: ==============================================================
20/12/03 14:37:17 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:37:17 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:37:17 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:37:17 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:37:17 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:37:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:37:18 INFO Utils: Successfully started service 'sparkDriver' on port 42221.
20/12/03 14:37:18 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:37:18 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:37:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:37:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:37:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:37:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f6d785f8-c46a-41d3-9449-2848211935d0
20/12/03 14:37:18 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:37:18 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:37:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:37:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:37:19 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:37:19 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:37:20 INFO Configuration: resource-types.xml not found
20/12/03 14:37:20 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:37:20 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:37:20 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:37:20 INFO Client: Setting up container launch context for our AM
20/12/03 14:37:20 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:37:20 INFO Client: Preparing resources for our AM container
20/12/03 14:37:20 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:37:23 INFO Client: Uploading resource file:/tmp/spark-1d7b2206-e32f-4c7f-aaba-88ab7141fb71/__spark_libs__18080114938934522063.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0028/__spark_libs__18080114938934522063.zip
20/12/03 14:37:24 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0028/pyspark.zip
20/12/03 14:37:24 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0028/py4j-0.10.9-src.zip
20/12/03 14:37:24 INFO Client: Uploading resource file:/tmp/spark-1d7b2206-e32f-4c7f-aaba-88ab7141fb71/__spark_conf__1426916506664799470.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0028/__spark_conf__.zip
20/12/03 14:37:24 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:37:24 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:37:24 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:37:24 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:37:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:37:24 INFO Client: Submitting application application_1607015337794_0028 to ResourceManager
20/12/03 14:37:24 INFO YarnClientImpl: Submitted application application_1607015337794_0028
20/12/03 14:37:25 INFO Client: Application report for application_1607015337794_0028 (state: ACCEPTED)
20/12/03 14:37:25 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024244499
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0028/
	 user: bsuconn
20/12/03 14:37:26 INFO Client: Application report for application_1607015337794_0028 (state: ACCEPTED)
20/12/03 14:37:27 INFO Client: Application report for application_1607015337794_0028 (state: ACCEPTED)
20/12/03 14:37:28 INFO Client: Application report for application_1607015337794_0028 (state: ACCEPTED)
20/12/03 14:37:29 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0028), /proxy/application_1607015337794_0028
20/12/03 14:37:29 INFO Client: Application report for application_1607015337794_0028 (state: RUNNING)
20/12/03 14:37:29 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024244499
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0028/
	 user: bsuconn
20/12/03 14:37:29 INFO YarnClientSchedulerBackend: Application application_1607015337794_0028 has started running.
20/12/03 14:37:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45585.
20/12/03 14:37:29 INFO NettyBlockTransferService: Server created on 192.168.1.9:45585
20/12/03 14:37:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 14:37:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 45585, None)
20/12/03 14:37:29 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:45585 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 45585, None)
20/12/03 14:37:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 45585, None)
20/12/03 14:37:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 45585, None)
20/12/03 14:37:29 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:37:30 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 14:37:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 14:37:34 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:37784) with ID 1
20/12/03 14:37:34 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/03 14:37:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40141 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 40141, None)
20/12/03 14:37:34 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 14:37:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 14:37:34 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 14:37:34 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:37:34 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:37:34 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:37:34 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:37:34 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:37:35 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:42221/files/darima.zip with timestamp 1607024255168
20/12/03 14:37:35 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-1d7b2206-e32f-4c7f-aaba-88ab7141fb71/userFiles-3df30b98-42a6-47a9-8ca7-64b8abf4bbf7/darima.zip
20/12/03 14:37:36 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 14:37:36 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 14:37:36 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 14:37:36 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 14:37:36 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 14:37:36 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 14:37:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 14:37:37 INFO MemoryStore: MemoryStore cleared
20/12/03 14:37:37 INFO BlockManager: BlockManager stopped
20/12/03 14:37:37 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 14:37:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 14:37:37 INFO SparkContext: Successfully stopped SparkContext
20/12/03 14:37:37 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:37:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-1d7b2206-e32f-4c7f-aaba-88ab7141fb71
20/12/03 14:37:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-1d7b2206-e32f-4c7f-aaba-88ab7141fb71/pyspark-0c8938a2-aade-4680-b071-37a443ed0e3f
20/12/03 14:37:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-47ab48b6-8e03-4d80-908e-620902c04a48
20/12/03 14:37:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:37:39 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:37:39 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:37:39 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:37:39 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:37:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:37:40 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:37:40 INFO ResourceUtils: ==============================================================
20/12/03 14:37:40 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:37:40 INFO ResourceUtils: ==============================================================
20/12/03 14:37:40 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:37:40 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:37:40 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:37:40 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:37:40 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:37:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:37:40 INFO Utils: Successfully started service 'sparkDriver' on port 34325.
20/12/03 14:37:40 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:37:40 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:37:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:37:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:37:40 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:37:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-08f775eb-5a9d-4933-83d1-8895f122ab02
20/12/03 14:37:40 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:37:41 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:37:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:37:41 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:37:41 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:37:42 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:37:42 INFO Configuration: resource-types.xml not found
20/12/03 14:37:42 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:37:42 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:37:42 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:37:42 INFO Client: Setting up container launch context for our AM
20/12/03 14:37:42 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:37:42 INFO Client: Preparing resources for our AM container
20/12/03 14:37:42 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:37:44 INFO Client: Uploading resource file:/tmp/spark-4767fa9f-fd50-4b3b-9345-0904becf48f4/__spark_libs__16543712150960004771.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0029/__spark_libs__16543712150960004771.zip
20/12/03 14:37:45 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0029/pyspark.zip
20/12/03 14:37:45 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0029/py4j-0.10.9-src.zip
20/12/03 14:37:46 INFO Client: Uploading resource file:/tmp/spark-4767fa9f-fd50-4b3b-9345-0904becf48f4/__spark_conf__4422418029925647186.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0029/__spark_conf__.zip
20/12/03 14:37:46 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:37:46 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:37:46 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:37:46 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:37:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:37:46 INFO Client: Submitting application application_1607015337794_0029 to ResourceManager
20/12/03 14:37:46 INFO YarnClientImpl: Submitted application application_1607015337794_0029
20/12/03 14:37:47 INFO Client: Application report for application_1607015337794_0029 (state: ACCEPTED)
20/12/03 14:37:47 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024266225
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0029/
	 user: bsuconn
20/12/03 14:37:48 INFO Client: Application report for application_1607015337794_0029 (state: ACCEPTED)
20/12/03 14:37:49 INFO Client: Application report for application_1607015337794_0029 (state: ACCEPTED)
20/12/03 14:37:50 INFO Client: Application report for application_1607015337794_0029 (state: ACCEPTED)
20/12/03 14:37:51 INFO Client: Application report for application_1607015337794_0029 (state: ACCEPTED)
20/12/03 14:37:51 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0029), /proxy/application_1607015337794_0029
20/12/03 14:37:52 INFO Client: Application report for application_1607015337794_0029 (state: RUNNING)
20/12/03 14:37:52 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024266225
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0029/
	 user: bsuconn
20/12/03 14:37:52 INFO YarnClientSchedulerBackend: Application application_1607015337794_0029 has started running.
20/12/03 14:37:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41697.
20/12/03 14:37:52 INFO NettyBlockTransferService: Server created on 192.168.1.9:41697
20/12/03 14:37:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 14:37:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 41697, None)
20/12/03 14:37:52 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:41697 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 41697, None)
20/12/03 14:37:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 41697, None)
20/12/03 14:37:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 41697, None)
20/12/03 14:37:52 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:37:52 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 14:37:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 14:37:57 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:49822) with ID 1
20/12/03 14:37:58 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:41067 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 41067, None)
20/12/03 14:37:58 INFO DiskBlockManager: Shutdown hook called
20/12/03 14:37:58 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:37:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-4767fa9f-fd50-4b3b-9345-0904becf48f4
20/12/03 14:37:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-af8b408f-ee53-4543-b44d-eee28203cbd4
20/12/03 14:37:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-4767fa9f-fd50-4b3b-9345-0904becf48f4/userFiles-1df6b138-2906-4a42-8247-c363924afd60
20/12/03 14:38:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:38:10 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:38:10 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:38:10 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:38:10 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:38:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:38:10 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:38:10 INFO ResourceUtils: ==============================================================
20/12/03 14:38:11 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:38:11 INFO ResourceUtils: ==============================================================
20/12/03 14:38:11 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:38:11 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:38:11 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:38:11 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:38:11 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:38:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:38:11 INFO Utils: Successfully started service 'sparkDriver' on port 44397.
20/12/03 14:38:11 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:38:11 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:38:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:38:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:38:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:38:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f3e7c449-8e1d-4cdf-a697-a54f5af3e24d
20/12/03 14:38:11 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:38:11 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:38:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:38:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:38:12 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:38:12 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:38:13 INFO Configuration: resource-types.xml not found
20/12/03 14:38:13 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:38:13 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:38:13 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:38:13 INFO Client: Setting up container launch context for our AM
20/12/03 14:38:13 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:38:13 INFO Client: Preparing resources for our AM container
20/12/03 14:38:13 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:38:15 INFO Client: Uploading resource file:/tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/__spark_libs__10517782528497610298.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0030/__spark_libs__10517782528497610298.zip
20/12/03 14:38:16 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0030/pyspark.zip
20/12/03 14:38:16 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0030/py4j-0.10.9-src.zip
20/12/03 14:38:16 INFO Client: Uploading resource file:/tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/__spark_conf__14728815816515752310.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0030/__spark_conf__.zip
20/12/03 14:38:16 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:38:16 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:38:16 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:38:16 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:38:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:38:16 INFO Client: Submitting application application_1607015337794_0030 to ResourceManager
20/12/03 14:38:16 INFO YarnClientImpl: Submitted application application_1607015337794_0030
20/12/03 14:38:17 INFO Client: Application report for application_1607015337794_0030 (state: ACCEPTED)
20/12/03 14:38:17 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024296519
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0030/
	 user: bsuconn
20/12/03 14:38:18 INFO Client: Application report for application_1607015337794_0030 (state: ACCEPTED)
20/12/03 14:38:19 INFO Client: Application report for application_1607015337794_0030 (state: ACCEPTED)
20/12/03 14:38:20 INFO Client: Application report for application_1607015337794_0030 (state: ACCEPTED)
20/12/03 14:38:21 INFO Client: Application report for application_1607015337794_0030 (state: RUNNING)
20/12/03 14:38:21 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024296519
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0030/
	 user: bsuconn
20/12/03 14:38:21 INFO YarnClientSchedulerBackend: Application application_1607015337794_0030 has started running.
20/12/03 14:38:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42365.
20/12/03 14:38:21 INFO NettyBlockTransferService: Server created on 192.168.1.9:42365
20/12/03 14:38:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 14:38:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 42365, None)
20/12/03 14:38:21 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:42365 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 42365, None)
20/12/03 14:38:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 42365, None)
20/12/03 14:38:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 42365, None)
20/12/03 14:38:21 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0030), /proxy/application_1607015337794_0030
20/12/03 14:38:22 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 14:38:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 14:38:26 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:48786) with ID 1
20/12/03 14:38:26 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/03 14:38:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:33963 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 33963, None)
20/12/03 14:38:26 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 14:38:26 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 14:38:26 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 14:38:26 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:38:26 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:38:26 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:38:26 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:38:26 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:38:27 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:44397/files/darima.zip with timestamp 1607024307753
20/12/03 14:38:27 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/userFiles-86de8731-b10c-40a7-8198-5b6b1d862d6a/darima.zip
20/12/03 14:38:30 INFO InMemoryFileIndex: It took 56 ms to list leaf files for 1 paths.
20/12/03 14:38:31 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
20/12/03 14:38:32 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:38:32 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:38:32 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:38:32 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:38:33 INFO CodeGenerator: Code generated in 302.43721 ms
20/12/03 14:38:33 INFO CodeGenerator: Code generated in 29.495407 ms
20/12/03 14:38:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:38:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:38:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:42365 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:38:33 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/03 14:38:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:38:33 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 14:38:33 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 14:38:33 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 14:38:33 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/03 14:38:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/03 14:38:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/03 14:38:33 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:38:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 14:38:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 14:38:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:42365 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:38:33 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 14:38:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:38:33 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 14:38:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:38:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:33963 (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:38:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:33963 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:38:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3316 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:38:37 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/03 14:38:37 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.453 s
20/12/03 14:38:37 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:38:37 INFO DAGScheduler: running: Set()
20/12/03 14:38:37 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/03 14:38:37 INFO DAGScheduler: failed: Set()
20/12/03 14:38:37 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:38:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/03 14:38:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/03 14:38:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:42365 (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:38:37 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/03 14:38:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:38:37 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/03 14:38:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:38:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:33963 (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:38:37 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:48786
20/12/03 14:38:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 389 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:38:37 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/03 14:38:37 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.405 s
20/12/03 14:38:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 14:38:37 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/03 14:38:37 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.966757 s
20/12/03 14:38:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/03 14:38:39 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:42365 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:38:39 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:33963 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:38:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:42365 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:38:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:33963 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:38:40 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:38:40 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:38:40 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:38:40 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:38:40 INFO CodeGenerator: Code generated in 22.703743 ms
20/12/03 14:38:40 INFO CodeGenerator: Code generated in 16.136677 ms
20/12/03 14:38:40 INFO CodeGenerator: Code generated in 19.801317 ms
20/12/03 14:38:40 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:38:40 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:38:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:42365 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:38:40 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/userFiles-86de8731-b10c-40a7-8198-5b6b1d862d6a/darima.zip/darima/dlsa.py:22
20/12/03 14:38:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:38:40 INFO SparkContext: Starting job: toPandas at /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/userFiles-86de8731-b10c-40a7-8198-5b6b1d862d6a/darima.zip/darima/dlsa.py:22
20/12/03 14:38:40 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/userFiles-86de8731-b10c-40a7-8198-5b6b1d862d6a/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/03 14:38:40 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/userFiles-86de8731-b10c-40a7-8198-5b6b1d862d6a/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/03 14:38:40 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/userFiles-86de8731-b10c-40a7-8198-5b6b1d862d6a/darima.zip/darima/dlsa.py:22)
20/12/03 14:38:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/03 14:38:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/03 14:38:40 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/userFiles-86de8731-b10c-40a7-8198-5b6b1d862d6a/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:38:40 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/03 14:38:40 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/03 14:38:40 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:42365 (size: 11.6 KiB, free: 5.8 GiB)
20/12/03 14:38:40 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/03 14:38:40 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/userFiles-86de8731-b10c-40a7-8198-5b6b1d862d6a/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/03 14:38:40 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/03 14:38:40 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:38:40 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:33963 (size: 11.6 KiB, free: 912.3 MiB)
20/12/03 14:38:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:33963 (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 14:38:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1532 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:38:42 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/03 14:38:42 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 56151
20/12/03 14:38:42 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/userFiles-86de8731-b10c-40a7-8198-5b6b1d862d6a/darima.zip/darima/dlsa.py:22) finished in 1.559 s
20/12/03 14:38:42 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:38:42 INFO DAGScheduler: running: Set()
20/12/03 14:38:42 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/03 14:38:42 INFO DAGScheduler: failed: Set()
20/12/03 14:38:42 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/userFiles-86de8731-b10c-40a7-8198-5b6b1d862d6a/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:38:42 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/03 14:38:42 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/03 14:38:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:42365 (size: 131.5 KiB, free: 5.8 GiB)
20/12/03 14:38:42 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/03 14:38:42 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/userFiles-86de8731-b10c-40a7-8198-5b6b1d862d6a/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/03 14:38:42 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/03 14:38:42 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:38:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:33963 (size: 131.5 KiB, free: 912.1 MiB)
20/12/03 14:38:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:48786
20/12/03 14:38:45 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:38:45 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/03 14:38:46 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:38:46 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/03 14:38:47 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:38:47 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/03 14:38:48 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:38:48 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/03 14:38:48 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:38:48 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/03 14:38:49 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:38:49 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/03 14:38:50 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:38:50 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/03 14:38:50 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/03 14:38:50 INFO YarnScheduler: Cancelling stage 3
20/12/03 14:38:50 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/03 14:38:50 INFO YarnScheduler: Stage 3 was cancelled
20/12/03 14:38:50 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/userFiles-86de8731-b10c-40a7-8198-5b6b1d862d6a/darima.zip/darima/dlsa.py:22) failed in 8.487 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/03 14:38:50 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/userFiles-86de8731-b10c-40a7-8198-5b6b1d862d6a/darima.zip/darima/dlsa.py:22, took 10.113191 s
20/12/03 14:38:50 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/03 14:38:50 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/03 14:38:51 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 14:38:51 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 14:38:51 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 14:38:51 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 14:38:51 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 14:38:51 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 14:38:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 14:38:51 INFO MemoryStore: MemoryStore cleared
20/12/03 14:38:51 INFO BlockManager: BlockManager stopped
20/12/03 14:38:51 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 14:38:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 14:38:51 INFO SparkContext: Successfully stopped SparkContext
20/12/03 14:38:51 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-0ef41686-dc88-4760-b30d-173a2bbd5e7e
20/12/03 14:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28/pyspark-d244e059-7827-41de-9720-35b1ebc8d14e
20/12/03 14:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-76eb97bd-72b6-4e5e-a155-29379a9f2a28
20/12/03 14:38:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:38:54 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:38:54 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:38:54 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:38:54 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:38:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:38:54 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:38:54 INFO ResourceUtils: ==============================================================
20/12/03 14:38:54 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:38:54 INFO ResourceUtils: ==============================================================
20/12/03 14:38:54 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:38:54 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:38:54 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:38:54 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:38:54 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:38:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:38:55 INFO Utils: Successfully started service 'sparkDriver' on port 37939.
20/12/03 14:38:55 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:38:55 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:38:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:38:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:38:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:38:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f64ea9f9-f0fd-4855-a1bc-5aa6337a0bd1
20/12/03 14:38:55 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:38:55 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:38:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:38:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:38:56 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:38:56 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:38:57 INFO Configuration: resource-types.xml not found
20/12/03 14:38:57 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:38:57 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:38:57 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:38:57 INFO Client: Setting up container launch context for our AM
20/12/03 14:38:57 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:38:57 INFO Client: Preparing resources for our AM container
20/12/03 14:38:57 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:39:00 INFO Client: Uploading resource file:/tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/__spark_libs__10631878452495747300.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0031/__spark_libs__10631878452495747300.zip
20/12/03 14:39:01 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0031/pyspark.zip
20/12/03 14:39:01 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0031/py4j-0.10.9-src.zip
20/12/03 14:39:01 INFO Client: Uploading resource file:/tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/__spark_conf__775177708446657315.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0031/__spark_conf__.zip
20/12/03 14:39:01 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:39:01 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:39:01 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:39:01 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:39:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:39:01 INFO Client: Submitting application application_1607015337794_0031 to ResourceManager
20/12/03 14:39:01 INFO YarnClientImpl: Submitted application application_1607015337794_0031
20/12/03 14:39:02 INFO Client: Application report for application_1607015337794_0031 (state: ACCEPTED)
20/12/03 14:39:02 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024341537
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0031/
	 user: bsuconn
20/12/03 14:39:03 INFO Client: Application report for application_1607015337794_0031 (state: ACCEPTED)
20/12/03 14:39:04 INFO Client: Application report for application_1607015337794_0031 (state: ACCEPTED)
20/12/03 14:39:05 INFO Client: Application report for application_1607015337794_0031 (state: ACCEPTED)
20/12/03 14:39:06 INFO Client: Application report for application_1607015337794_0031 (state: RUNNING)
20/12/03 14:39:06 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024341537
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0031/
	 user: bsuconn
20/12/03 14:39:06 INFO YarnClientSchedulerBackend: Application application_1607015337794_0031 has started running.
20/12/03 14:39:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41007.
20/12/03 14:39:06 INFO NettyBlockTransferService: Server created on 192.168.1.9:41007
20/12/03 14:39:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 14:39:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 41007, None)
20/12/03 14:39:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:41007 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 41007, None)
20/12/03 14:39:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 41007, None)
20/12/03 14:39:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 41007, None)
20/12/03 14:39:06 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0031), /proxy/application_1607015337794_0031
20/12/03 14:39:06 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:39:07 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 14:39:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 14:39:12 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:54078) with ID 1
20/12/03 14:39:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:44849 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 44849, None)
20/12/03 14:39:13 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:54084) with ID 2
20/12/03 14:39:13 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/03 14:39:13 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:34283 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 34283, None)
20/12/03 14:39:13 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 14:39:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 14:39:13 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 14:39:13 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:39:13 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:39:13 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:39:13 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:39:13 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:39:14 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:37939/files/darima.zip with timestamp 1607024354569
20/12/03 14:39:14 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/userFiles-5d151def-ddd7-44e3-a3da-773b1c4d2e46/darima.zip
20/12/03 14:39:16 INFO InMemoryFileIndex: It took 61 ms to list leaf files for 1 paths.
20/12/03 14:39:18 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
20/12/03 14:39:19 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:39:19 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:39:19 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:39:19 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:39:19 INFO CodeGenerator: Code generated in 285.628687 ms
20/12/03 14:39:19 INFO CodeGenerator: Code generated in 32.746034 ms
20/12/03 14:39:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:39:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:39:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:41007 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:39:20 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/03 14:39:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:39:20 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 14:39:20 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 14:39:20 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 14:39:20 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/03 14:39:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/03 14:39:20 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/03 14:39:20 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:39:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 14:39:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 14:39:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:41007 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:39:20 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 14:39:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:39:20 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 14:39:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:39:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:44849 (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:39:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:44849 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:39:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3210 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:39:23 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/03 14:39:23 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.370 s
20/12/03 14:39:23 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:39:23 INFO DAGScheduler: running: Set()
20/12/03 14:39:23 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/03 14:39:23 INFO DAGScheduler: failed: Set()
20/12/03 14:39:23 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:39:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/03 14:39:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/03 14:39:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:41007 (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:39:23 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/03 14:39:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:39:23 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/03 14:39:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:39:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:34283 (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:39:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:54084
20/12/03 14:39:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1837 ms on 192.168.1.9 (executor 2) (1/1)
20/12/03 14:39:25 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 1.870 s
20/12/03 14:39:25 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/03 14:39:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 14:39:25 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/03 14:39:25 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 5.348877 s
20/12/03 14:39:27 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/03 14:39:27 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:44849 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:39:27 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:41007 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:39:28 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:44849 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:39:28 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:41007 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:39:28 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:41007 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:39:28 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:34283 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:39:28 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:39:28 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:39:28 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:39:28 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:39:28 INFO CodeGenerator: Code generated in 18.013686 ms
20/12/03 14:39:28 INFO CodeGenerator: Code generated in 11.26936 ms
20/12/03 14:39:28 INFO CodeGenerator: Code generated in 13.747738 ms
20/12/03 14:39:28 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:39:28 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:39:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:41007 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:39:28 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/userFiles-5d151def-ddd7-44e3-a3da-773b1c4d2e46/darima.zip/darima/dlsa.py:22
20/12/03 14:39:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:39:28 INFO SparkContext: Starting job: toPandas at /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/userFiles-5d151def-ddd7-44e3-a3da-773b1c4d2e46/darima.zip/darima/dlsa.py:22
20/12/03 14:39:28 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/userFiles-5d151def-ddd7-44e3-a3da-773b1c4d2e46/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/03 14:39:28 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/userFiles-5d151def-ddd7-44e3-a3da-773b1c4d2e46/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/03 14:39:28 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/userFiles-5d151def-ddd7-44e3-a3da-773b1c4d2e46/darima.zip/darima/dlsa.py:22)
20/12/03 14:39:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/03 14:39:28 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/03 14:39:28 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/userFiles-5d151def-ddd7-44e3-a3da-773b1c4d2e46/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:39:28 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/03 14:39:28 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/03 14:39:28 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:41007 (size: 11.6 KiB, free: 5.8 GiB)
20/12/03 14:39:28 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/03 14:39:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/userFiles-5d151def-ddd7-44e3-a3da-773b1c4d2e46/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/03 14:39:28 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/03 14:39:28 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:39:28 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:34283 (size: 11.6 KiB, free: 912.3 MiB)
20/12/03 14:39:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:34283 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:39:31 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2764 ms on 192.168.1.9 (executor 2) (1/1)
20/12/03 14:39:31 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/03 14:39:31 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 55215
20/12/03 14:39:31 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/userFiles-5d151def-ddd7-44e3-a3da-773b1c4d2e46/darima.zip/darima/dlsa.py:22) finished in 2.788 s
20/12/03 14:39:31 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:39:31 INFO DAGScheduler: running: Set()
20/12/03 14:39:31 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/03 14:39:31 INFO DAGScheduler: failed: Set()
20/12/03 14:39:31 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/userFiles-5d151def-ddd7-44e3-a3da-773b1c4d2e46/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:39:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/03 14:39:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/03 14:39:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:41007 (size: 131.5 KiB, free: 5.8 GiB)
20/12/03 14:39:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/03 14:39:31 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/userFiles-5d151def-ddd7-44e3-a3da-773b1c4d2e46/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/03 14:39:31 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/03 14:39:31 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:39:31 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:39:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:34283 (size: 131.5 KiB, free: 912.1 MiB)
20/12/03 14:39:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:44849 (size: 131.5 KiB, free: 912.2 MiB)
20/12/03 14:39:32 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:54078
20/12/03 14:39:32 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:54084
20/12/03 14:39:37 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:39:37 WARN TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/03 14:39:39 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:39:39 INFO TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/03 14:39:39 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 7, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:39:39 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/03 14:39:40 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:39:40 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/03 14:39:41 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 9, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:39:41 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/03 14:39:42 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:39:42 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/03 14:39:42 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 11, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:39:42 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 9) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/03 14:39:43 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:39:43 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 10) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/03 14:39:43 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:39:43 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 11) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/03 14:39:44 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 14, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:39:44 INFO TaskSetManager: Lost task 2.3 in stage 3.0 (TID 12) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 9]
20/12/03 14:39:44 ERROR TaskSetManager: Task 2 in stage 3.0 failed 4 times; aborting job
20/12/03 14:39:44 INFO YarnScheduler: Cancelling stage 3
20/12/03 14:39:44 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/03 14:39:44 INFO YarnScheduler: Stage 3 was cancelled
20/12/03 14:39:44 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/userFiles-5d151def-ddd7-44e3-a3da-773b1c4d2e46/darima.zip/darima/dlsa.py:22) failed in 12.963 s due to Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/03 14:39:44 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/userFiles-5d151def-ddd7-44e3-a3da-773b1c4d2e46/darima.zip/darima/dlsa.py:22, took 15.808990 s
20/12/03 14:39:44 WARN TaskSetManager: Lost task 3.3 in stage 3.0 (TID 14, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/03 14:39:44 WARN TaskSetManager: Lost task 0.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/03 14:39:44 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/03 14:39:45 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 14:39:45 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 14:39:45 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 14:39:45 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 14:39:45 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 14:39:45 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 14:39:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 14:39:45 INFO MemoryStore: MemoryStore cleared
20/12/03 14:39:45 INFO BlockManager: BlockManager stopped
20/12/03 14:39:45 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 14:39:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 14:39:45 INFO SparkContext: Successfully stopped SparkContext
20/12/03 14:39:45 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:39:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629/pyspark-6b7ad8da-0e6d-4714-99b0-a3f4a0817b4a
20/12/03 14:39:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-02334635-e70f-4e4d-b418-20d0ab2fa629
20/12/03 14:39:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b3652e5-fc1c-4ff8-8156-81799a012ff8
20/12/03 14:39:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:39:48 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:39:48 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:39:48 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:39:48 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:39:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:39:49 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:39:49 INFO ResourceUtils: ==============================================================
20/12/03 14:39:49 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:39:49 INFO ResourceUtils: ==============================================================
20/12/03 14:39:49 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:39:49 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:39:49 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:39:49 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:39:49 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:39:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:39:49 INFO Utils: Successfully started service 'sparkDriver' on port 35989.
20/12/03 14:39:49 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:39:49 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:39:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:39:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:39:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:39:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8408b074-7bc2-41d4-b508-03484c14682b
20/12/03 14:39:49 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:39:49 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:39:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:39:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:39:50 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:39:50 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:39:51 INFO Configuration: resource-types.xml not found
20/12/03 14:39:51 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:39:51 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:39:51 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:39:51 INFO Client: Setting up container launch context for our AM
20/12/03 14:39:51 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:39:51 INFO Client: Preparing resources for our AM container
20/12/03 14:39:51 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:39:53 INFO Client: Uploading resource file:/tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/__spark_libs__14004822449235814264.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0032/__spark_libs__14004822449235814264.zip
20/12/03 14:39:54 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0032/pyspark.zip
20/12/03 14:39:54 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0032/py4j-0.10.9-src.zip
20/12/03 14:39:54 INFO Client: Uploading resource file:/tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/__spark_conf__11521416090176577847.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0032/__spark_conf__.zip
20/12/03 14:39:54 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:39:54 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:39:54 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:39:54 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:39:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:39:54 INFO Client: Submitting application application_1607015337794_0032 to ResourceManager
20/12/03 14:39:54 INFO YarnClientImpl: Submitted application application_1607015337794_0032
20/12/03 14:39:55 INFO Client: Application report for application_1607015337794_0032 (state: ACCEPTED)
20/12/03 14:39:55 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024394518
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0032/
	 user: bsuconn
20/12/03 14:39:56 INFO Client: Application report for application_1607015337794_0032 (state: ACCEPTED)
20/12/03 14:39:57 INFO Client: Application report for application_1607015337794_0032 (state: ACCEPTED)
20/12/03 14:39:58 INFO Client: Application report for application_1607015337794_0032 (state: ACCEPTED)
20/12/03 14:39:59 INFO Client: Application report for application_1607015337794_0032 (state: ACCEPTED)
20/12/03 14:39:59 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0032), /proxy/application_1607015337794_0032
20/12/03 14:40:00 INFO Client: Application report for application_1607015337794_0032 (state: RUNNING)
20/12/03 14:40:00 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024394518
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0032/
	 user: bsuconn
20/12/03 14:40:00 INFO YarnClientSchedulerBackend: Application application_1607015337794_0032 has started running.
20/12/03 14:40:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44383.
20/12/03 14:40:00 INFO NettyBlockTransferService: Server created on 192.168.1.9:44383
20/12/03 14:40:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 14:40:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 44383, None)
20/12/03 14:40:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:44383 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 44383, None)
20/12/03 14:40:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 44383, None)
20/12/03 14:40:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 44383, None)
20/12/03 14:40:00 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 14:40:00 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:40:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 14:40:05 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:47312) with ID 1
20/12/03 14:40:05 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:46767 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 46767, None)
20/12/03 14:40:06 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:47316) with ID 2
20/12/03 14:40:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:35661 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 35661, None)
20/12/03 14:40:20 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
20/12/03 14:40:20 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 14:40:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 14:40:20 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 14:40:20 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:40:20 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:40:20 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:40:20 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:40:20 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:40:21 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:35989/files/darima.zip with timestamp 1607024421265
20/12/03 14:40:21 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/userFiles-bb946129-1318-4925-a3e7-3679e370058a/darima.zip
20/12/03 14:40:23 INFO InMemoryFileIndex: It took 68 ms to list leaf files for 1 paths.
20/12/03 14:40:25 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/12/03 14:40:25 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:40:25 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:40:25 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:40:25 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:40:26 INFO CodeGenerator: Code generated in 524.167623 ms
20/12/03 14:40:26 INFO CodeGenerator: Code generated in 33.861819 ms
20/12/03 14:40:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:40:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:40:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:44383 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:40:27 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/03 14:40:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:40:27 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 14:40:27 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 14:40:27 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 14:40:27 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/03 14:40:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/03 14:40:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/03 14:40:27 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:40:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 14:40:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 14:40:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:44383 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:40:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 14:40:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:40:27 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 14:40:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:40:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:35661 (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:40:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:35661 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:40:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3254 ms on 192.168.1.9 (executor 2) (1/1)
20/12/03 14:40:30 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/03 14:40:30 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.407 s
20/12/03 14:40:30 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:40:30 INFO DAGScheduler: running: Set()
20/12/03 14:40:30 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/03 14:40:30 INFO DAGScheduler: failed: Set()
20/12/03 14:40:30 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:40:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/03 14:40:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/03 14:40:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:44383 (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:40:30 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/03 14:40:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:40:30 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/03 14:40:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:40:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:46767 (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:40:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:47312
20/12/03 14:40:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1767 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:40:32 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/03 14:40:32 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 1.785 s
20/12/03 14:40:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 14:40:32 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/03 14:40:32 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 5.287702 s
20/12/03 14:40:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:35661 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:40:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:44383 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:40:34 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:44383 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:40:34 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:35661 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:40:34 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:44383 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:40:34 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:46767 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:40:34 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/03 14:40:35 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:40:35 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:40:35 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:40:35 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:40:35 INFO CodeGenerator: Code generated in 27.616085 ms
20/12/03 14:40:35 INFO CodeGenerator: Code generated in 16.524825 ms
20/12/03 14:40:35 INFO CodeGenerator: Code generated in 20.109354 ms
20/12/03 14:40:35 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:40:35 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:40:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:44383 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:40:35 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/userFiles-bb946129-1318-4925-a3e7-3679e370058a/darima.zip/darima/dlsa.py:22
20/12/03 14:40:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:40:36 INFO SparkContext: Starting job: toPandas at /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/userFiles-bb946129-1318-4925-a3e7-3679e370058a/darima.zip/darima/dlsa.py:22
20/12/03 14:40:36 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/userFiles-bb946129-1318-4925-a3e7-3679e370058a/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/03 14:40:36 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/userFiles-bb946129-1318-4925-a3e7-3679e370058a/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/03 14:40:36 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/userFiles-bb946129-1318-4925-a3e7-3679e370058a/darima.zip/darima/dlsa.py:22)
20/12/03 14:40:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/03 14:40:36 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/03 14:40:36 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/userFiles-bb946129-1318-4925-a3e7-3679e370058a/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:40:36 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/03 14:40:36 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/03 14:40:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:44383 (size: 11.6 KiB, free: 5.8 GiB)
20/12/03 14:40:36 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/03 14:40:36 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/userFiles-bb946129-1318-4925-a3e7-3679e370058a/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/03 14:40:36 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/03 14:40:36 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:40:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:46767 (size: 11.6 KiB, free: 912.3 MiB)
20/12/03 14:40:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:46767 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:40:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2706 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:40:38 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 59699
20/12/03 14:40:38 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/03 14:40:38 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/userFiles-bb946129-1318-4925-a3e7-3679e370058a/darima.zip/darima/dlsa.py:22) finished in 2.731 s
20/12/03 14:40:38 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:40:38 INFO DAGScheduler: running: Set()
20/12/03 14:40:38 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/03 14:40:38 INFO DAGScheduler: failed: Set()
20/12/03 14:40:38 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/userFiles-bb946129-1318-4925-a3e7-3679e370058a/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:40:38 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/03 14:40:38 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/03 14:40:38 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:44383 (size: 131.5 KiB, free: 5.8 GiB)
20/12/03 14:40:38 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/03 14:40:38 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/userFiles-bb946129-1318-4925-a3e7-3679e370058a/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/03 14:40:38 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/03 14:40:38 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:40:38 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:40:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:46767 (size: 131.5 KiB, free: 912.1 MiB)
20/12/03 14:40:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:35661 (size: 131.5 KiB, free: 912.2 MiB)
20/12/03 14:40:39 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:47312
20/12/03 14:40:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:47316
20/12/03 14:40:45 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:40:45 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/03 14:40:47 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:40:47 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/03 14:40:47 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 7, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:40:47 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/03 14:40:48 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:40:48 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/03 14:40:48 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 9, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:40:48 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/03 14:40:49 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:40:49 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/03 14:40:50 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 11, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:40:50 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 9) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/03 14:40:51 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:40:51 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 10) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/03 14:40:51 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:40:51 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 11) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/03 14:40:52 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 14, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:40:52 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 12) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 9]
20/12/03 14:40:52 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/03 14:40:52 INFO YarnScheduler: Cancelling stage 3
20/12/03 14:40:52 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/03 14:40:52 INFO YarnScheduler: Stage 3 was cancelled
20/12/03 14:40:52 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/userFiles-bb946129-1318-4925-a3e7-3679e370058a/darima.zip/darima/dlsa.py:22) failed in 13.760 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 172, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/03 14:40:52 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/userFiles-bb946129-1318-4925-a3e7-3679e370058a/darima.zip/darima/dlsa.py:22, took 16.562918 s
20/12/03 14:40:52 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 14, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/03 14:40:53 WARN TaskSetManager: Lost task 3.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/03 14:40:53 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/03 14:40:53 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 14:40:53 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 14:40:53 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 14:40:53 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 14:40:53 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 14:40:53 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 14:40:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 14:40:53 INFO MemoryStore: MemoryStore cleared
20/12/03 14:40:53 INFO BlockManager: BlockManager stopped
20/12/03 14:40:53 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 14:40:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 14:40:53 INFO SparkContext: Successfully stopped SparkContext
20/12/03 14:40:53 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:40:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535
20/12/03 14:40:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-913a7a95-cfe8-41d3-815c-8b55ebb48535/pyspark-f5f68bcb-dd58-43e7-979b-699077754bb4
20/12/03 14:40:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-7c8d2ab5-3935-4d33-8a67-9ca35cfa8560
20/12/03 14:49:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:49:26 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:49:26 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:49:26 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:49:26 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:49:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:49:27 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:49:27 INFO ResourceUtils: ==============================================================
20/12/03 14:49:27 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:49:27 INFO ResourceUtils: ==============================================================
20/12/03 14:49:27 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:49:27 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:49:27 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:49:27 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:49:27 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:49:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:49:27 INFO Utils: Successfully started service 'sparkDriver' on port 42559.
20/12/03 14:49:27 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:49:27 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:49:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:49:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:49:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:49:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e295b541-a7fd-4de3-8235-9ef96d4f6220
20/12/03 14:49:28 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:49:28 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:49:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:49:28 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:49:28 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:49:29 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:49:29 INFO Configuration: resource-types.xml not found
20/12/03 14:49:29 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:49:29 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:49:29 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:49:29 INFO Client: Setting up container launch context for our AM
20/12/03 14:49:29 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:49:29 INFO Client: Preparing resources for our AM container
20/12/03 14:49:29 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:49:31 INFO Client: Uploading resource file:/tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/__spark_libs__91477837918097769.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0033/__spark_libs__91477837918097769.zip
20/12/03 14:49:32 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0033/pyspark.zip
20/12/03 14:49:32 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0033/py4j-0.10.9-src.zip
20/12/03 14:49:32 INFO Client: Uploading resource file:/tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/__spark_conf__10618611195886317708.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0033/__spark_conf__.zip
20/12/03 14:49:32 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:49:32 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:49:32 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:49:32 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:49:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:49:32 INFO Client: Submitting application application_1607015337794_0033 to ResourceManager
20/12/03 14:49:32 INFO YarnClientImpl: Submitted application application_1607015337794_0033
20/12/03 14:49:33 INFO Client: Application report for application_1607015337794_0033 (state: ACCEPTED)
20/12/03 14:49:33 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024972794
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0033/
	 user: bsuconn
20/12/03 14:49:34 INFO Client: Application report for application_1607015337794_0033 (state: ACCEPTED)
20/12/03 14:49:35 INFO Client: Application report for application_1607015337794_0033 (state: ACCEPTED)
20/12/03 14:49:36 INFO Client: Application report for application_1607015337794_0033 (state: ACCEPTED)
20/12/03 14:49:37 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0033), /proxy/application_1607015337794_0033
20/12/03 14:49:37 INFO Client: Application report for application_1607015337794_0033 (state: RUNNING)
20/12/03 14:49:37 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607024972794
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0033/
	 user: bsuconn
20/12/03 14:49:37 INFO YarnClientSchedulerBackend: Application application_1607015337794_0033 has started running.
20/12/03 14:49:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40971.
20/12/03 14:49:37 INFO NettyBlockTransferService: Server created on 192.168.1.9:40971
20/12/03 14:49:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 14:49:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 40971, None)
20/12/03 14:49:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:40971 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 40971, None)
20/12/03 14:49:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 40971, None)
20/12/03 14:49:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 40971, None)
20/12/03 14:49:38 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:49:38 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 14:49:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 14:49:42 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:58344) with ID 1
20/12/03 14:49:42 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/03 14:49:42 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:39545 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 39545, None)
20/12/03 14:49:42 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 14:49:42 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 14:49:42 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 14:49:42 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:49:42 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:49:42 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:49:42 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:49:42 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:49:43 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:42559/files/darima.zip with timestamp 1607024983650
20/12/03 14:49:43 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/userFiles-eb66b274-5a3e-4877-b7bf-e476916ea841/darima.zip
20/12/03 14:49:45 INFO InMemoryFileIndex: It took 77 ms to list leaf files for 1 paths.
20/12/03 14:49:47 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
20/12/03 14:49:47 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:49:47 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:49:47 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:49:47 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:49:48 INFO CodeGenerator: Code generated in 357.453068 ms
20/12/03 14:49:48 INFO CodeGenerator: Code generated in 28.399087 ms
20/12/03 14:49:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:49:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:49:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:40971 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:49:48 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/03 14:49:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:49:48 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 14:49:48 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 14:49:48 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 14:49:48 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/03 14:49:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/03 14:49:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/03 14:49:48 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:49:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 14:49:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 14:49:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:40971 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:49:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 14:49:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:49:49 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 14:49:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:49:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:39545 (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:49:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:39545 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:49:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3396 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:49:52 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/03 14:49:52 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.554 s
20/12/03 14:49:52 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:49:52 INFO DAGScheduler: running: Set()
20/12/03 14:49:52 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/03 14:49:52 INFO DAGScheduler: failed: Set()
20/12/03 14:49:52 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:49:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/03 14:49:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/03 14:49:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:40971 (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:49:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/03 14:49:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:49:52 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/03 14:49:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:49:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:39545 (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:49:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:58344
20/12/03 14:49:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 370 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:49:52 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/03 14:49:52 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.386 s
20/12/03 14:49:52 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 14:49:52 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/03 14:49:52 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 4.042743 s
20/12/03 14:49:54 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:39545 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:49:54 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:40971 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:49:54 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:39545 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:49:54 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:40971 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:49:55 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/03 14:49:55 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:49:55 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:49:55 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:49:55 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:49:56 INFO CodeGenerator: Code generated in 26.860742 ms
20/12/03 14:49:56 INFO CodeGenerator: Code generated in 19.101068 ms
20/12/03 14:49:56 INFO CodeGenerator: Code generated in 23.870891 ms
20/12/03 14:49:56 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:49:56 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:49:56 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:40971 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:49:56 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/userFiles-eb66b274-5a3e-4877-b7bf-e476916ea841/darima.zip/darima/dlsa.py:22
20/12/03 14:49:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:49:56 INFO SparkContext: Starting job: toPandas at /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/userFiles-eb66b274-5a3e-4877-b7bf-e476916ea841/darima.zip/darima/dlsa.py:22
20/12/03 14:49:56 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/userFiles-eb66b274-5a3e-4877-b7bf-e476916ea841/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/03 14:49:56 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/userFiles-eb66b274-5a3e-4877-b7bf-e476916ea841/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/03 14:49:56 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/userFiles-eb66b274-5a3e-4877-b7bf-e476916ea841/darima.zip/darima/dlsa.py:22)
20/12/03 14:49:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/03 14:49:56 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/03 14:49:56 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/userFiles-eb66b274-5a3e-4877-b7bf-e476916ea841/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:49:56 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/03 14:49:56 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/03 14:49:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:40971 (size: 11.6 KiB, free: 5.8 GiB)
20/12/03 14:49:56 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/03 14:49:56 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/userFiles-eb66b274-5a3e-4877-b7bf-e476916ea841/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/03 14:49:56 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/03 14:49:56 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:49:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:39545 (size: 11.6 KiB, free: 912.3 MiB)
20/12/03 14:49:57 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:39545 (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 14:49:57 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1341 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:49:57 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/03 14:49:57 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 34835
20/12/03 14:49:57 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/userFiles-eb66b274-5a3e-4877-b7bf-e476916ea841/darima.zip/darima/dlsa.py:22) finished in 1.364 s
20/12/03 14:49:57 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:49:57 INFO DAGScheduler: running: Set()
20/12/03 14:49:57 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/03 14:49:57 INFO DAGScheduler: failed: Set()
20/12/03 14:49:57 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/userFiles-eb66b274-5a3e-4877-b7bf-e476916ea841/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:49:57 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/03 14:49:57 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/03 14:49:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:40971 (size: 131.5 KiB, free: 5.8 GiB)
20/12/03 14:49:57 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/03 14:49:57 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/userFiles-eb66b274-5a3e-4877-b7bf-e476916ea841/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/03 14:49:57 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/03 14:49:57 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:49:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:39545 (size: 131.5 KiB, free: 912.1 MiB)
20/12/03 14:49:58 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:58344
20/12/03 14:50:01 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:50:01 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/03 14:50:02 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:50:02 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/03 14:50:03 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:50:03 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/03 14:50:03 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:50:03 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/03 14:50:04 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:50:04 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/03 14:50:05 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:50:05 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/03 14:50:06 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:50:06 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/03 14:50:06 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/03 14:50:06 INFO YarnScheduler: Cancelling stage 3
20/12/03 14:50:06 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/03 14:50:06 INFO YarnScheduler: Stage 3 was cancelled
20/12/03 14:50:06 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/userFiles-eb66b274-5a3e-4877-b7bf-e476916ea841/darima.zip/darima/dlsa.py:22) failed in 8.448 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/03 14:50:06 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/userFiles-eb66b274-5a3e-4877-b7bf-e476916ea841/darima.zip/darima/dlsa.py:22, took 9.878158 s
20/12/03 14:50:06 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/03 14:50:06 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/03 14:50:06 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 14:50:06 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 14:50:06 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 14:50:06 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 14:50:06 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 14:50:06 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 14:50:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 14:50:06 INFO MemoryStore: MemoryStore cleared
20/12/03 14:50:06 INFO BlockManager: BlockManager stopped
20/12/03 14:50:07 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 14:50:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 14:50:07 INFO SparkContext: Successfully stopped SparkContext
20/12/03 14:50:07 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:50:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-b2684dd8-b4e2-4b77-9373-ff76526112fa
20/12/03 14:50:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad/pyspark-d99f99c9-6f08-43ca-bbf3-6cd22432e651
20/12/03 14:50:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-928fc476-046a-4909-9d22-4fca2abb2bad
20/12/03 14:50:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:50:09 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:50:09 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:50:09 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:50:09 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:50:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:50:10 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:50:10 INFO ResourceUtils: ==============================================================
20/12/03 14:50:10 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:50:10 INFO ResourceUtils: ==============================================================
20/12/03 14:50:10 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:50:10 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:50:10 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:50:10 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:50:10 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:50:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:50:10 INFO Utils: Successfully started service 'sparkDriver' on port 38095.
20/12/03 14:50:10 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:50:10 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:50:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:50:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:50:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:50:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1182923d-641d-410d-a1fd-258e26e8af27
20/12/03 14:50:11 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:50:11 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:50:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:50:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:50:12 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:50:12 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:50:13 INFO Configuration: resource-types.xml not found
20/12/03 14:50:13 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:50:13 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:50:13 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:50:13 INFO Client: Setting up container launch context for our AM
20/12/03 14:50:13 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:50:13 INFO Client: Preparing resources for our AM container
20/12/03 14:50:13 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:50:15 INFO Client: Uploading resource file:/tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/__spark_libs__16289330445374015640.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0034/__spark_libs__16289330445374015640.zip
20/12/03 14:50:16 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0034/pyspark.zip
20/12/03 14:50:16 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0034/py4j-0.10.9-src.zip
20/12/03 14:50:16 INFO Client: Uploading resource file:/tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/__spark_conf__13746792238937548065.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0034/__spark_conf__.zip
20/12/03 14:50:16 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:50:16 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:50:16 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:50:16 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:50:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:50:16 INFO Client: Submitting application application_1607015337794_0034 to ResourceManager
20/12/03 14:50:16 INFO YarnClientImpl: Submitted application application_1607015337794_0034
20/12/03 14:50:17 INFO Client: Application report for application_1607015337794_0034 (state: ACCEPTED)
20/12/03 14:50:17 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607025016894
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0034/
	 user: bsuconn
20/12/03 14:50:18 INFO Client: Application report for application_1607015337794_0034 (state: ACCEPTED)
20/12/03 14:50:19 INFO Client: Application report for application_1607015337794_0034 (state: ACCEPTED)
20/12/03 14:50:20 INFO Client: Application report for application_1607015337794_0034 (state: ACCEPTED)
20/12/03 14:50:21 INFO Client: Application report for application_1607015337794_0034 (state: ACCEPTED)
20/12/03 14:50:22 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0034), /proxy/application_1607015337794_0034
20/12/03 14:50:22 INFO Client: Application report for application_1607015337794_0034 (state: RUNNING)
20/12/03 14:50:22 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607025016894
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0034/
	 user: bsuconn
20/12/03 14:50:22 INFO YarnClientSchedulerBackend: Application application_1607015337794_0034 has started running.
20/12/03 14:50:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38963.
20/12/03 14:50:22 INFO NettyBlockTransferService: Server created on 192.168.1.9:38963
20/12/03 14:50:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 14:50:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 38963, None)
20/12/03 14:50:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:38963 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 38963, None)
20/12/03 14:50:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 38963, None)
20/12/03 14:50:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 38963, None)
20/12/03 14:50:23 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:50:23 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 14:50:28 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 14:50:29 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:54588) with ID 1
20/12/03 14:50:29 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:42477 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 42477, None)
20/12/03 14:50:31 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:54592) with ID 2
20/12/03 14:50:31 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/03 14:50:31 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:38609 with 912.3 MiB RAM, BlockManagerId(2, 192.168.1.9, 38609, None)
20/12/03 14:50:31 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 14:50:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 14:50:31 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 14:50:31 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:50:31 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:50:31 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:50:31 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:50:31 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:50:32 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:38095/files/darima.zip with timestamp 1607025032173
20/12/03 14:50:32 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/userFiles-2f2388a8-f90a-4356-a6ee-2d8463517fde/darima.zip
20/12/03 14:50:34 INFO InMemoryFileIndex: It took 72 ms to list leaf files for 1 paths.
20/12/03 14:50:35 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/12/03 14:50:36 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:50:36 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:50:36 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:50:36 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:50:37 INFO CodeGenerator: Code generated in 292.046701 ms
20/12/03 14:50:37 INFO CodeGenerator: Code generated in 28.828461 ms
20/12/03 14:50:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:50:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:50:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:38963 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:50:37 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/03 14:50:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:50:37 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 14:50:37 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 14:50:37 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 14:50:37 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/03 14:50:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/03 14:50:37 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/03 14:50:37 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:50:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 14:50:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 14:50:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:38963 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:50:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 14:50:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:50:37 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 14:50:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:50:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:42477 (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:50:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:42477 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:50:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3305 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:50:41 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/03 14:50:41 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.477 s
20/12/03 14:50:41 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:50:41 INFO DAGScheduler: running: Set()
20/12/03 14:50:41 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/03 14:50:41 INFO DAGScheduler: failed: Set()
20/12/03 14:50:41 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:50:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/03 14:50:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/03 14:50:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:38963 (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:50:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/03 14:50:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:50:41 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/03 14:50:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:50:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:42477 (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:50:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:54588
20/12/03 14:50:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 391 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:50:41 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.407 s
20/12/03 14:50:41 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/03 14:50:41 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 14:50:41 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/03 14:50:41 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 4.007650 s
20/12/03 14:50:43 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:42477 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:50:43 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:38963 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:50:43 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:38963 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:50:43 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:42477 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:50:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:38963 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:50:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:42477 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:50:44 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/03 14:50:45 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:50:45 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:50:45 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:50:45 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:50:45 INFO CodeGenerator: Code generated in 25.164255 ms
20/12/03 14:50:45 INFO CodeGenerator: Code generated in 16.382204 ms
20/12/03 14:50:45 INFO CodeGenerator: Code generated in 21.626241 ms
20/12/03 14:50:45 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:50:45 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:50:45 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:38963 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:50:45 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/userFiles-2f2388a8-f90a-4356-a6ee-2d8463517fde/darima.zip/darima/dlsa.py:22
20/12/03 14:50:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:50:45 INFO SparkContext: Starting job: toPandas at /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/userFiles-2f2388a8-f90a-4356-a6ee-2d8463517fde/darima.zip/darima/dlsa.py:22
20/12/03 14:50:45 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/userFiles-2f2388a8-f90a-4356-a6ee-2d8463517fde/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/03 14:50:45 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/userFiles-2f2388a8-f90a-4356-a6ee-2d8463517fde/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/03 14:50:45 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/userFiles-2f2388a8-f90a-4356-a6ee-2d8463517fde/darima.zip/darima/dlsa.py:22)
20/12/03 14:50:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/03 14:50:45 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/03 14:50:45 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/userFiles-2f2388a8-f90a-4356-a6ee-2d8463517fde/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:50:45 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/03 14:50:45 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/03 14:50:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:38963 (size: 11.6 KiB, free: 5.8 GiB)
20/12/03 14:50:45 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/03 14:50:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/userFiles-2f2388a8-f90a-4356-a6ee-2d8463517fde/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/03 14:50:45 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/03 14:50:45 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:50:46 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:38609 (size: 11.6 KiB, free: 912.3 MiB)
20/12/03 14:50:48 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:38609 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:50:50 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 4351 ms on 192.168.1.9 (executor 2) (1/1)
20/12/03 14:50:50 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/03 14:50:50 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 32769
20/12/03 14:50:50 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/userFiles-2f2388a8-f90a-4356-a6ee-2d8463517fde/darima.zip/darima/dlsa.py:22) finished in 4.378 s
20/12/03 14:50:50 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:50:50 INFO DAGScheduler: running: Set()
20/12/03 14:50:50 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/03 14:50:50 INFO DAGScheduler: failed: Set()
20/12/03 14:50:50 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/userFiles-2f2388a8-f90a-4356-a6ee-2d8463517fde/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:50:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/03 14:50:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/03 14:50:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:38963 (size: 131.5 KiB, free: 5.8 GiB)
20/12/03 14:50:50 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/03 14:50:50 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/userFiles-2f2388a8-f90a-4356-a6ee-2d8463517fde/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/03 14:50:50 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/03 14:50:50 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:50:50 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:50:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:42477 (size: 131.5 KiB, free: 912.2 MiB)
20/12/03 14:50:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:38609 (size: 131.5 KiB, free: 912.1 MiB)
20/12/03 14:50:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:54588
20/12/03 14:50:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:54592
20/12/03 14:50:57 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 5, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:50:57 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/03 14:50:58 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:50:58 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/03 14:50:59 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 7, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:50:59 INFO TaskSetManager: Lost task 3.0 in stage 3.0 (TID 5) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/03 14:50:59 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:50:59 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/03 14:51:00 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 9, 192.168.1.9, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:51:00 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 7) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/03 14:51:01 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:51:01 INFO TaskSetManager: Lost task 3.1 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/03 14:51:02 INFO TaskSetManager: Starting task 3.2 in stage 3.0 (TID 11, 192.168.1.9, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:51:02 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 9) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/03 14:51:02 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:51:02 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 10) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 7]
20/12/03 14:51:03 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:51:03 INFO TaskSetManager: Lost task 3.2 in stage 3.0 (TID 11) on 192.168.1.9, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 8]
20/12/03 14:51:04 INFO TaskSetManager: Starting task 3.3 in stage 3.0 (TID 14, 192.168.1.9, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
20/12/03 14:51:04 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 12) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 9]
20/12/03 14:51:04 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/03 14:51:04 INFO YarnScheduler: Cancelling stage 3
20/12/03 14:51:04 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/03 14:51:04 INFO YarnScheduler: Stage 3 was cancelled
20/12/03 14:51:04 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/userFiles-2f2388a8-f90a-4356-a6ee-2d8463517fde/darima.zip/darima/dlsa.py:22) failed in 14.073 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 12, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/03 14:51:04 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/userFiles-2f2388a8-f90a-4356-a6ee-2d8463517fde/darima.zip/darima/dlsa.py:22, took 18.512172 s
20/12/03 14:51:04 WARN TaskSetManager: Lost task 3.3 in stage 3.0 (TID 14, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/03 14:51:04 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 13, 192.168.1.9, executor 2): TaskKilled (Stage cancelled)
20/12/03 14:51:04 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/03 14:51:04 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 14:51:04 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 14:51:04 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 14:51:04 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 14:51:04 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 14:51:05 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 14:51:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 14:51:05 INFO MemoryStore: MemoryStore cleared
20/12/03 14:51:05 INFO BlockManager: BlockManager stopped
20/12/03 14:51:05 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 14:51:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 14:51:05 INFO SparkContext: Successfully stopped SparkContext
20/12/03 14:51:05 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:51:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-a2fd0f4e-0635-428b-b189-c05e0e5b2fd1
20/12/03 14:51:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137
20/12/03 14:51:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-0724e221-276d-44cb-b1ac-678b7a4a8137/pyspark-74d02dd8-2016-4224-81a7-3d92b7d40602
20/12/03 14:51:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:51:08 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:51:08 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:51:08 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:51:08 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:51:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:51:09 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:51:09 INFO ResourceUtils: ==============================================================
20/12/03 14:51:09 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:51:09 INFO ResourceUtils: ==============================================================
20/12/03 14:51:09 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:51:09 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:51:09 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:51:09 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:51:09 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:51:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:51:09 INFO Utils: Successfully started service 'sparkDriver' on port 37301.
20/12/03 14:51:09 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:51:09 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:51:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:51:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:51:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:51:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bfbee09d-6de4-4b77-a259-c452b57fbbfc
20/12/03 14:51:09 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:51:09 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:51:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:51:10 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:51:10 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:51:11 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:51:11 INFO Configuration: resource-types.xml not found
20/12/03 14:51:11 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:51:11 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:51:11 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:51:11 INFO Client: Setting up container launch context for our AM
20/12/03 14:51:11 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:51:11 INFO Client: Preparing resources for our AM container
20/12/03 14:51:11 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:51:11 INFO DiskBlockManager: Shutdown hook called
20/12/03 14:51:11 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:51:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-4ad2256e-490f-4462-8f2f-c8126f1c0d4f
20/12/03 14:51:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-00eb070a-d98d-44f6-b6b4-bf3e21cfd0c9
20/12/03 14:51:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-4ad2256e-490f-4462-8f2f-c8126f1c0d4f/userFiles-c7e6e322-da25-4cb9-86db-9ed2856ff1fd
20/12/03 14:54:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:54:28 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:54:28 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:54:28 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:54:28 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:54:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:54:29 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:54:29 INFO ResourceUtils: ==============================================================
20/12/03 14:54:29 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:54:29 INFO ResourceUtils: ==============================================================
20/12/03 14:54:29 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:54:29 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:54:29 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:54:29 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:54:29 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:54:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:54:29 INFO Utils: Successfully started service 'sparkDriver' on port 46507.
20/12/03 14:54:29 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:54:29 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:54:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:54:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:54:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:54:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-03016835-6923-4ae2-8f34-41ae24f4e9a5
20/12/03 14:54:29 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:54:29 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:54:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:54:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:54:30 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:54:31 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:54:31 INFO Configuration: resource-types.xml not found
20/12/03 14:54:31 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:54:31 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:54:31 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:54:31 INFO Client: Setting up container launch context for our AM
20/12/03 14:54:31 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:54:31 INFO Client: Preparing resources for our AM container
20/12/03 14:54:31 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:54:33 INFO Client: Uploading resource file:/tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/__spark_libs__11263717602377227349.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0036/__spark_libs__11263717602377227349.zip
20/12/03 14:54:34 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0036/pyspark.zip
20/12/03 14:54:34 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0036/py4j-0.10.9-src.zip
20/12/03 14:54:35 INFO Client: Uploading resource file:/tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/__spark_conf__8984439060983252238.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0036/__spark_conf__.zip
20/12/03 14:54:35 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:54:35 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:54:35 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:54:35 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:54:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:54:35 INFO Client: Submitting application application_1607015337794_0036 to ResourceManager
20/12/03 14:54:35 INFO YarnClientImpl: Submitted application application_1607015337794_0036
20/12/03 14:54:36 INFO Client: Application report for application_1607015337794_0036 (state: ACCEPTED)
20/12/03 14:54:36 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607025275119
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0036/
	 user: bsuconn
20/12/03 14:54:37 INFO Client: Application report for application_1607015337794_0036 (state: ACCEPTED)
20/12/03 14:54:38 INFO Client: Application report for application_1607015337794_0036 (state: ACCEPTED)
20/12/03 14:54:39 INFO Client: Application report for application_1607015337794_0036 (state: ACCEPTED)
20/12/03 14:54:40 INFO Client: Application report for application_1607015337794_0036 (state: ACCEPTED)
20/12/03 14:54:40 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0036), /proxy/application_1607015337794_0036
20/12/03 14:54:41 INFO Client: Application report for application_1607015337794_0036 (state: RUNNING)
20/12/03 14:54:41 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607025275119
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0036/
	 user: bsuconn
20/12/03 14:54:41 INFO YarnClientSchedulerBackend: Application application_1607015337794_0036 has started running.
20/12/03 14:54:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39643.
20/12/03 14:54:41 INFO NettyBlockTransferService: Server created on 192.168.1.9:39643
20/12/03 14:54:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 14:54:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 39643, None)
20/12/03 14:54:41 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:39643 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 39643, None)
20/12/03 14:54:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 39643, None)
20/12/03 14:54:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 39643, None)
20/12/03 14:54:41 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:54:41 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 14:54:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 14:54:45 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:37636) with ID 1
20/12/03 14:54:45 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/03 14:54:45 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:35941 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 35941, None)
20/12/03 14:54:45 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 14:54:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 14:54:45 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 14:54:45 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:54:45 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:54:45 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:54:45 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:54:45 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:54:46 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:46507/files/darima.zip with timestamp 1607025286723
20/12/03 14:54:46 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/userFiles-31aee83a-6406-4964-94e0-e1759d7cc1e4/darima.zip
20/12/03 14:54:48 INFO InMemoryFileIndex: It took 95 ms to list leaf files for 1 paths.
20/12/03 14:54:50 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.
20/12/03 14:54:51 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:54:51 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:54:51 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:54:51 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:54:52 INFO CodeGenerator: Code generated in 258.2718 ms
20/12/03 14:54:52 INFO CodeGenerator: Code generated in 37.73503 ms
20/12/03 14:54:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:54:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:54:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:39643 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:54:52 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/03 14:54:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:54:52 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 14:54:52 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 14:54:52 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 14:54:52 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/03 14:54:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/03 14:54:52 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/03 14:54:52 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:54:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 14:54:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 14:54:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:39643 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:54:53 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 14:54:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:54:53 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 14:54:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:54:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:35941 (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:54:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:35941 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:54:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3247 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:54:56 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/03 14:54:56 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.418 s
20/12/03 14:54:56 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:54:56 INFO DAGScheduler: running: Set()
20/12/03 14:54:56 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/03 14:54:56 INFO DAGScheduler: failed: Set()
20/12/03 14:54:56 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:54:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/03 14:54:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/03 14:54:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:39643 (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:54:56 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/03 14:54:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:54:56 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/03 14:54:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:54:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:35941 (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:54:56 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:37636
20/12/03 14:54:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 374 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:54:56 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/03 14:54:56 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.391 s
20/12/03 14:54:56 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 14:54:56 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/03 14:54:56 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.925116 s
20/12/03 14:54:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:39643 in memory (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:54:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.9:35941 in memory (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:54:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:35941 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:54:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:39643 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:54:58 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:39643 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:54:58 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:35941 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:54:59 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/03 14:54:59 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:54:59 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:54:59 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:54:59 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:55:00 INFO CodeGenerator: Code generated in 32.016721 ms
20/12/03 14:55:00 INFO CodeGenerator: Code generated in 15.384549 ms
20/12/03 14:55:00 INFO CodeGenerator: Code generated in 19.650607 ms
20/12/03 14:55:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:55:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:55:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:39643 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:55:00 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/userFiles-31aee83a-6406-4964-94e0-e1759d7cc1e4/darima.zip/darima/dlsa.py:22
20/12/03 14:55:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:55:00 INFO SparkContext: Starting job: toPandas at /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/userFiles-31aee83a-6406-4964-94e0-e1759d7cc1e4/darima.zip/darima/dlsa.py:22
20/12/03 14:55:00 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/userFiles-31aee83a-6406-4964-94e0-e1759d7cc1e4/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/03 14:55:00 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/userFiles-31aee83a-6406-4964-94e0-e1759d7cc1e4/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/03 14:55:00 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/userFiles-31aee83a-6406-4964-94e0-e1759d7cc1e4/darima.zip/darima/dlsa.py:22)
20/12/03 14:55:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/03 14:55:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/03 14:55:00 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/userFiles-31aee83a-6406-4964-94e0-e1759d7cc1e4/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:55:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.3 KiB, free 5.8 GiB)
20/12/03 14:55:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/03 14:55:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:39643 (size: 11.6 KiB, free: 5.8 GiB)
20/12/03 14:55:00 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/03 14:55:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/userFiles-31aee83a-6406-4964-94e0-e1759d7cc1e4/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/03 14:55:00 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/03 14:55:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:55:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:35941 (size: 11.6 KiB, free: 912.3 MiB)
20/12/03 14:55:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:35941 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:55:01 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1405 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:55:01 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/03 14:55:01 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50263
20/12/03 14:55:01 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/userFiles-31aee83a-6406-4964-94e0-e1759d7cc1e4/darima.zip/darima/dlsa.py:22) finished in 1.435 s
20/12/03 14:55:01 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:55:01 INFO DAGScheduler: running: Set()
20/12/03 14:55:01 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/03 14:55:01 INFO DAGScheduler: failed: Set()
20/12/03 14:55:01 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/userFiles-31aee83a-6406-4964-94e0-e1759d7cc1e4/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:55:02 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.1 KiB, free 5.8 GiB)
20/12/03 14:55:02 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.5 KiB, free 5.8 GiB)
20/12/03 14:55:02 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:39643 (size: 131.5 KiB, free: 5.8 GiB)
20/12/03 14:55:02 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/03 14:55:02 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/userFiles-31aee83a-6406-4964-94e0-e1759d7cc1e4/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/03 14:55:02 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/03 14:55:02 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:55:02 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:35941 (size: 131.5 KiB, free: 912.1 MiB)
20/12/03 14:55:02 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:37636
20/12/03 14:55:05 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:55:05 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/03 14:55:06 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:55:06 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/03 14:55:07 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:55:07 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/03 14:55:08 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:55:08 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/03 14:55:09 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:55:09 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/03 14:55:10 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:55:10 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/03 14:55:10 INFO TaskSetManager: Starting task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:55:10 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/03 14:55:11 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/03 14:55:11 INFO YarnScheduler: Cancelling stage 3
20/12/03 14:55:11 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/03 14:55:11 INFO YarnScheduler: Stage 3 was cancelled
20/12/03 14:55:11 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/userFiles-31aee83a-6406-4964-94e0-e1759d7cc1e4/darima.zip/darima/dlsa.py:22) failed in 9.096 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/03 14:55:11 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/userFiles-31aee83a-6406-4964-94e0-e1759d7cc1e4/darima.zip/darima/dlsa.py:22, took 10.589078 s
20/12/03 14:55:11 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 10, 192.168.1.9, executor 1): TaskKilled (Stage cancelled)
20/12/03 14:55:11 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/03 14:55:11 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 14:55:11 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 14:55:11 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 14:55:11 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 14:55:11 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 14:55:11 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 14:55:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 14:55:11 INFO MemoryStore: MemoryStore cleared
20/12/03 14:55:11 INFO BlockManager: BlockManager stopped
20/12/03 14:55:11 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 14:55:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 14:55:11 INFO SparkContext: Successfully stopped SparkContext
20/12/03 14:55:11 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:55:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965
20/12/03 14:55:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-91b9b107-462d-45ef-a566-ae421731d965/pyspark-b41d1392-39a2-4110-9be5-0a815a337579
20/12/03 14:55:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-a40c432e-fbe0-48bd-8c32-1d63051452f9
20/12/03 14:55:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:55:14 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:55:14 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:55:14 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:55:14 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:55:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:55:15 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:55:15 INFO ResourceUtils: ==============================================================
20/12/03 14:55:15 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:55:15 INFO ResourceUtils: ==============================================================
20/12/03 14:55:15 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:55:15 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:55:15 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:55:15 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:55:15 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:55:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:55:15 INFO Utils: Successfully started service 'sparkDriver' on port 37861.
20/12/03 14:55:15 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:55:15 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:55:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:55:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:55:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:55:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-eca96ed2-f11e-43ba-a6bb-a7bf88bc7259
20/12/03 14:55:15 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:55:15 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:55:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:55:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:55:16 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:55:16 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:55:17 INFO Configuration: resource-types.xml not found
20/12/03 14:55:17 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:55:17 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:55:17 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:55:17 INFO Client: Setting up container launch context for our AM
20/12/03 14:55:17 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:55:17 INFO Client: Preparing resources for our AM container
20/12/03 14:55:17 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:55:19 INFO Client: Uploading resource file:/tmp/spark-9d8c559c-e6bf-4f33-b9b5-b208f32d8f08/__spark_libs__9598157757072855347.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0037/__spark_libs__9598157757072855347.zip
20/12/03 14:55:20 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0037/pyspark.zip
20/12/03 14:55:20 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0037/py4j-0.10.9-src.zip
20/12/03 14:55:20 INFO Client: Uploading resource file:/tmp/spark-9d8c559c-e6bf-4f33-b9b5-b208f32d8f08/__spark_conf__10382597448250132075.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0037/__spark_conf__.zip
20/12/03 14:55:20 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:55:20 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:55:20 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:55:20 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:55:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:55:20 INFO Client: Submitting application application_1607015337794_0037 to ResourceManager
20/12/03 14:55:20 INFO YarnClientImpl: Submitted application application_1607015337794_0037
20/12/03 14:55:20 INFO DiskBlockManager: Shutdown hook called
20/12/03 14:55:21 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:55:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-9d8c559c-e6bf-4f33-b9b5-b208f32d8f08
20/12/03 14:55:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-dd429670-871f-4ba3-bec5-ac238206d6fd
20/12/03 14:55:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-9d8c559c-e6bf-4f33-b9b5-b208f32d8f08/userFiles-f82ad3c3-a124-4cfe-93f0-a960454a9274
20/12/03 14:58:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:58:46 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:58:46 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:58:46 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:58:46 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:58:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:58:46 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:58:47 INFO ResourceUtils: ==============================================================
20/12/03 14:58:47 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:58:47 INFO ResourceUtils: ==============================================================
20/12/03 14:58:47 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:58:47 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:58:47 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:58:47 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:58:47 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:58:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:58:47 INFO Utils: Successfully started service 'sparkDriver' on port 38409.
20/12/03 14:58:47 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:58:47 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:58:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:58:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:58:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:58:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ab582332-a612-462a-a690-0545e7a3b602
20/12/03 14:58:47 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:58:47 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:58:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:58:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:58:48 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:58:49 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:58:49 INFO Configuration: resource-types.xml not found
20/12/03 14:58:49 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:58:49 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:58:49 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:58:49 INFO Client: Setting up container launch context for our AM
20/12/03 14:58:49 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:58:49 INFO Client: Preparing resources for our AM container
20/12/03 14:58:49 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:58:51 INFO Client: Uploading resource file:/tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/__spark_libs__5079911082573937169.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0038/__spark_libs__5079911082573937169.zip
20/12/03 14:58:52 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0038/pyspark.zip
20/12/03 14:58:53 INFO Client: Uploading resource file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0038/py4j-0.10.9-src.zip
20/12/03 14:58:53 INFO Client: Uploading resource file:/tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/__spark_conf__13640535847860892556.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0038/__spark_conf__.zip
20/12/03 14:58:54 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:58:54 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:58:54 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:58:54 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:58:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:58:54 INFO Client: Submitting application application_1607015337794_0038 to ResourceManager
20/12/03 14:58:54 INFO YarnClientImpl: Submitted application application_1607015337794_0038
20/12/03 14:58:55 INFO Client: Application report for application_1607015337794_0038 (state: ACCEPTED)
20/12/03 14:58:55 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607025534298
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0038/
	 user: bsuconn
20/12/03 14:58:56 INFO Client: Application report for application_1607015337794_0038 (state: ACCEPTED)
20/12/03 14:58:57 INFO Client: Application report for application_1607015337794_0038 (state: ACCEPTED)
20/12/03 14:58:58 INFO Client: Application report for application_1607015337794_0038 (state: ACCEPTED)
20/12/03 14:58:59 INFO Client: Application report for application_1607015337794_0038 (state: RUNNING)
20/12/03 14:58:59 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.1.9
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607025534298
	 final status: UNDEFINED
	 tracking URL: http://pop-os:8088/proxy/application_1607015337794_0038/
	 user: bsuconn
20/12/03 14:58:59 INFO YarnClientSchedulerBackend: Application application_1607015337794_0038 has started running.
20/12/03 14:58:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46631.
20/12/03 14:58:59 INFO NettyBlockTransferService: Server created on 192.168.1.9:46631
20/12/03 14:58:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/03 14:58:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.9, 46631, None)
20/12/03 14:58:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:46631 with 5.8 GiB RAM, BlockManagerId(driver, 192.168.1.9, 46631, None)
20/12/03 14:58:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.9, 46631, None)
20/12/03 14:58:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.9, 46631, None)
20/12/03 14:58:59 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> pop-os, PROXY_URI_BASES -> http://pop-os:8088/proxy/application_1607015337794_0038), /proxy/application_1607015337794_0038
20/12/03 14:58:59 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:59:00 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/12/03 14:59:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/03 14:59:04 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.9:52388) with ID 1
20/12/03 14:59:04 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/12/03 14:59:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.9:35677 with 912.3 MiB RAM, BlockManagerId(1, 192.168.1.9, 35677, None)
20/12/03 14:59:04 INFO SharedState: loading hive config file: file:/home/bsuconn/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
20/12/03 14:59:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse').
20/12/03 14:59:04 INFO SharedState: Warehouse path is 'file:/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/spark-warehouse'.
20/12/03 14:59:04 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:59:04 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:59:04 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:59:04 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:59:04 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/12/03 14:59:05 INFO SparkContext: Added file darima-master/bash/darima.zip at spark://192.168.1.9:38409/files/darima.zip with timestamp 1607025545432
20/12/03 14:59:05 INFO Utils: Copying /home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/bash/darima.zip to /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/userFiles-dc1a2234-89fb-4342-8550-76626e0f7c7e/darima.zip
20/12/03 14:59:07 INFO InMemoryFileIndex: It took 70 ms to list leaf files for 1 paths.
20/12/03 14:59:08 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
20/12/03 14:59:09 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:59:09 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:59:09 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:59:09 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:59:10 INFO CodeGenerator: Code generated in 325.894497 ms
20/12/03 14:59:10 INFO CodeGenerator: Code generated in 27.712875 ms
20/12/03 14:59:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:59:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:59:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:46631 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:59:10 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
20/12/03 14:59:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:59:10 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
20/12/03 14:59:10 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
20/12/03 14:59:10 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/03 14:59:10 INFO DAGScheduler: Final stage: ResultStage 1 (count at NativeMethodAccessorImpl.java:0)
20/12/03 14:59:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/12/03 14:59:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/12/03 14:59:10 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:59:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 5.8 GiB)
20/12/03 14:59:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 5.8 GiB)
20/12/03 14:59:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:46631 (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:59:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/03 14:59:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:59:10 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/12/03 14:59:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:59:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.9:35677 (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:59:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.9:35677 (size: 28.7 KiB, free: 912.3 MiB)
20/12/03 14:59:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3175 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:59:13 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/03 14:59:13 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 3.340 s
20/12/03 14:59:13 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:59:13 INFO DAGScheduler: running: Set()
20/12/03 14:59:13 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/12/03 14:59:13 INFO DAGScheduler: failed: Set()
20/12/03 14:59:13 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/03 14:59:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 5.8 GiB)
20/12/03 14:59:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 5.8 GiB)
20/12/03 14:59:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:46631 (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:59:14 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
20/12/03 14:59:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/03 14:59:14 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
20/12/03 14:59:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:59:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.9:35677 (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:59:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.9:52388
20/12/03 14:59:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 335 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:59:14 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/03 14:59:14 INFO DAGScheduler: ResultStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.352 s
20/12/03 14:59:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/03 14:59:14 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
20/12/03 14:59:14 INFO DAGScheduler: Job 0 finished: count at NativeMethodAccessorImpl.java:0, took 3.779538 s
20/12/03 14:59:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
20/12/03 14:59:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:35677 in memory (size: 8.0 KiB, free: 912.3 MiB)
20/12/03 14:59:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.9:46631 in memory (size: 8.0 KiB, free: 5.8 GiB)
20/12/03 14:59:16 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:46631 in memory (size: 5.0 KiB, free: 5.8 GiB)
20/12/03 14:59:16 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.9:35677 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/12/03 14:59:17 INFO FileSourceStrategy: Pruning directories with: 
20/12/03 14:59:17 INFO FileSourceStrategy: Pushed Filters: 
20/12/03 14:59:17 INFO FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, t#1,traffic_volume#0)
20/12/03 14:59:17 INFO FileSourceStrategy: Output Data Schema: struct<traffic_volume: double, t: string>
20/12/03 14:59:17 INFO CodeGenerator: Code generated in 29.148161 ms
20/12/03 14:59:17 INFO CodeGenerator: Code generated in 17.479883 ms
20/12/03 14:59:17 INFO CodeGenerator: Code generated in 21.021521 ms
20/12/03 14:59:17 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.4 KiB, free 5.8 GiB)
20/12/03 14:59:17 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 5.8 GiB)
20/12/03 14:59:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:46631 (size: 28.7 KiB, free: 5.8 GiB)
20/12/03 14:59:17 INFO SparkContext: Created broadcast 3 from toPandas at /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/userFiles-dc1a2234-89fb-4342-8550-76626e0f7c7e/darima.zip/darima/dlsa.py:22
20/12/03 14:59:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/12/03 14:59:17 INFO SparkContext: Starting job: toPandas at /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/userFiles-dc1a2234-89fb-4342-8550-76626e0f7c7e/darima.zip/darima/dlsa.py:22
20/12/03 14:59:17 INFO DAGScheduler: Registering RDD 13 (toPandas at /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/userFiles-dc1a2234-89fb-4342-8550-76626e0f7c7e/darima.zip/darima/dlsa.py:22) as input to shuffle 1
20/12/03 14:59:17 INFO DAGScheduler: Got job 1 (toPandas at /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/userFiles-dc1a2234-89fb-4342-8550-76626e0f7c7e/darima.zip/darima/dlsa.py:22) with 200 output partitions
20/12/03 14:59:17 INFO DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/userFiles-dc1a2234-89fb-4342-8550-76626e0f7c7e/darima.zip/darima/dlsa.py:22)
20/12/03 14:59:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/12/03 14:59:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/12/03 14:59:17 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/userFiles-dc1a2234-89fb-4342-8550-76626e0f7c7e/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:59:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.2 KiB, free 5.8 GiB)
20/12/03 14:59:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 5.8 GiB)
20/12/03 14:59:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:46631 (size: 11.6 KiB, free: 5.8 GiB)
20/12/03 14:59:17 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/03 14:59:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at toPandas at /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/userFiles-dc1a2234-89fb-4342-8550-76626e0f7c7e/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0))
20/12/03 14:59:17 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/12/03 14:59:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7779 bytes)
20/12/03 14:59:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.9:35677 (size: 11.6 KiB, free: 912.3 MiB)
20/12/03 14:59:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.9:35677 (size: 28.7 KiB, free: 912.2 MiB)
20/12/03 14:59:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1295 ms on 192.168.1.9 (executor 1) (1/1)
20/12/03 14:59:19 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/03 14:59:19 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 37197
20/12/03 14:59:19 INFO DAGScheduler: ShuffleMapStage 2 (toPandas at /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/userFiles-dc1a2234-89fb-4342-8550-76626e0f7c7e/darima.zip/darima/dlsa.py:22) finished in 1.320 s
20/12/03 14:59:19 INFO DAGScheduler: looking for newly runnable stages
20/12/03 14:59:19 INFO DAGScheduler: running: Set()
20/12/03 14:59:19 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/12/03 14:59:19 INFO DAGScheduler: failed: Set()
20/12/03 14:59:19 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/userFiles-dc1a2234-89fb-4342-8550-76626e0f7c7e/darima.zip/darima/dlsa.py:22), which has no missing parents
20/12/03 14:59:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 428.0 KiB, free 5.8 GiB)
20/12/03 14:59:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 131.4 KiB, free 5.8 GiB)
20/12/03 14:59:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:46631 (size: 131.4 KiB, free: 5.8 GiB)
20/12/03 14:59:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
20/12/03 14:59:19 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at toPandas at /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/userFiles-dc1a2234-89fb-4342-8550-76626e0f7c7e/darima.zip/darima/dlsa.py:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/12/03 14:59:19 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
20/12/03 14:59:19 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:59:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.9:35677 (size: 131.4 KiB, free: 912.1 MiB)
20/12/03 14:59:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.9:52388
20/12/03 14:59:22 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 4, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:59:22 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/03 14:59:23 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 5, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:59:23 INFO TaskSetManager: Lost task 2.0 in stage 3.0 (TID 4) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 1]
20/12/03 14:59:24 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 6, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:59:24 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 5) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 2]
20/12/03 14:59:25 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 7, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:59:25 INFO TaskSetManager: Lost task 2.1 in stage 3.0 (TID 6) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 3]
20/12/03 14:59:26 INFO TaskSetManager: Starting task 2.2 in stage 3.0 (TID 8, 192.168.1.9, executor 1, partition 2, NODE_LOCAL, 7336 bytes)
20/12/03 14:59:26 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 7) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 4]
20/12/03 14:59:27 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
20/12/03 14:59:27 INFO TaskSetManager: Lost task 2.2 in stage 3.0 (TID 8) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 5]
20/12/03 14:59:28 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 9) on 192.168.1.9, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’

) [duplicate 6]
20/12/03 14:59:28 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
20/12/03 14:59:28 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/12/03 14:59:28 INFO YarnScheduler: Cancelling stage 3
20/12/03 14:59:28 INFO YarnScheduler: Killing all running tasks in stage 3: Stage cancelled
20/12/03 14:59:28 INFO DAGScheduler: ResultStage 3 (toPandas at /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/userFiles-dc1a2234-89fb-4342-8550-76626e0f7c7e/darima.zip/darima/dlsa.py:22) failed in 8.796 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, 192.168.1.9, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 605, in main
    process()
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 255, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 88, in dump_stream
    for batch in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/sql/pandas/serializers.py", line 248, in init_stream_yield_batches
    for series in iterator:
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 429, in mapper
    return f(keys, vals)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 175, in <lambda>
    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/worker.py", line 160, in wrapped
    result = f(pd.concat(value_series, axis=1))
  File "/home/bsuconn/.local/lib/python3.8/site-packages/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/home/bsuconn/Documents/Fall_2020/STAT_5825/Project/darima-master/run_darima.py", line 169, in darima_model_udf
    return darima_model(sample_df = sample_df, Y_name = Y_name, period = period, tol = tol,
  File "./darima.zip/darima/model.py", line 52, in darima_model
    dfitted = sarima2ar_model(robjects.FloatVector(x_train), period = period, tol = tol,
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 197, in __call__
    return (super(SignatureTranslatedFunction, self)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/robjects/functions.py", line 125, in __call__
    res = super(Function, self).__call__(*new_args, **new_kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface_lib/conversion.py", line 44, in _
    cdata = function(*args, **kwargs)
  File "/home/bsuconn/.local/lib/python3.8/site-packages/rpy2/rinterface.py", line 624, in __call__
    raise embedded.RRuntimeError(_rinterface._geterrmessage())
rpy2.rinterface_lib.embedded.RRuntimeError: Error in loadNamespace(name) : there is no package called ‘forecast’


	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3562)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/03 14:59:28 INFO DAGScheduler: Job 1 failed: toPandas at /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/userFiles-dc1a2234-89fb-4342-8550-76626e0f7c7e/darima.zip/darima/dlsa.py:22, took 10.176234 s
20/12/03 14:59:28 INFO SparkContext: Invoking stop() from shutdown hook
20/12/03 14:59:28 INFO SparkUI: Stopped Spark web UI at http://192.168.1.9:4040
20/12/03 14:59:28 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/12/03 14:59:28 INFO YarnClientSchedulerBackend: Shutting down all executors
20/12/03 14:59:28 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/12/03 14:59:28 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
20/12/03 14:59:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/03 14:59:28 INFO MemoryStore: MemoryStore cleared
20/12/03 14:59:28 INFO BlockManager: BlockManager stopped
20/12/03 14:59:28 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/03 14:59:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/03 14:59:28 INFO SparkContext: Successfully stopped SparkContext
20/12/03 14:59:28 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:59:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4
20/12/03 14:59:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-42ff0642-7e10-4e7a-947a-533d8586d0a4/pyspark-42562692-8cc8-4391-b029-86b75ed59eb2
20/12/03 14:59:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-5b115889-e8d9-4503-999e-ff11c135d56d
20/12/03 14:59:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/03 14:59:31 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:59:31 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:59:31 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:59:31 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:59:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:59:32 INFO SparkContext: Running Spark version 3.0.1
20/12/03 14:59:32 INFO ResourceUtils: ==============================================================
20/12/03 14:59:32 INFO ResourceUtils: Resources for spark.driver:

20/12/03 14:59:32 INFO ResourceUtils: ==============================================================
20/12/03 14:59:32 INFO SparkContext: Submitted application: Spark DARIMA App
20/12/03 14:59:32 INFO SecurityManager: Changing view acls to: bsuconn
20/12/03 14:59:32 INFO SecurityManager: Changing modify acls to: bsuconn
20/12/03 14:59:32 INFO SecurityManager: Changing view acls groups to: 
20/12/03 14:59:32 INFO SecurityManager: Changing modify acls groups to: 
20/12/03 14:59:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bsuconn); groups with view permissions: Set(); users  with modify permissions: Set(bsuconn); groups with modify permissions: Set()
20/12/03 14:59:32 INFO Utils: Successfully started service 'sparkDriver' on port 33097.
20/12/03 14:59:32 INFO SparkEnv: Registering MapOutputTracker
20/12/03 14:59:32 INFO SparkEnv: Registering BlockManagerMaster
20/12/03 14:59:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/03 14:59:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/03 14:59:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/03 14:59:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4cdbec7b-bb16-416c-877e-9acd391fda69
20/12/03 14:59:32 INFO MemoryStore: MemoryStore started with capacity 5.8 GiB
20/12/03 14:59:32 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/03 14:59:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/03 14:59:33 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.9:4040
20/12/03 14:59:33 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/12/03 14:59:33 INFO Client: Requesting a new application from cluster with 1 NodeManagers
20/12/03 14:59:34 INFO Configuration: resource-types.xml not found
20/12/03 14:59:34 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/03 14:59:34 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
20/12/03 14:59:34 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/12/03 14:59:34 INFO Client: Setting up container launch context for our AM
20/12/03 14:59:34 INFO Client: Setting up the launch environment for our AM container
20/12/03 14:59:34 INFO Client: Preparing resources for our AM container
20/12/03 14:59:34 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/03 14:59:37 INFO Client: Uploading resource file:/tmp/spark-ecdd6872-8f5c-46a2-ab54-301b7d3dcee9/__spark_libs__2458270534836139269.zip -> hdfs://localhost:9000/user/bsuconn/.sparkStaging/application_1607015337794_0039/__spark_libs__2458270534836139269.zip
20/12/03 14:59:38 INFO DiskBlockManager: Shutdown hook called
20/12/03 14:59:38 INFO ShutdownHookManager: Shutdown hook called
20/12/03 14:59:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-ecdd6872-8f5c-46a2-ab54-301b7d3dcee9
20/12/03 14:59:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-710e48f4-cf5e-414a-bd2f-596c7f0543cd
20/12/03 14:59:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-ecdd6872-8f5c-46a2-ab54-301b7d3dcee9/userFiles-d538bff4-fc54-44ad-9568-306214a7a247
